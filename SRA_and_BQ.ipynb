{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore SRA Download and BigQuery\n",
    "Often you will access SRA data using the sra-tools with [fasterq-dump](https://github.com/ncbi/sra-tools/wiki/HowTo%3A-fasterq-dump), but that tool only downloads one accession number at a time. To download batch data, we used the ipyrad api and borrow language from an ipyrad cookbook data download example. To see more ipyrad example notebooks go to this [github page](https://github.com/dereneaton/ipyrad/blob/master/newdocs/API-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1, download SRA data and align to a reference genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required software\n",
    "Specify the number of CPUs on your VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CPU=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\n",
    "bash Mambaforge-$(uname)-$(uname -m).sh -b -p $HOME/mambaforge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the following since this is a fresh VM\n",
    "#see the pangolin notebook for how to install mamba and add to your path\n",
    "#you can also use conda, since it is preinstalled, it is just a lot slower and less reliable\n",
    "!./mambaforge/bin/mamba install -y -c bioconda -c conda-forge ipyrad sra-tools vcftools biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyrad.analysis as ipa\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for a published data set by its accession ID\n",
    "You can find the study ID or individual sample IDs from published papers or by searching the NCBI or related databases. ipyrad can take as input one or more accessions IDs for individual Runs or Studies (SRR or SRP, and similarly ERR or ERP, etc.). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "From an sratools object you can fetch just the info, or you can download the files as well. Here we call `.run()` to download the data into a designated workdir. There are arguments for how to name the files according to name fields in the fetch_runinfo table. The accessions argument here is a list of the first five SRR sample IDs in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually define list of SRA files\n",
    "list_of_srrs = ['SRR14086881','SRR14086882','SRR14086883','SRR14086884','SRR14086885','SRR14086886','SRR14086887','SRR14086888','SRR14086889','SRR14086890']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sra object\n",
    "sra2 = ipa.sratools(accessions=list_of_srrs, workdir=\"downloaded\")\n",
    "\n",
    "# call download (run) function\n",
    "sra2.run(auto=True, name_fields=(1,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data files\n",
    "You can see that the files were named according to the SRR and species name in the table. The intermediate .sra files were removed and only the fastq files were saved. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also navigate in the menu on the left\n",
    "! ls -l downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the data to a reference genome\n",
    "First, download the sars-cov-2 reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "#download the reference\n",
    "#use the bio.entrez toolkit within biopython to download the accession numbers\n",
    "#save those sequences to a single fasta file\n",
    "Entrez.email = \"email@example.com\"  # Always tell NCBI who you are\n",
    "filename = \"sarscov2_reference.fasta\"\n",
    "acc_nums=['NC_045512']\n",
    "for acc in acc_nums:\n",
    "        net_handle = Entrez.efetch(\n",
    "            db=\"nucleotide\", id=acc, rettype=\"fasta\", retmode=\"text\"\n",
    "        )\n",
    "        out_handle = open(filename, \"a\")\n",
    "        out_handle.write(net_handle.read())\n",
    "        out_handle.close()\n",
    "        net_handle.close()\n",
    "        print(\"Saved\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run BWA. It will just take a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the reference\n",
    "!bwa index sarscov2_reference.fasta\n",
    "# run bwa mem\n",
    "!bwa mem -t $CPU sarscov2_reference.fasta ./downloaded/SRR14086881_MSHSPSP-RNP263.fastq > sars.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the quality of the alignment, what percentage of the input reads aligned to the reference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools flagstat sars.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it for Part 1! We downloaded SRA data, downloaded a reference from Genbank, and then aligned our fastq files to the reference! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2, Copy VCF to Storage Bucket and then Run a Query with BigQuery\n",
    "\n",
    "Here we are going to shift gears and use Google BigQuery, which is a serverless (you aren't responsible for managing VMs) data warehouse for analyzing data using SQL. We could translate that to 'a giant database with very fast SQL query capabilities'. We will download a vcf file, create a bucket, copy our data to the bucket, and then import some example bigquery vcf files to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download a vcf from this [galaxy url](https://usegalaxy.org/u/carlosfarkas/h/snpeffsars-cov-2) which comes from this [paper](https://www.frontiersin.org/articles/10.3389/fmicb.2021.665041/full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl https://usegalaxy.org/datasets/bbd44e69cb8906b553c8fa023002fca0/display?to_ext=vcf --output sra-sars.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "vcftools --vcf sra-covid.vcf --maf 0.005 --recode --out sra-covid-maf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cloud bucket\n",
    "The data is currently stored on this notebook instance, which is fine for any analyses here, but if we want to use that data somewhere else on GCP, or to access it when the VM is shut down, we need to copy the data to a cloud storage bucket. If we do not have one made, we can create one using the gsutil command line tool, or you can do it using the [console](https://cloud.google.com/storage/docs/quickstart-console)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your own project here and bucket name of choice. It must be globally unique\n",
    "!export PROJECT='us-gcp-ame-adv-c01-npd-1'\n",
    "!export BUCKET='vcf-to-bq2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot with the gsutil mb command, read more [here](https://cloud.google.com/storage/docs/gsutil/commands/mb).\n",
    "Note that if you end up with permission issues further down, you may need to skip these cells and create the bucket using the UI. You can also try going to IAM and adding the necessary permissions to the service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -c regional -l us-east4 gs://vcf-to-bq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://vcf-to-bq2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's copy the vcf file to the gs bucket using gsutil cp. If you are moving lots of files use gsutil -m cp /ex_dir/* or recursive with gsutil -m cp -r /ex_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp sra-covid-maf.recode.vcf gs://vcf-to-bq2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at our bucket to make sure the files are organized as expected. You can also just look at the bucket in the UI\n",
    "!gsutil ls gs://vcf-to-bq2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore VCF queries in BigQuery\n",
    "The rest of the notebook could be run in the BigQuery UI in the console. Here we are querying bigquery datasets via the APi, but if you paste the block of SQL code in the editor window of bigquery, it would have the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the biquery api\n",
    "from google.cloud import bigquery\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the client for the API\n",
    "client = bigquery.Client(location=\"US\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some cancer genomic data from the [ISB Cancer Gateway in the Cloud](https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/progapi/bigqueryGUI/GettingStartedWithGoogleBigQuery.html). If you follow that link, it gives a pretty good tutorial on how to use BigQuery using the UI. You can also use some of the sample notebooks from the left hand nav menu that walk you through how to use bigquery from a notebook in addition to what we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the most frequently observed mutations are for KRAS mutant tumor samples.\n",
    "query = \"\"\"\n",
    "WITH temp1 AS (\n",
    "   SELECT\n",
    "     sample_barcode_tumor,\n",
    "     Hugo_Symbol,\n",
    "     Variant_Classification,\n",
    "     Variant_Type,\n",
    "     SIFT,\n",
    "     PolyPhen\n",
    "   FROM `isb-cgc-bq.TCGA_versioned.somatic_mutation_hg38_gdc_r10`\n",
    "   GROUP BY\n",
    "     sample_barcode_tumor,\n",
    "     Hugo_Symbol,\n",
    "     Variant_Classification,\n",
    "     Variant_Type,\n",
    "     SIFT,\n",
    "     PolyPhen)\n",
    "SELECT\n",
    "  COUNT(*) AS num,\n",
    "  Hugo_Symbol,\n",
    "  Variant_Classification,\n",
    "  Variant_Type,\n",
    "  SIFT,\n",
    "  PolyPhen\n",
    "FROM temp1\n",
    "GROUP BY\n",
    "  Hugo_Symbol,\n",
    "  Variant_Classification,\n",
    "  Variant_Type,\n",
    "  SIFT,\n",
    "  PolyPhen\n",
    "ORDER BY num DESC\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's follow these steps to look at the [DeepVariant platinum genomes calls](https://cloud.google.com/life-sciences/docs/tutorials/analyze-variants-advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first look at total calls per sample\n",
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  call.name AS call_name,\n",
    "  COUNT(call.name) AS call_count_for_call_set\n",
    "FROM\n",
    "  `bigquery-public-data.human_genome_variants.platinum_genomes_deepvariant_variants_20180823` v, v.call\n",
    "GROUP BY\n",
    "  call_name\n",
    "ORDER BY\n",
    "  call_name\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People don't actually have ~30 million variants, so let's filter to real variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  call.name AS call_name,\n",
    "  COUNT(call.name) AS call_count_for_call_set\n",
    "FROM\n",
    "  `bigquery-public-data.human_genome_variants.platinum_genomes_deepvariant_variants_20180823` v, v.call\n",
    "WHERE\n",
    "  EXISTS (SELECT 1\n",
    "            FROM UNNEST(v.alternate_bases) AS alt\n",
    "          WHERE\n",
    "            alt.alt NOT IN (\"<NON_REF>\", \"<*>\"))\n",
    "GROUP BY\n",
    "  call_name\n",
    "ORDER BY\n",
    "  call_name\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the no-calls and the number of variants starts to look more reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  call.name AS call_name,\n",
    "  COUNT(call.name) AS call_count_for_call_set\n",
    "FROM\n",
    "  `bigquery-public-data.human_genome_variants.platinum_genomes_deepvariant_variants_20180823` v, v.call\n",
    "WHERE\n",
    "  EXISTS (SELECT 1 FROM UNNEST(call.genotype) AS gt WHERE gt > 0)\n",
    "  AND NOT EXISTS (SELECT 1 FROM UNNEST(call.genotype) AS gt WHERE gt < 0)\n",
    "GROUP BY\n",
    "  call_name\n",
    "ORDER BY\n",
    "  call_name\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Make sure before you move onto the next thing, you try running a few queries in the BigQuery editor within the UI. You can also create a new dataset under your project and explore the public datasets available there too."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

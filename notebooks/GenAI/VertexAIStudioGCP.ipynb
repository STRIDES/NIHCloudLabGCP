{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# Vertex AI Studio on GCP - Article Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In research you often need to read several papers to understand new methods which can be very time consuming. In this tutorial we will use generative AI to summarize long documents but with the goal of perserving the most important information using Vertex AI Studio's Article Summary model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "+ You need access to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "+ Learn how to use LLMs for summarization\n",
    "+ Learn how to navigate Vertex AI to deploy and use LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### A note on costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Generative AI pricing](https://cloud.google.com/vertex-ai/pricing#generative_ai_models), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_xMwRLuyDrj"
   },
   "source": [
    "Here you will use LLM via the API to summarize the extracted texts. Please note that LLMs currently have input text limit and stuffing a large input text might not be accepted. You can read more about quotas and limits [here](https://cloud.google.com/vertex-ai/docs/quotas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Vertex AI Studio console by navigating to Vertex AI via the search bar on the console. On the left side menu scroll down to Vertex AI Studio, click **Language**.\n",
    "\n",
    " <img src=\"../../images/GCPGenStudio2.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "\n",
    "Scroll down **\"Prompt examples\"** then to **Summarization** and click **\"Open\"** on **Article Summary**. You will see a prompt session were you will need to enter in the contents of your article as the console does not allow you to upload files. For this tutorial this article is about how gut microbiota affects Alzeheimer's disease because of the gut-brain-microbiota axis network [here](https://www.aging-us.com/article/102930/pdf).\n",
    "\n",
    " <img src=\"../../images/GCPGenStudio2.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "To the right you can control the parameters that we have been using before this is a great way to test what each parameter does and how they effect each other. Once you are done click **submit**, you should have a similar output as below. For explainations on the parameters **temperature, Output token limit, top p, and top k** see the following article [here](https://cloud.google.com/vertex-ai/docs/generative-ai/text/test-text-prompts#generative-ai-test-text-prompt-drest).\n",
    "\n",
    " <img src=\"../../images/GCPGenStudio3.png\" width=\"600\" height=\"600\">\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try increasing the temperature parameter and see if we recieve a different output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/GCPGenStudio4.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our output becomes shorter and straight to the point this is because the temperature parameter controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less creative responses, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aVrDWkJs3Y"
   },
   "source": [
    "### Troubleshooting\n",
    "\n",
    "If you model responds with an error its generally because the extracted text is too long for the generative model to process. In order for the model to work best try to not include any abstract or reference sections of your article, if errors still come up try limiting the article even more by removing other sections such as the intro or extracting the first 30,000 words of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtp21WX3T7d_"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Although full text is too large for the model, you have managed to create a concise, paragraph of the most important information from a portion of the PDF using the model. This method is the most simiplest and is ideal for shorter documents but can still be used when you limit the character number you want the model to read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "Shut down your Vertex AI instance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "summarization_large_documents.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

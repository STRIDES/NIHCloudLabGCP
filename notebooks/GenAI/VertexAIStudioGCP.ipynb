{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertex AI Studio on GCP - Article Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In research you often need to read several papers to understand new methods which can be very time consuming. In this tutorial we will use generative AI to summarize long documents but with the goal of perserving the most important information using Vertex AI Studio's Article Summary model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "+ You need access to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "+ Learn how to use LLMs for summarization\n",
    "+ Learn how to navigate Vertex AI to deploy and use LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Generative AI pricing](https://cloud.google.com/vertex-ai/pricing#generative_ai_models), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will use LLM via the API to summarize the extracted texts. Please note that LLMs currently have input text limit and stuffing a large input text might not be accepted. You can read more about quotas and limits [here](https://cloud.google.com/vertex-ai/docs/quotas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the horizontal three lines on the top left of the console, scroll down to **Vertex AI**, click **Prompt Gallery**. \n",
    "\n",
    " <img src=\"../../images/vertex-ai-create-prompt.jpeg\" width=\"200\">\n",
    "\n",
    "\n",
    "Search for **Summarizing texts** and click on it. You will see the system instructions and the prompt on the chat that are populated with the summarizing example. Since we want to summarize our own article, we clear the prompt by clicking **Clear prompt** on the top right and then **Insert Media**, select **ByURL** if you want to upload a document from the web or **Upload** if you want to upload it from your local machine. Paste the url link and press **Insert**. The document will be loaded into the Prompt and you will have to enter some instruction of what to do with the uploaded document i.e. \"Please summarize the uploaded document.\" For this tutorial this article is about how gut microbiota affects Alzeheimer's disease because of the gut-brain-microbiota axis network [here](https://www.aging-us.com/article/102930/pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " <img src=\"../../images/VertexAIStudioGCP_new05.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "To the right you can control the parameters that we have been using before this is a great way to test what each parameter does and how they effect each other. Once you are done click the submit arrow, you should have a similar output as below. For explainations on the parameters **temperature, Output token limit, top p, and top k** see the following article [here](https://cloud.google.com/vertex-ai/docs/generative-ai/text/test-text-prompts#generative-ai-test-text-prompt-drest).\n",
    "\n",
    " <img src=\"../../images/VertexAIStudioGCP_new06.png\" width=\"600\" height=\"600\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try increasing the temperature parameter and see if we recieve a different output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/VertexAIStudioGCP_new07.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our output becomes shorter and straight to the point this is because the temperature parameter controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less creative responses, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "If you model responds with an error its generally because the extracted text is too long for the generative model to process. In order for the model to work best try to not include any abstract or reference sections of your article, if errors still come up try limiting the article even more by removing other sections such as the intro or extracting the first 30,000 words of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although full text is too large for the model, you have managed to create a concise, paragraph of the most important information from a portion of the PDF using the model. This method is the most simiplest and is ideal for shorter documents but can still be used when you limit the character number you want the model to read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "Shut down your Vertex AI instance"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f00e116",
   "metadata": {},
   "source": [
    "# Using LangChain on Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f6f80-73aa-4a31-9eb5-ac30d5958fe7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This tutorial is designed to give you the basics of using langchain to work with Large Language Models (LLMs) for document summarization and basic chat bot functionality. You could take what we have here to build a front end application using something like streamlit, or other further iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aff184",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ Learn the basics of LangChain\n",
    "+ Learn how to use the Google Cloud tools from LangChain\n",
    "+ Learn how to deploy and interact with LLMs\n",
    "+ Learn how to use vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d170e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "+ You need access to Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf708984",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0d198-f1cf-40ec-b813-4b1e8d50ab80",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662e8f8-66ce-4ca6-a121-d087c499390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U google-cloud-aiplatform langchain langchain-community langchain-google-vertexai pypdf faiss-cpu --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd534fb6-29c8-4c15-b9cc-88f667ec8127",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6851a-f15d-4881-8173-9b788a009201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "! export USER_AGENT=\"my-langchain-script/1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b34ed1-4ad0-4ab6-8e9f-148c6ef3f575",
   "metadata": {},
   "source": [
    "### Summarize a scientific article using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1b6cc-862e-4a67-a755-fbc4f7595c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://pubmed.ncbi.nlm.nih.gov/37883540/\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bd138-d852-40ba-87bd-ee559483aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(model_name=\"gemini-1.5-flash-002\")\n",
    "print('the LLM and default params are : ', llm)\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "print('\\n''the LLM chain used is ''\\n', chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2c20d-7678-4f6d-81c7-0b2a2b62d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the summary of the document in a single paragraph is: ')\n",
    "\n",
    "print(chain.invoke(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fc982-3d07-4501-8a54-6957100ebaff",
   "metadata": {},
   "source": [
    "**Now try using [a different LLM](https://python.langchain.com/docs/integrations/llms/) and see if you can get the code to run!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883dab5-cabd-4eea-bddc-c4c14b2bf5dc",
   "metadata": {},
   "source": [
    "### Ask a general question to an LLM, without the context of a specific source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad234c3-47c4-4aaf-a5b1-a3323555a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cdbda-6446-4bbb-8018-f24fce5a7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323a512-5826-4498-baa6-65dca1dc6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What evidence do we have for chimpanzees going through menopause?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a49df-81f4-4afa-acd6-799e4cfba921",
   "metadata": {},
   "source": [
    "### Build a simple Chat Bot to query specific content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c47bde-210d-46d7-ab3a-43ee000d293e",
   "metadata": {},
   "source": [
    "### Load your PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a518c3b-f20c-42df-956c-2f75081c1a6f",
   "metadata": {},
   "source": [
    "Read more about document loaders from langchain [here](https://python.langchain.com/docs/how_to/document_loader_pdf/). Note that we are both loading, and splitting our document. You can read more about the default document chunking/splitting procedures [here](https://python.langchain.com/docs/concepts/text_splitters/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b3383-a8ea-45f7-95bb-a5da4055c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user-agent \"Chrome\" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10954554/pdf/41586_2024_Article_7159.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bcbbb-8e24-424d-931d-c9b6c09fb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"41586_2024_Article_7159.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a30a3-8a71-47ff-b264-83517e2b163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could also load from the web url\n",
    "# loader = WebBaseLoader(\"https://pubmed.ncbi.nlm.nih.gov/37883540/\")\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f2d86-7e59-40f0-bfea-facd6fec226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878675c2-183c-445f-8829-b403ae3d2858",
   "metadata": {},
   "source": [
    "### Create a vector store\n",
    "One of the usual methods for organizing and searching through unstructured data is to convert it into embedded vectors, which are compact (numerical) representations. These vectors are stored, and when you want to find something similar, you turn your query into an embedded vector as well. A \"vector store\" then manages the stored data and helps you find the most similar vectors to your query. Read more about vector stores in langchain [here](https://python.langchain.com/docs/how_to/#vector-stores). Here we are going to use a very meta technique using the Facebook AI Similarity Search (FAISS) library. You can explore the various vector store options [here](https://python.langchain.com/docs/integrations/vectorstores/). Here we are using embeddings to downselect the total information we want to feed to the LLM downstream. As token limits go up, we will eventually be able to feed a whole document to the LLM, but for now, you will usually need to use this method to downsample. If your document is small enough, just push it directly to the LLM. Also, use embeddings for when you want to query over many documents (1000's). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144b302-d8a5-4c12-9e8a-8bff530c7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the document using FAISS\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "faiss_index = FAISS.from_documents(pages, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88830a-0681-4f0b-9ea1-b0a112d27091",
   "metadata": {},
   "source": [
    "Define the user query, which will also be converted to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394026e-f70e-4528-9f72-b35b87f1af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What evidence is there that toothed whales go through menopause'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8aab69-2042-4b75-8f5d-f06449daf063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = faiss_index.similarity_search(query, k=5)\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecee51f-2305-4906-bba4-a362ee1c742d",
   "metadata": {},
   "source": [
    "Now we have summaries of our query based on the article. Now we need to pass the summaries to our LLM and generate a single summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc812d-bb3d-4d2a-97fc-344b7c120c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"content\": lambda pages: \"\\n\\n\".join(\n",
    "            format_document(page, doc_prompt) for page in pages\n",
    "        )\n",
    "    }\n",
    "    | PromptTemplate.from_template(\"Summarize the following content in around 200 words:\\n\\n{content}\")\n",
    "    | ChatVertexAI(model=\"gemini-1.5-flash-002\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b92df8-33db-45e6-aa67-85e2d8f18f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7596948-c752-42f5-a4f7-6d96bf93e20b",
   "metadata": {},
   "source": [
    "Here are a few example prompts, try runnning them in the template and chain below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187e188-07f1-4d67-a958-8b7080d725e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Instructions: You need to summarize text from several documents. \\\n",
    "                                   Be professional, factual, and succinct in the response. \\\n",
    "                                   Your answer is ONLY based on information in the documents above. \\\n",
    "                                   If you can not answer the question, answer \\\n",
    "                                   I am sorry, I am unable to answer the question based on the information provided \\\n",
    "                                   ONLY use information that is based on the documents. \\\n",
    "                                   \\\n",
    "                                   Document number: \\\n",
    "                                   Documents: {page_content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d6717-d9e2-4365-9607-5b17afd65731",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"page_content\": lambda pages: \"\\n\\n\".join(\n",
    "            format_document(page, doc_prompt) for page in pages\n",
    "        )\n",
    "    }\n",
    "    | PromptTemplate.from_template(prompt_str) \n",
    "    | ChatVertexAI(model=\"gemini-1.5-flash-002\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1f59f-4ca6-4ac1-a9d2-6cf9265f7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8637cd-36d2-4946-b5f2-051e57a284ea",
   "metadata": {},
   "source": [
    "### Deploy a local Model\n",
    "If you want to avoid sending data over the internet, you can deploy a model to an endpoint following [these instructions](https://cloud.google.com/vertex-ai/docs/general/deployment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f114a47-90db-403f-8b32-0f40dc220877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model garden\n",
    "#https://cloud.google.com/vertex-ai/docs/general/deployment#what_happens_when_you_deploy_a_model\n",
    "from langchain_google_vertexai import VertexAIModelGarden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961cf2b-71d9-4ecf-a50b-8a748ba8291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAIModelGarden(\n",
    "    project=\"YOUR PROJECT ID\",\n",
    "    endpoint_id=\"YOUR ENDPOINT ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df24a4-0a2e-4077-a831-b8194b8c312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAIModelGarden(\n",
    "    project=\"YOUR PROJECT ID\",\n",
    "    endpoint_id=\"YOUR ENDPOINT ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6c4c7-e824-417a-a41a-ddd37cb4e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(\"What are the greatest questions left to answer in biomedical research?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd7731-fc36-42e9-b387-5354f7b133b9",
   "metadata": {},
   "source": [
    "You can repeat any of the methods shown above, but using the locally deployed LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9acefe-3fa7-4408-a0de-94c370d0560b",
   "metadata": {},
   "source": [
    "### Generate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1c313-b0e0-4ff2-8496-aaa9ba8e8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", max_output_tokens=1000, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c527588-a71c-4719-8856-068b2bc3e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a python function that checks if a string is a valid email address\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591a4d2-dabf-4d2c-a2a6-cd97b882a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541e5ad-faa4-423c-ba09-ce49a7e10f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a Nextflow module from nf-core to run bwa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ba6a9-ff54-4f58-895e-cc7b8ae9b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682802fa-1de6-479a-a219-e6b784e74a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a Snakemake module from nf-core to run bwa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07198a26-2595-44c7-897a-7b7b9dfcd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165b6d6-8293-45b8-aedc-93395c884659",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "+ You learned how to work with LLMs in Vertex AI using LangChain\n",
    "+ You learned how to use vector stores and document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836b24d",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e487b",
   "metadata": {},
   "source": [
    "Make sure to stop your VM"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

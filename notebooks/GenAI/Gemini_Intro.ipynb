{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51ed24c-53dd-4239-8be0-0e0422596ba3",
   "metadata": {},
   "source": [
    "# Intro to GCP's Gemini "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc412f-5338-45ba-8b29-9039c19208d9",
   "metadata": {},
   "source": [
    "**Gemini** is a Google multimodal model that has the capability to **summarize, chat, and generate text from images or videos**. Gemini comes in two model versions **Gemini Pro** and **Gemini Pro Vision**, for this tutorial we will be looking into utilizing both models via python packages and GCPs model playground, **Vertex AI Studio**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ Learn how to interact with Gemini as a chatbot from a Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "+ You need access to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec6d40-b5b3-434f-adc4-2838b7f49d1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Update the google-cloud-aiplatform package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9781dd-9764-4e9c-88ba-fcd7bb95842a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.74.0-py2.py3-none-any.whl.metadata (31 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth<3.0.0dev,>=2.14.1 (from google-cloud-aiplatform)\n",
      "  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-aiplatform)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-aiplatform)\n",
      "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/codespace/.local/lib/python3.12/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_bigquery-3.27.0-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_resource_manager-1.13.1-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform)\n",
      "  Downloading shapely-2.0.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting docstring-parser<1 (from google-cloud-aiplatform)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.11.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n",
      "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain) (2.1.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from langchain) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /home/codespace/.local/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
      "  Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Downloading google_crc32c-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.21->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<0.4.0,>=0.3.21->langchain)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading google_cloud_aiplatform-1.74.0-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.9-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "Downloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading google_cloud_bigquery-3.27.0-py2.py3-none-any.whl (240 kB)\n",
      "Downloading google_cloud_resource_manager-1.13.1-py2.py3-none-any.whl (358 kB)\n",
      "Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
      "Downloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Downloading shapely-2.0.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.68.1-py3-none-any.whl (14 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading orjson-3.10.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: typing-extensions, shapely, python-dotenv, pyasn1, protobuf, propcache, orjson, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, grpcio, greenlet, google-crc32c, frozenlist, docstring-parser, cachetools, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, rsa, requests-toolbelt, pydantic-core, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, aiosignal, pydantic, grpcio-status, google-auth, dataclasses-json, aiohttp, pydantic-settings, langsmith, grpc-google-iam-v1, google-api-core, langchain-core, google-cloud-core, langchain-text-splitters, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, langchain, google-cloud-aiplatform, langchain-community\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.9 aiosignal-1.3.1 annotated-types-0.7.0 cachetools-5.5.0 dataclasses-json-0.6.7 docstring-parser-0.16 frozenlist-1.5.0 google-api-core-2.23.0 google-auth-2.36.0 google-cloud-aiplatform-1.74.0 google-cloud-bigquery-3.27.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.13.1 google-cloud-storage-2.19.0 google-crc32c-1.6.0 google-resumable-media-2.7.2 googleapis-common-protos-1.66.0 greenlet-3.1.1 grpc-google-iam-v1-0.13.1 grpcio-1.68.1 grpcio-status-1.68.1 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.9 langchain-community-0.3.9 langchain-core-0.3.21 langchain-text-splitters-0.3.2 langsmith-0.1.147 marshmallow-3.23.1 multidict-6.1.0 mypy-extensions-1.0.0 orjson-3.10.12 propcache-0.2.1 proto-plus-1.25.0 protobuf-5.29.1 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.3 pydantic-core-2.27.1 pydantic-settings-2.6.1 python-dotenv-1.0.1 requests-toolbelt-1.0.0 rsa-4.9 shapely-2.0.6 typing-extensions-4.12.2 typing-inspect-0.9.0 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-cloud-aiplatform  langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbbed7-becf-48d0-98c3-6dd9942fd377",
   "metadata": {},
   "source": [
    "Next we initialize the Gemini model by setting out project id and location. We are also pulling in the packages:\n",
    "- **GenerativeModel:** Allows us to specify and launch the Gemini model we need (e.g. Gemini Pro, Gemini Pro Vision).\n",
    "- **ChatSession:** Set Gemini Pro in chatbot mode.\n",
    "- **Part:** Loads in files from buckets.\n",
    "- **Image:** Loads in image files locally.\n",
    "- **GenerationConfig:** Allows us to configure the models temperature, top p, top k, and max tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc9232-383f-405b-b1a8-fab64a80492d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking parameters from ENV\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Image, GenerativeModel, ChatSession, Part, GenerationConfig\n",
    "\n",
    "import json\n",
    "# Load env.json\n",
    "try:\n",
    "    with open(\"env.json\") as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    config = {}\n",
    "\n",
    "\n",
    "# Assign parameters from Env.\n",
    "project_id = config.get(\"NOTEBOOK_GCP_PROJECT_ID\")\n",
    "location = config.get(\"NOTEBOOK_GCP_LOCATION\")\n",
    "\n",
    "\n",
    "params = globals().get('parameters', {})\n",
    "pid = params.get('NOTEBOOK_GCP_PROJECT_ID')\n",
    "print(f\"My PID: {pid}\")\n",
    "\n",
    "print(\"Checking parameters from ENV\")\n",
    "print(project_id)\n",
    "print(location)\n",
    "\n",
    "# TODO( FOR developer): If not defined in ENV earlier, uncomment and add it below\n",
    "#project_id = \"<PROJECT_ID>\"\n",
    "#location = \"<LOCATION>\" #(e.g., us-central1)\n",
    "\n",
    "vertexai.init(project=project_id, location=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f24ba-4034-4424-91d8-1229682755ab",
   "metadata": {},
   "source": [
    "### Gemini as a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dec028-6d30-4078-ac54-62b849ae9ced",
   "metadata": {},
   "source": [
    "For dealing with text, code generation, natural language tasks we can use the **gemini-pro** model and to set our model in **chatbot mode** we need to use the `start_chat()` function. You will see below we also created a function named **get_chat_response** which will send the prompt or message we have for our model using the `send_message()` function and returns only the text of the chats response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc5b25-c796-4015-82dc-6bc861bb525f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")\n",
    "chat = model.start_chat()\n",
    "\n",
    "def get_chat_response(chat: ChatSession, prompt: str):\n",
    "    response = chat.send_message(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40e87a-76ac-469b-a504-fb903319cbaf",
   "metadata": {},
   "source": [
    "Now that we have our functions lets ask our Gemini chatbot some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3277de-ab85-417f-b1d8-b21985a7a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello.\"\n",
    "print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a0e3d-fbcb-4562-bb5f-b439a92e80e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"List gen ai use cases that are Life Science or Health Care related. \"\n",
    "print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfda59a-c440-4489-8f00-a4316b827292",
   "metadata": {},
   "source": [
    "We can even ask it to **generate code or debug code**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b917b2-22b5-4011-a9c4-d8a667cf6b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"create a python code that will replace all null values to zero within a csv file\"\n",
    "print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d57d2-dbe1-434a-9345-fe5ae3315a21",
   "metadata": {},
   "source": [
    "### Gemini as a Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856bd61-3e24-40ea-8da4-1a6edc5f3e1d",
   "metadata": {},
   "source": [
    "We can generate text like asking Gemini Pro to summarize articles we provide locally (using langchain). As of now Gemini does not support loading in documents that are not videos and images directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7df19-a625-40dc-b4e5-faff5e7ba241",
   "metadata": {},
   "source": [
    "First we will load in a file using langchains text loader. You can also use langchain to load in files from your bucket following the instructions [here](https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cf184-c815-425c-9742-7625123e02bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#download the article\n",
    "!wget --user-agent \"Chrome\" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10954554/pdf/41586_2024_Article_7159.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becd6c2-daf0-4287-80e5-06cf419287bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"41586_2024_Article_7159.pdf\")\n",
    "ex_file=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d6c19-7d68-49e1-85e9-91bcd6bd1775",
   "metadata": {},
   "source": [
    "We can configure our model to give us the best optimal output by setting the parameters below:\n",
    "- **Max_Output_Token**: Max number of words to generate.\n",
    "- **Temperature:** Controls randomness, higher values increase diversity meaning a more unique response make the model to think harder. Must be a number from 0 to 1.\n",
    "- **Top_p (nucleus):** The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus. Must be a number from 0 to 1.\n",
    "- **Top_k:** Sample from the k most likely next tokens at each step. Lower k focuses on higher probability tokens. This means the model choses the most probable words. Lower values eliminate fewer coherent words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4228e44-9639-40da-8f69-343be93b65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.9,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=8192,\n",
    ")\n",
    "\n",
    "def summarizer(file: str) -> str:\n",
    "        \n",
    "    # Query the model\n",
    "    response = model.generate_content(\n",
    "        [\n",
    "            # Add an example query\n",
    "            \"summarize this file.\",\n",
    "            file\n",
    "        ],\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    #print(response)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb722d6-ef79-4a0b-9327-04811e7f8ffc",
   "metadata": {},
   "source": [
    "Here we are inputting only the page content from our document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7ea82-c58e-42ee-b01e-6aa51e324b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarizer(ex_file[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c10bd8-1570-4dfa-9b2a-999e3f149faf",
   "metadata": {},
   "source": [
    "### Gemini as a Image to Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476052b3-ff64-4e5a-819c-7a18daf7f413",
   "metadata": {},
   "source": [
    "Gemini Pro Vision can generate text from images and videos. These text can be descriptions or questions about the image or video. You can download an image or retrieve an image from your bucket or locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ee84f-f058-411a-944d-4e149cd0e9bc",
   "metadata": {},
   "source": [
    "Images can only be in the following formats: \n",
    "- PNG - image/png\n",
    "- JPEG - image/jpeg\n",
    "\n",
    "Our function below takes in a prompt and the image, we have also included a if statement to recognize if the function should use `Image` to load in a image locally or `Part` to load it from a bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501efefe-d52f-43b3-b4eb-3d3fe81f4a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img2text(image_path: str, img_prompt: str) -> str:\n",
    "    multimodal_model = GenerativeModel(\"gemini-pro-vision\")\n",
    "    if \"gs://\" in image_path:\n",
    "        image1=Part.from_uri(image_path, mime_type=\"image/jpeg\")\n",
    "    else: \n",
    "        image1=Image.load_from_file(image_path)\n",
    "        \n",
    "    responses = multimodal_model.generate_content(\n",
    "        [image1, img_prompt],\n",
    "        generation_config={\n",
    "            \"max_output_tokens\": 2048,\n",
    "            \"temperature\": 0.4,\n",
    "            \"top_p\": 1,\n",
    "            \"top_k\": 32\n",
    "        },\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in responses:\n",
    "        print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafaaa8-f3c7-472a-8f0b-6595d2112636",
   "metadata": {},
   "source": [
    "Lets look at an image locally, by loading a image first, this a image of a Covid virus from the [CDC Public Health Image Library](https://phil.cdc.gov/details.aspx?pid=23312)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939f105-89c2-4c38-80f8-2cddf8dcb0ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! wget -O example_image_covid.jpg \"https://phil.cdc.gov//PHIL_Images/23312/23312_lores.jpg\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d9fc7-0ca9-44a7-b431-a9698b1a636c",
   "metadata": {},
   "source": [
    "Now run our function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e81656-4943-439d-9fbe-df439e0e30df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(img2text(\"example_image_covid.jpg\", \"describe this image.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc152659-7bc4-46bc-9ead-4736b4ad2706",
   "metadata": {},
   "source": [
    "Next we'll look at an image from a bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81197d53-dd3d-4358-9835-ef513ec11d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(img2text(\"gs://generativeai-downloads/images/scones.jpg\", \"describe this image.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3c48f-66e9-40e1-8623-04f73b672507",
   "metadata": {},
   "source": [
    "We can even ask for more details related to the items in our image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8395784-ea68-4a95-a0bb-b3d618f68054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_prompt=\"How do you make whats in this image?\"\n",
    "image=\"gs://generativeai-downloads/images/scones.jpg\"\n",
    "print(img2text(image, img_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f7ba6-8236-437a-ba99-ea1d887efd64",
   "metadata": {},
   "source": [
    "### Gemini as a Video to Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbbd35-f36f-4467-8a1d-48ccd942d7cc",
   "metadata": {},
   "source": [
    "Just like images we will be using the same model Gemini Pro Vision. We can load videos locally and from a bucket just like images. Video files can only be in the following formats:\n",
    "- MOV - video/mov\n",
    "- MPEG - video/mpeg\n",
    "- MP4 - video/mp4\n",
    "- MPG - video/mpg\n",
    "- AVI - video/avi\n",
    "- WMV - video/wmv\n",
    "- MPEGPS - video/mpegps\n",
    "- FLS - video/flv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f94141-b5ba-49bf-a3d7-d0b68ccdd39a",
   "metadata": {},
   "source": [
    "Our function below takes a video from a public bucket and asks for a prompt and the location of the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18561c6-8f8f-46e4-b3ee-0d5fc96f2d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def video2text(video_path: str, video_prompt: str) -> str:\n",
    "    # Query the model\n",
    "    multimodal_model = GenerativeModel(\"gemini-pro-vision\")\n",
    "    response = multimodal_model.generate_content(\n",
    "        [\n",
    "            # Add an example image\n",
    "            Part.from_uri(\n",
    "                video_path, mime_type=\"video/mp4\"\n",
    "            ),\n",
    "            # Add an example query\n",
    "            video_prompt,\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in response :\n",
    "        return print(chunk.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f9100-b5a4-446c-a03d-dd89a1b4bbde",
   "metadata": {},
   "source": [
    "Run the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55990074-0365-45f5-9fa6-bedbe93c9932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_prompt = \"What is this video about in detail?\"\n",
    "video = \"gs://cloud-samples-data/video/Machine Learning Solving Problems Big, Small, and Prickly.mp4\"\n",
    "print(video2text(video, video_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32253b4-d892-4c0c-896e-320c8df479c7",
   "metadata": {},
   "source": [
    "## Gemini on Vertex AI Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614883f2-e53a-4ba5-855c-209d755a6e6f",
   "metadata": {},
   "source": [
    "You can also use Gemini Pro and Pro Vision in Vertex AI's playground called **Vertex AI Studio**. To locate Vertex AI Studio search Vertex AI and on the left hand side locate Vertex AI Studio as the image below shows. To utilize Gemini Pro Vision locate and click **Multimodal** you will have the option to use your own prompt or explore some of the other set prompts such as Extract text from images, image question answering , etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a634e11-3bd5-4048-b6e4-46d5aac6ce34",
   "metadata": {},
   "source": [
    "![Gemini1](../../images/Gemini_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3716d9d-49af-40c9-acce-757c53be9a12",
   "metadata": {},
   "source": [
    "For this tutorial we will select Open on the **Prompt Design** option. We will upload the COVID image we downloaded before by clicking **INSERT MEDIA** and selecting our file. Then we will ask it a question, here we asked \"Describe treatments for the item in this image\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72f9cd-0d06-47e4-b67c-45039372d967",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Gemini3](../../images/Gemini_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88f06b-770a-43be-b875-b4054c3ccbf3",
   "metadata": {},
   "source": [
    "To utilize Gemini Pro locate and click **Language** on the left side menu. You have the option to use a prompt or chat and if you would like to focus on text or code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952cd5c-c93d-428f-8036-ac658eebfba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Gemini2](../../images/Gemini_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810c0db-d145-4acd-a2cc-b6ce8231fa14",
   "metadata": {},
   "source": [
    "Here we picked the **TEXT CHAT** option and asked the bot to describe covid and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4635d-d8f3-42df-959d-dff92259813c",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Gemini4](../../images/Gemini_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc53b99-e6ed-45f1-a0e1-9b146e18e46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

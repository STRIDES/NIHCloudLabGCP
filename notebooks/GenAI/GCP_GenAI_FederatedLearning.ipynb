{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4dfe2b",
   "metadata": {},
   "source": [
    "## Running the Federate Learning process on Vertex AI \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "For this tutorial it is recommended to use -------(specify machine type) to speed up processes, this notebooks was run using the machinetype n1-highcpu-8 (8 vCPUs, 7.199 GB RAM) on Tensorflow. Visit the following tutorial to set up notebooks that utilize: [GPUs Spinning up a Vertex AI Notebook](https://github.com/STRIDES/NIHCloudLabGCP/blob/42ee2b7dbffce54e53a212d8c02ac16fd872c5be/docs/vertexai.md)\n",
    "\n",
    "## Learning Objectives\n",
    "* Learn how to adapt the federated learning process to Google Clouds' Vertex AI. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b403d",
   "metadata": {},
   "source": [
    "The training and validation datasets are currently stored in your working environment. Create a cloud storage bucket and push these csv files to the bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23024e",
   "metadata": {},
   "source": [
    "Create a Google cloud storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d363e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Conflict",
     "evalue": "409 POST https://storage.googleapis.com/storage/v1/b?project=cit-cl-odss-ramsivakumar-df75&prettyPrint=false: Your previous request to create the named bucket succeeded and you already own it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConflict\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Create bucket\u001b[39;00m\n\u001b[1;32m      7\u001b[0m bucket \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbucket(BUCKET)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mbucket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/bucket.py:976\u001b[0m, in \u001b[0;36mBucket.create\u001b[0;34m(self, client, project, location, predefined_acl, predefined_default_object_acl, enable_object_retention, timeout, retry)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates current bucket.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03mIf the bucket already exists, will raise\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    (Optional) How to retry the RPC. See: :ref:`configuring_retries`\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_require_client(client)\n\u001b[0;32m--> 976\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_bucket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_or_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_default_object_acl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_default_object_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_object_retention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_object_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/client.py:970\u001b[0m, in \u001b[0;36mClient.create_bucket\u001b[0;34m(self, bucket_or_name, requester_pays, project, user_project, location, data_locations, predefined_acl, predefined_default_object_acl, enable_object_retention, timeout, retry)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_locations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomPlacementConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataLocations\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_locations}\n\u001b[0;32m--> 970\u001b[0m api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post_resource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_target_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m bucket\u001b[38;5;241m.\u001b[39m_set_properties(api_response)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bucket\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/client.py:627\u001b[0m, in \u001b[0;36mClient._post_resource\u001b[0;34m(self, path, data, query_params, headers, timeout, retry, _target_object)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_resource\u001b[39m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    568\u001b[0m     path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m     _target_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    575\u001b[0m ):\n\u001b[1;32m    576\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper for bucket / blob methods making API 'POST' calls.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m            If the bucket is not found.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_target_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_target_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/_http.py:72\u001b[0m, in \u001b[0;36mConnection.api_request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry:\n\u001b[1;32m     71\u001b[0m         call \u001b[38;5;241m=\u001b[39m retry(call)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mConflict\u001b[0m: 409 POST https://storage.googleapis.com/storage/v1/b?project=cit-cl-odss-ramsivakumar-df75&prettyPrint=false: Your previous request to create the named bucket succeeded and you already own it."
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "BUCKET='federated-learning-resources'\n",
    "client = storage.Client()\n",
    "\n",
    "#Create bucket\n",
    "bucket = client.bucket(BUCKET)\n",
    "bucket.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843872c",
   "metadata": {},
   "source": [
    "Push the training and validation datasets to the storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c326c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "#push train dataset to GCS bucket\n",
    "csv_buffer = BytesIO()\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET)\n",
    "bucket.blob('full_train_data.csv').upload_from_file(csv_buffer, 'text/csv')\n",
    "\n",
    "#push val dataset to GCS bucket\n",
    "csv_buffer = BytesIO()\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET)\n",
    "bucket.blob('full_val_data.csv').upload_from_file(csv_buffer, 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c2982",
   "metadata": {},
   "source": [
    "Store the path of these files in their respective variable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7adba6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to path\n",
    "training_input_path = f'gs://{BUCKET}/full_train_data.csv'\n",
    "\n",
    "# save validation_dataset to gc bucket\n",
    "validation_input_path = f'gs://{BUCKET}/full_val_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4d514",
   "metadata": {},
   "source": [
    "Create a python script called task.py in a folder called \"scripts\". This script adapts the code snippets run above to work with resources from cloud storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4a2e4-2609-4de2-8804-bb9fbb003eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Creates the following directories and files\n",
    "```\n",
    "# Importing Libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import Sequential\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "\n",
    "# BreastCancerDataset Class\n",
    "class BreastCancerDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        scaler = StandardScaler()\n",
    "        self.X = torch.tensor(scaler.fit_transform(df.iloc[:,1:-1].values))   # first (ID) and last (diagnosis) columns are excluded\n",
    "        self.y =  torch.tensor(df.iloc[:,-1].values)                          # load the diagnosis (malignant=1, benign=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Function to Load Data from GCS \n",
    "# Description: This function stores downloads files from cloud storage and reads them into a pandas dataframe. \n",
    "def load_dataset_from_gcs(bucket_name, file_path):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "    data = blob.download_as_string()\n",
    "    df = pd.read_csv(BytesIO(data))\n",
    "    return df\n",
    "\n",
    "# Client Class (same as above)\n",
    "class Client:\n",
    "    def __init__(self, name, model, train_loader, val_loader, optimizer, criterion):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.metrics = dict({\"train_acc\": list(), \"train_loss\": list(), \"val_acc\": list(), \"val_loss\": list()})\n",
    "\n",
    "        print(f\"[INFO] Initialized client '{self.name}' with {len(train_loader.dataset)} train and {len(val_loader.dataset)} validation samples\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            Trains the model of the client for 1 epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        correct_predictions = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # iterate over training dataset\n",
    "        for inputs, labels in self.train_loader:\n",
    "            # make predictions\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "            # apply gradient\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calculate number of correct predictions\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / len(self.train_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"train_acc\"].append(accuracy)\n",
    "        self.metrics[\"train_loss\"].append(epoch_loss)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "            Validates the model of the client based on the given validation data loader.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # iterate over validation data loader and make predictions\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = correct_predictions / len(self.val_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"val_acc\"].append(accuracy)\n",
    "        self.metrics[\"val_loss\"].append(average_loss)\n",
    "\n",
    "# SimpleNN Model Definition (same as above)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_input):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.NN = Sequential(\n",
    "            nn.Linear(n_input, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.NN(x)\n",
    "        return logits\n",
    "\n",
    "# FedAvg Function (same as above)\n",
    "\n",
    "def fed_avg(global_state_dict, client_states, n_data_points):\n",
    "    \"\"\"\n",
    "    Averages the weights of client models to update the global model by FedAvg.\n",
    "\n",
    "    Args:\n",
    "        global_state_dict: The state dict of the global PyTorch model.\n",
    "        client_states: A list of PyTorch models state dicts representing client models.\n",
    "        n_data_points: A list with the number of data points per client.\n",
    "\n",
    "    Returns:\n",
    "        The state dict of the updated global PyTorch model.\n",
    "    \"\"\"\n",
    "    averaged_state_dict = OrderedDict()\n",
    "\n",
    "    for key in global_state_dict.keys():\n",
    "        for state, n in zip(client_states, n_data_points):\n",
    "            averaged_state_dict[key] =+ state[key] * (n/ sum(n_data_points))\n",
    "\n",
    "    return averaged_state_dict\n",
    "\n",
    "# FLServer Class\n",
    "class FLServer:\n",
    "    def __init__(self, model, clients):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.n_data_points = [len(client.train_loader.dataset) for client in self.clients]\n",
    "\n",
    "    def run(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            print(f\"Epoch {i}\")\n",
    "\n",
    "            # Step 2 of figure at the beginning of the tutorial\n",
    "            for client in self.clients:\n",
    "                client.train()\n",
    "\n",
    "            # aggregate the models using FedAvg (Step 3 & 4 of figure at the beginning of the tutorial)\n",
    "            client_states = [client.model.state_dict() for client in self.clients]                 # Step 3\n",
    "            aggregated_state = fed_avg(self.model.state_dict(), client_states, self.n_data_points) # Step 4\n",
    "            self.model.load_state_dict(aggregated_state)\n",
    "\n",
    "            # redistribute central model (Step 1 of figure at the beginning of the tutorial)\n",
    "            for client in fl_server.clients:\n",
    "                client.model.load_state_dict(aggregated_state)\n",
    "\n",
    "            # run validation of aggregated model\n",
    "            for client in self.clients:\n",
    "                client.validate()\n",
    "\n",
    "            # repeat for n epochs (Step 5 of figure at the beginning of the tutorial)\n",
    "\n",
    "# Plotting Metrics\n",
    "def plot_metrics(client):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for k, v in client.metrics.items():\n",
    "        x_vals = range(len(v))\n",
    "        plt.plot(x_vals, v, label=k)\n",
    "\n",
    "    plt.ylim(bottom=0.0, top=1.0)\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.title(client.name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Running Prediction on validation data \n",
    "def run_prediction(model, bucket_name, validation_file_path):\n",
    "    model.eval()\n",
    "    val_df = load_dataset_from_gcs(bucket_name, validation_file_path)\n",
    "    val_data = BreastCancerDataset(val_df)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / len(val_dataloader.dataset)\n",
    "    print(f\"{accuracy:.2f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    import argparse\n",
    "    #arguments are parsed from the command line\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--bucket_name', type=str, required=True, help='GCS bucket name')\n",
    "    parser.add_argument('--train_file', type=str, required=True, help='Path to the training file in GCS')\n",
    "    parser.add_argument('--validation_file', type=str, required=True, help='Path to the validation file in GCS')\n",
    "    parser.add_argument('--output_dir', type=str, required=True, help='Output directory for the model in GCS')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train')\n",
    "    parser.add_argument('--batch_size', type=int, default=50, help='Batch size for training')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load datasets from GCS\n",
    "    train_df = load_dataset_from_gcs(args.bucket_name, args.train_file)\n",
    "    val_df = load_dataset_from_gcs(args.bucket_name, args.validation_file)\n",
    "\n",
    "    train_data = BreastCancerDataset(train_df)\n",
    "    val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model and client for centralized training\n",
    "    model = SimpleNN(n_input=30)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.BCELoss()\n",
    "    central_client = Client(\"central\", model, train_dataloader, val_dataloader, optimizer, criterion)\n",
    "\n",
    "    # Centralized training\n",
    "    for i in range(args.epochs):\n",
    "        print(f\"Epoch {i}\")\n",
    "        central_client.train()\n",
    "        central_client.validate()\n",
    "\n",
    "    plot_metrics(central_client)\n",
    "\n",
    "    print(\"Accuracy of the centrally trained model:\")\n",
    "    run_prediction(central_client.model, args.bucket_name, args.test_file)\n",
    "\n",
    "    # Federated Learning\n",
    "    fed_model = SimpleNN(n_input=30)\n",
    "    clients = list()\n",
    "    for i in range(2):\n",
    "        train_df = load_dataset_from_gcs(args.bucket_name, f\"client_{i}/train_data.csv\")\n",
    "        val_df = load_dataset_from_gcs(args.bucket_name, f\"client_{i}/val_data.csv\")\n",
    "\n",
    "        train_data = BreastCancerDataset(train_df)\n",
    "        val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=7, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.SGD(fed_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        clients.append(Client(f\"client_{i}\", fed_model, train_dataloader, val_dataloader, optimizer, criterion))\n",
    "\n",
    "    fl_server = FLServer(fed_model, clients)\n",
    "\n",
    "    for client in fl_server.clients:\n",
    "        client.model.load_state_dict(fl_server.model.state_dict())\n",
    "\n",
    "    fl_server.run(epochs=args.epochs)\n",
    "\n",
    "    for client in fl_server.clients:\n",
    "        plot_metrics(client)\n",
    "\n",
    "    print(\"Model trained with federated learning accuracy:\")\n",
    "    run_prediction(fl_server.model, args.bucket_name, args.test_file)\n",
    "\n",
    "    # Save the model to GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(args.bucket_name)\n",
    "    model_path = os.path.join(args.output_dir, \"fed_model.pth\")\n",
    "    torch.save(fed_model.state_dict(), model_path)\n",
    "    bucket.blob(model_path).upload_from_filename(model_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fa8cd",
   "metadata": {},
   "source": [
    "Create a file called \"setup.py\" in your working directory. When this script is run, it will create a python package that contains all of the dependcies of your run. \n",
    "\n",
    "\n",
    "```python\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='breast_cancer_federated_learning',\n",
    "    version='0.1',\n",
    "    install_requires=[\n",
    "        'torch',\n",
    "        'numpy',\n",
    "        'pandas',\n",
    "        'matplotlib',\n",
    "        'scikit-learn',\n",
    "        'google-cloud-storage',\n",
    "        'google-cloud-aiplatform',\n",
    "    ],\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Breast Cancer Federated Learning Training Script',\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e05ef3",
   "metadata": {},
   "source": [
    "Run setup.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee905aab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating breast_cancer_federated_learning.egg-info\n",
      "writing breast_cancer_federated_learning.egg-info/PKG-INFO\n",
      "writing dependency_links to breast_cancer_federated_learning.egg-info/dependency_links.txt\n",
      "writing requirements to breast_cancer_federated_learning.egg-info/requires.txt\n",
      "writing top-level names to breast_cancer_federated_learning.egg-info/top_level.txt\n",
      "writing manifest file 'breast_cancer_federated_learning.egg-info/SOURCES.txt'\n",
      "reading manifest file 'breast_cancer_federated_learning.egg-info/SOURCES.txt'\n",
      "writing manifest file 'breast_cancer_federated_learning.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "creating breast_cancer_federated_learning-0.1\n",
      "creating breast_cancer_federated_learning-0.1/breast_cancer_federated_learning.egg-info\n",
      "copying files to breast_cancer_federated_learning-0.1...\n",
      "copying breast_cancer_federated_learning.egg-info/PKG-INFO -> breast_cancer_federated_learning-0.1/breast_cancer_federated_learning.egg-info\n",
      "copying breast_cancer_federated_learning.egg-info/SOURCES.txt -> breast_cancer_federated_learning-0.1/breast_cancer_federated_learning.egg-info\n",
      "copying breast_cancer_federated_learning.egg-info/dependency_links.txt -> breast_cancer_federated_learning-0.1/breast_cancer_federated_learning.egg-info\n",
      "copying breast_cancer_federated_learning.egg-info/requires.txt -> breast_cancer_federated_learning-0.1/breast_cancer_federated_learning.egg-info\n",
      "copying breast_cancer_federated_learning.egg-info/top_level.txt -> breast_cancer_federated_learning-0.1/breast_cancer_federated_learning.egg-info\n",
      "copying breast_cancer_federated_learning.egg-info/SOURCES.txt -> breast_cancer_federated_learning-0.1/breast_cancer_federated_learning.egg-info\n",
      "Writing breast_cancer_federated_learning-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'breast_cancer_federated_learning-0.1' (and everything under it)\n",
      "running bdist_wheel\n",
      "running build\n",
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:79: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to build/bdist.linux-x86_64/wheel\n",
      "running install\n",
      "running install_egg_info\n",
      "Copying breast_cancer_federated_learning.egg-info to build/bdist.linux-x86_64/wheel/./breast_cancer_federated_learning-0.1-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating build/bdist.linux-x86_64/wheel/breast_cancer_federated_learning-0.1.dist-info/WHEEL\n",
      "creating 'dist/breast_cancer_federated_learning-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "adding 'breast_cancer_federated_learning-0.1.dist-info/METADATA'\n",
      "adding 'breast_cancer_federated_learning-0.1.dist-info/WHEEL'\n",
      "adding 'breast_cancer_federated_learning-0.1.dist-info/top_level.txt'\n",
      "adding 'breast_cancer_federated_learning-0.1.dist-info/RECORD'\n",
      "removing build/bdist.linux-x86_64/wheel\n"
     ]
    }
   ],
   "source": [
    "! python3 ./scripts/setup.py sdist bdist_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896e983",
   "metadata": {},
   "source": [
    "Copy the python package to your cloud storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad4e66c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/breast_cancer_federated_learning-0.1-py3-none-any.whl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dist/breast_cancer_federated_learning-0.1-py3-none-any.whl gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3580b0b2",
   "metadata": {},
   "source": [
    "Submit the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b18f4bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai.custom-jobs.create) unrecognized arguments:\n",
      "  --python-module=scripts/task.py\n",
      "  --python-version=3.7 (did you mean '--python-package-uris'?)\n",
      "  --runtime-version=2.4\n",
      "  --machine-type=n1-standard-4\n",
      "  To search the help text of gcloud commands, run:\n",
      "  gcloud help -- SEARCH_TERMS\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs create \\\n",
    "  --region=us-central1 \\\n",
    "  --display-name=breast-cancer-federated-learning-job \\\n",
    "  --python-package-uris=gs://{BUCKET}/breast_cancer_federated_learning-0.1-py3-none-any.whl \\\n",
    "  --python-module=scripts/task.py \\\n",
    "  --args=\"--bucket_name=gs://{BUCKET},--val_file=gs://gs://{BUCKET}full_val_data.csv,--train_file=gs://{BUCKET}/full_train_data.csv,--output_dir=${BUCKET}/output,--epochs=10,--batch_size=7\" \\\n",
    "  --python-version=3.7 \\\n",
    "  --runtime-version=2.4 \\\n",
    "  --machine-type=n1-standard-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1dbf0",
   "metadata": {},
   "source": [
    "Upload the model to the Vertex AI Model Registry. This will allow you to deploy endpoints using the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c145de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TF_PREDICTION_IMAGE_URI_RUNTIME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m vertexai\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mproject, location\u001b[38;5;241m=\u001b[39mlocation, staging_bucket\u001b[38;5;241m=\u001b[39mBUCKET)\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mupload(\n\u001b[1;32m     16\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mMODEL_DISPLAY_NAME,\n\u001b[1;32m     17\u001b[0m     description\u001b[38;5;241m=\u001b[39mMODEL_DESCRIPTION,\n\u001b[0;32m---> 18\u001b[0m     serving_container_image_uri\u001b[38;5;241m=\u001b[39m\u001b[43mTF_PREDICTION_IMAGE_URI_RUNTIME\u001b[49m,\n\u001b[1;32m     19\u001b[0m     serving_container_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--allow_precompilation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--allow_compression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--use_tfrt\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m     artifact_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/saved_model_artifacts_tf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m#directory where our artifacts are in our bucket\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TF_PREDICTION_IMAGE_URI_RUNTIME' is not defined"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "#give your model a name\n",
    "MODEL_DISPLAY_NAME = \"federated-learning-model\"\n",
    "MODEL_DESCRIPTION = \"Runs federated learning process\" \n",
    "\n",
    "#add your project ID and location\n",
    "project=''\n",
    "location=''\n",
    "\n",
    "vertexai.init(project=project, location=location, staging_bucket=BUCKET)\n",
    "\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    description=MODEL_DESCRIPTION,\n",
    "    serving_container_image_uri=TF_PREDICTION_IMAGE_URI_RUNTIME,\n",
    "    serving_container_args=[\"--allow_precompilation\", \"--allow_compression\", \"--use_tfrt\"],\n",
    "    artifact_uri=f'gs://{BUCKET}/saved_model_artifacts_tf', #directory where our artifacts are in our bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3527ad",
   "metadata": {},
   "source": [
    "Create an endpoint and deploy it to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_DISPLAY_NAME = \"fl-endpoint\" \n",
    "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
    "\n",
    "model_endpoint = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count=1,\n",
    "    traffic_percentage=100,\n",
    "    deploy_request_timeout=1200,\n",
    "    sync=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-4.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-4:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

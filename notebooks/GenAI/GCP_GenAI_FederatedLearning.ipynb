{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4dfe2b",
   "metadata": {},
   "source": [
    "## Running the Federate Learning process on Vertex AI \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "For this tutorial it is recommended to use -------(specify machine type) to speed up processes, this notebooks was run using the machinetype n1-highcpu-8 (8 vCPUs, 7.199 GB RAM) on Tensorflow. Visit the following tutorial to set up notebooks that utilize: [GPUs Spinning up a Vertex AI Notebook](https://github.com/STRIDES/NIHCloudLabGCP/blob/42ee2b7dbffce54e53a212d8c02ac16fd872c5be/docs/vertexai.md)\n",
    "\n",
    "## Learning Objectives\n",
    "* Learn how to adapt the federated learning process to Google Clouds' Vertex AI. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b403d",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "The training and validation datasets are currently stored in your working environment. Create a cloud storage bucket and push these csv files to the bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23024e",
   "metadata": {},
   "source": [
    "Create a Google cloud storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d363e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "BUCKET='federated-learning-resources'\n",
    "client = storage.Client()\n",
    "\n",
    "#Create bucket\n",
    "bucket = client.bucket(BUCKET)\n",
    "bucket.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843872c",
   "metadata": {},
   "source": [
    "Push the training and validation datasets to the storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c326c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "#push train dataset to GCS bucket\n",
    "csv_buffer = BytesIO()\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET)\n",
    "bucket.blob('full_train_data.csv').upload_from_file(csv_buffer, 'text/csv')\n",
    "\n",
    "#push val dataset to GCS bucket\n",
    "csv_buffer = BytesIO()\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET)\n",
    "bucket.blob('full_val_data.csv').upload_from_file(csv_buffer, 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c2982",
   "metadata": {},
   "source": [
    "Store the path of these files in their respective variable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adba6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to path\n",
    "training_input_path = f'gs://{BUCKET}/full_train_data.csv'\n",
    "\n",
    "# save validation_dataset to gc bucket\n",
    "validation_input_path = f'gs://{BUCKET}/full_val_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4d514",
   "metadata": {},
   "source": [
    "Create a python script called task.py in a folder called \"scripts\". This script adapts the code snippets run above to work with resources from cloud storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4a2e4-2609-4de2-8804-bb9fbb003eb5",
   "metadata": {},
   "source": [
    "#Creates the following directories and files\n",
    "```\n",
    "# Importing Libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import Sequential\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "\n",
    "# BreastCancerDataset Class\n",
    "class BreastCancerDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        scaler = StandardScaler()\n",
    "        self.X = torch.tensor(scaler.fit_transform(df.iloc[:,1:-1].values))   # first (ID) and last (diagnosis) columns are excluded\n",
    "        self.y =  torch.tensor(df.iloc[:,-1].values)                          # load the diagnosis (malignant=1, benign=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Function to Load Data from GCS \n",
    "# Description: This function stores downloads files from cloud storage and reads them into a pandas dataframe. \n",
    "def load_dataset_from_gcs(bucket_name, file_path):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "    data = blob.download_as_string()\n",
    "    df = pd.read_csv(BytesIO(data))\n",
    "    return df\n",
    "\n",
    "# Client Class (same as above)\n",
    "class Client:\n",
    "    def __init__(self, name, model, train_loader, val_loader, optimizer, criterion):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.metrics = dict({\"train_acc\": list(), \"train_loss\": list(), \"val_acc\": list(), \"val_loss\": list()})\n",
    "\n",
    "        print(f\"[INFO] Initialized client '{self.name}' with {len(train_loader.dataset)} train and {len(val_loader.dataset)} validation samples\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            Trains the model of the client for 1 epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        correct_predictions = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # iterate over training dataset\n",
    "        for inputs, labels in self.train_loader:\n",
    "            # make predictions\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "            # apply gradient\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calculate number of correct predictions\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / len(self.train_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"train_acc\"].append(accuracy)\n",
    "        self.metrics[\"train_loss\"].append(epoch_loss)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "            Validates the model of the client based on the given validation data loader.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # iterate over validation data loader and make predictions\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = correct_predictions / len(self.val_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"val_acc\"].append(accuracy)\n",
    "        self.metrics[\"val_loss\"].append(average_loss)\n",
    "\n",
    "# SimpleNN Model Definition (same as above)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_input):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.NN = Sequential(\n",
    "            nn.Linear(n_input, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.NN(x)\n",
    "        return logits\n",
    "\n",
    "# FedAvg Function (same as above)\n",
    "\n",
    "def fed_avg(global_state_dict, client_states, n_data_points):\n",
    "    \"\"\"\n",
    "    Averages the weights of client models to update the global model by FedAvg.\n",
    "\n",
    "    Args:\n",
    "        global_state_dict: The state dict of the global PyTorch model.\n",
    "        client_states: A list of PyTorch models state dicts representing client models.\n",
    "        n_data_points: A list with the number of data points per client.\n",
    "\n",
    "    Returns:\n",
    "        The state dict of the updated global PyTorch model.\n",
    "    \"\"\"\n",
    "    averaged_state_dict = OrderedDict()\n",
    "\n",
    "    for key in global_state_dict.keys():\n",
    "        for state, n in zip(client_states, n_data_points):\n",
    "            averaged_state_dict[key] =+ state[key] * (n/ sum(n_data_points))\n",
    "\n",
    "    return averaged_state_dict\n",
    "\n",
    "# FLServer Class\n",
    "class FLServer:\n",
    "    def __init__(self, model, clients):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.n_data_points = [len(client.train_loader.dataset) for client in self.clients]\n",
    "\n",
    "    def run(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            print(f\"Epoch {i}\")\n",
    "\n",
    "            # Step 2 of figure at the beginning of the tutorial\n",
    "            for client in self.clients:\n",
    "                client.train()\n",
    "\n",
    "            # aggregate the models using FedAvg (Step 3 & 4 of figure at the beginning of the tutorial)\n",
    "            client_states = [client.model.state_dict() for client in self.clients]                 # Step 3\n",
    "            aggregated_state = fed_avg(self.model.state_dict(), client_states, self.n_data_points) # Step 4\n",
    "            self.model.load_state_dict(aggregated_state)\n",
    "\n",
    "            # redistribute central model (Step 1 of figure at the beginning of the tutorial)\n",
    "            for client in fl_server.clients:\n",
    "                client.model.load_state_dict(aggregated_state)\n",
    "\n",
    "            # run validation of aggregated model\n",
    "            for client in self.clients:\n",
    "                client.validate()\n",
    "\n",
    "            # repeat for n epochs (Step 5 of figure at the beginning of the tutorial)\n",
    "\n",
    "# Plotting Metrics\n",
    "def plot_metrics(client):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for k, v in client.metrics.items():\n",
    "        x_vals = range(len(v))\n",
    "        plt.plot(x_vals, v, label=k)\n",
    "\n",
    "    plt.ylim(bottom=0.0, top=1.0)\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.title(client.name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Running Prediction on validation data \n",
    "def run_prediction(model, bucket_name, validation_file_path):\n",
    "    model.eval()\n",
    "    val_df = load_dataset_from_gcs(bucket_name, validation_file_path)\n",
    "    val_data = BreastCancerDataset(val_df)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / len(val_dataloader.dataset)\n",
    "    print(f\"{accuracy:.2f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    import argparse\n",
    "    #arguments are parsed from the command line\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--bucket_name', type=str, required=True, help='GCS bucket name')\n",
    "    parser.add_argument('--train_file', type=str, required=True, help='Path to the training file in GCS')\n",
    "    parser.add_argument('--validation_file', type=str, required=True, help='Path to the validation file in GCS')\n",
    "    parser.add_argument('--output_dir', type=str, required=True, help='Output directory for the model in GCS')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train')\n",
    "    parser.add_argument('--batch_size', type=int, default=50, help='Batch size for training')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load datasets from GCS\n",
    "    train_df = load_dataset_from_gcs(args.bucket_name, args.train_file)\n",
    "    val_df = load_dataset_from_gcs(args.bucket_name, args.validation_file)\n",
    "\n",
    "    train_data = BreastCancerDataset(train_df)\n",
    "    val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model and client for centralized training\n",
    "    model = SimpleNN(n_input=30)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.BCELoss()\n",
    "    central_client = Client(\"central\", model, train_dataloader, val_dataloader, optimizer, criterion)\n",
    "\n",
    "    # Centralized training\n",
    "    for i in range(args.epochs):\n",
    "        print(f\"Epoch {i}\")\n",
    "        central_client.train()\n",
    "        central_client.validate()\n",
    "\n",
    "    plot_metrics(central_client)\n",
    "\n",
    "    print(\"Accuracy of the centrally trained model:\")\n",
    "    run_prediction(central_client.model, args.bucket_name, args.test_file)\n",
    "\n",
    "    # Federated Learning\n",
    "    fed_model = SimpleNN(n_input=30)\n",
    "    clients = list()\n",
    "    for i in range(2):\n",
    "        train_df = load_dataset_from_gcs(args.bucket_name, f\"client_{i}/train_data.csv\")\n",
    "        val_df = load_dataset_from_gcs(args.bucket_name, f\"client_{i}/val_data.csv\")\n",
    "\n",
    "        train_data = BreastCancerDataset(train_df)\n",
    "        val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=7, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.SGD(fed_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        clients.append(Client(f\"client_{i}\", fed_model, train_dataloader, val_dataloader, optimizer, criterion))\n",
    "\n",
    "    fl_server = FLServer(fed_model, clients)\n",
    "\n",
    "    for client in fl_server.clients:\n",
    "        client.model.load_state_dict(fl_server.model.state_dict())\n",
    "\n",
    "    fl_server.run(epochs=args.epochs)\n",
    "\n",
    "    for client in fl_server.clients:\n",
    "        plot_metrics(client)\n",
    "\n",
    "    print(\"Model trained with federated learning accuracy:\")\n",
    "    run_prediction(fl_server.model, args.bucket_name, args.test_file)\n",
    "\n",
    "    # Save the model to GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(args.bucket_name)\n",
    "    model_path = os.path.join(args.output_dir, \"fed_model.pth\")\n",
    "    torch.save(fed_model.state_dict(), model_path)\n",
    "    bucket.blob(model_path).upload_from_filename(model_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fa8cd",
   "metadata": {},
   "source": [
    "Create a file called \"setup.py\" in your working directory. When this script is run, it will create a python package that contains all of the dependcies of your run. \n",
    "\n",
    "\n",
    "```python\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='breast_cancer_federated_learning',\n",
    "    version='0.1',\n",
    "    install_requires=[\n",
    "        'torch',\n",
    "        'numpy',\n",
    "        'pandas',\n",
    "        'matplotlib',\n",
    "        'scikit-learn',\n",
    "        'google-cloud-storage',\n",
    "        'google-cloud-aiplatform',\n",
    "    ],\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Breast Cancer Federated Learning Training Script',\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e05ef3",
   "metadata": {},
   "source": [
    "Run setup.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee905aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ./scripts/setup.py sdist bdist_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896e983",
   "metadata": {},
   "source": [
    "Copy the python package to your cloud storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp dist/breast_cancer_federated_learning-0.1-py3-none-any.whl gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3580b0b2",
   "metadata": {},
   "source": [
    "Submit the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai custom-jobs create \\\n",
    "  --region=us-central1 \\\n",
    "  --display-name=breast-cancer-federated-learning-job \\\n",
    "  --python-package-uris=gs://{BUCKET}/breast_cancer_federated_learning-0.1-py3-none-any.whl \\\n",
    "  --python-module=scripts/task.py \\\n",
    "  --args=\"--bucket_name=gs://{BUCKET},--val_file=gs://gs://{BUCKET}full_val_data.csv,--train_file=gs://{BUCKET}/full_train_data.csv,--output_dir=${BUCKET}/output,--epochs=10,--batch_size=7\" \\\n",
    "  --python-version=3.7 \\\n",
    "  --runtime-version=2.4 \\\n",
    "  --machine-type=n1-standard-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1dbf0",
   "metadata": {},
   "source": [
    "Upload the model to the Vertex AI Model Registry. This will allow you to deploy endpoints using the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c145de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "#give your model a name\n",
    "MODEL_DISPLAY_NAME = \"federated-learning-model\"\n",
    "MODEL_DESCRIPTION = \"Runs federated learning process\" \n",
    "\n",
    "#add your project ID and location\n",
    "project=''\n",
    "location=''\n",
    "\n",
    "vertexai.init(project=project, location=location, staging_bucket=BUCKET)\n",
    "\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    description=MODEL_DESCRIPTION,\n",
    "    serving_container_image_uri=TF_PREDICTION_IMAGE_URI_RUNTIME,\n",
    "    serving_container_args=[\"--allow_precompilation\", \"--allow_compression\", \"--use_tfrt\"],\n",
    "    artifact_uri=f'gs://{BUCKET}/saved_model_artifacts_tf', #directory where our artifacts are in our bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3527ad",
   "metadata": {},
   "source": [
    "Create an endpoint and deploy it to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_DISPLAY_NAME = \"fl-endpoint\" \n",
    "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
    "\n",
    "model_endpoint = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count=1,\n",
    "    traffic_percentage=100,\n",
    "    deploy_request_timeout=1200,\n",
    "    sync=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

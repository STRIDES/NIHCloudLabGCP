{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2edc6187-82ae-44e2-852f-2ad2712c93aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a PubMed Chatbot on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1948e88",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecea2ad-7c65-4367-87e1-b021167c3a1d",
   "metadata": {},
   "source": [
    "For this tutorial we create a PubMed chatbot that will answer questions by gathering information from documents we have provided via an index. The model we will be using today is the pretrained 'gemini-2.0-flash' model from GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded04856",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Introduce langchain\n",
    "- Explain the differences between zero-shot, one-shot, and few-shot prompting\n",
    "- Practice using different document retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654c440",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "You need access to Verted AI. If you want to deploy a model (see below) then follow those instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561612b6",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01e74b-b5b4-4be9-b16e-ec55419318ef",
   "metadata": {},
   "source": [
    "### Optional: Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd13e7-afc9-416b-94dc-418a93e14587",
   "metadata": {},
   "source": [
    "In this tutorial we will be using a Google's gemini model **gemini-2.0-flash** which doesn't need to be deployed but if you would like to use another model you choose one from the **Model Garden** using the console which will allow you to add a model to your model registry, create an endpoint (or use an existing one), and deploy the model all in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e3ab1-5f7e-4028-a66f-9619926a2afd",
   "metadata": {},
   "source": [
    "### PubMed API vs RAG with Vertex AI Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a820eea-1538-4f40-86c4-eb14fe09e127",
   "metadata": {},
   "source": [
    "Our chatbot will rely on documents to answer our questions to do so we are supplying it a **vector index**. A vector index or index is a data structure that enables fast and accurate search and retrieval of vector embeddings from a large dataset of objects. We will be working with two options for our index: PubMed API vs RAG Vertex AI Vector Search method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314b115-9433-460d-b275-78aa50f0a858",
   "metadata": {},
   "source": [
    "**What is the difference?**\n",
    "\n",
    "The **PubMed API** is provided free by langchain to connect your model to more than **35 million citations** for biomedical literature from MEDLINE, life science journals, and online books. The langchain package for PubMed is already a retriever meaning that just simply using this tool will our chatbot beable to retrieve documents to refer to. \n",
    "\n",
    "**Vertex AI Vector Search** (formally known as Matching Engine) is a vector store from GCP that allows the user more **security and control** on which documents you wish to supply to your model. Vector Search, formerly known as Vertex AI Matching Engine, is a vector store or database that stores the **embeddings** of your documents and the metadata. Because this is not a retriever we have to make it so for our model to send back an output that also tells us which documents it is referencing, this is where RAG comes in. **RAG** stands for **Retrieval-augmented generation** it is a method or technique that **indexes documents** by first loading them in, splitting them into chucks (making it easier for our model to search for relevant splits), embedding the splits, then storing them in a vector store. The next steps in RAG are based on the question you ask your chatbot. If we were to ask it \"What is a cell?\" the vector store will be searched by a retriever to find relevant splits that have to do with our question, thus **retrieving relevant documents**. And finally our chatbot will **generate an answer** that makes sense of what a cell is, as part of the answer it will also point out which source documents it used to create the answer.\n",
    "\n",
    "We will be exploring both methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e2160-660a-40cd-886d-e4179fbe6c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install langchain langchain-google-vertexai langchain-community unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1690d-e93d-4cd3-89c6-8d06b5a071a8",
   "metadata": {},
   "source": [
    "### Setting up Vertex AI Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6330ddf-7972-4451-9fcb-98cf83f5d118",
   "metadata": {},
   "source": [
    "If you choose to use the RAG method with Vertex AI RAG Vector Search to supply documents to your model follow the instructions below:\n",
    "\n",
    "Set your project id, location, and bucket variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb694dc4-9e76-4091-9ddf-cd4eca816851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id='PROJECT_ID'\n",
    "location='REGION'\n",
    "bucket = 'UNIQUE_BUCKET_NAME'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02053f4d-fad7-44ab-a7c3-cfa1c218240f",
   "metadata": {},
   "source": [
    "### Gathering our Docs For our Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c9de7-4a06-4f85-b9ff-c8c9e51f8c70",
   "metadata": {},
   "source": [
    "AWS marketplace has PubMed database named **PubMed CentralÂ® (PMC)** that contains free full-text archive of biomedical and life sciences journal article at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM). We will be subsetting this database to add documents to our Vertex AI Vector Search Index. Ensure that you have the correct permissions to allow your environment to connect to buckets and Vertex AI.\n",
    "\n",
    "The first step will be to create a bucket that we will later use as our data source for our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d49432-cf03-4f19-aa82-ef7f8bad5bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make bucket\n",
    "!gsutil mb -l {location} gs://{bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad30ba-cee8-47f9-bc1e-ece8961ac66a",
   "metadata": {},
   "source": [
    "We will then download the metadata file from the PMC index directory, this will list all of the articles within the PMC bucket and their paths. We will use this to subset the database into our own bucket. Here we are using curl to connect to the public AWS s3 bucket where the metadata and documents are originally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b395e34-062d-4f77-afee-3601d471954a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#download the metadata file\n",
    "!curl -O http://pmc-oa-opendata.s3.amazonaws.com/oa_comm/txt/metadata/csv/oa_comm.filelist.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a8595a-767f-4cad-9273-62d8e2cf60d1",
   "metadata": {},
   "source": [
    "We only want the metadata of the first 50 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b0f29-2b07-43a6-800d-4aa5e957fe52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the file as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('oa_comm.filelist.csv')\n",
    "#first 50 files\n",
    "first_50=df[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1ae93-450e-4c79-83cc-ea46a1b507c1",
   "metadata": {},
   "source": [
    "Lets look at our metadata! We can see that the bucket path to the files are under the **Key** column this is what we will use to loop through the PMC bucket and copy the first 50 files to our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77b2aa-ed1b-4d27-8163-fdaa7a304582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_50.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5f36a-239c-4c15-80ab-f896d45849d3",
   "metadata": {},
   "source": [
    "The following commands will gather the location of each document with in AWS s3 bucket, output the text from the docs as bytes and save the bytes to our bucket in the form of a text file in a directory named \"docs\". This will all be done using curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cca24a-59d0-4dc7-b887-c8cc8547774f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def upload_blob_from_memory(bucket_name, contents, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_string(contents)\n",
    "\n",
    "    return print(\n",
    "        f\"{destination_blob_name} uploaded to {bucket_name}.\"\n",
    "    )\n",
    "\n",
    "for i in first_50['Key']:\n",
    "    doc_name=i.split(r'/')[-1]\n",
    "    x = requests.get(f'https://pmc-oa-opendata.s3.amazonaws.com/{i}')\n",
    "    doc = x.text\n",
    "    upload_blob_from_memory(bucket, doc, f'docs/{doc_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b396c8-baa9-44d6-948c-2326dc514839",
   "metadata": {},
   "source": [
    "### Creating an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fa941-bf59-4cae-9aa8-2f2741f3a1b1",
   "metadata": {},
   "source": [
    "To create our vector store index, we will first start by creating a dummy embeddings file. An index holds a set of records so our dummy data will be the first record and then later we will add our PubMed docs to the same index. Inorder for Vector Search to find our dummy embeddings file it too must be in our bucket and we will add it to the subdirectory 'init_index'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5092c-23f3-4f28-9308-f34b8d90c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "import json\n",
    "init_embedding = {\"id\": str(uuid.uuid4()), \"embedding\": list(np.zeros(768))}\n",
    "\n",
    "# dump embedding to a local file\n",
    "with open(\"embeddings_0.json\", \"w\") as f:\n",
    "    json.dump(init_embedding, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a4c42-dc17-48a3-a0bb-0cbea527ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move inital embeddings file to bucket\n",
    "!gsutil cp embeddings_0.json gs://{bucket}/init_index/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1a3cd-4f89-4271-b025-71af2bf25095",
   "metadata": {},
   "source": [
    "Now we can make our index, this can take up to 30min to 1hr. \n",
    "\n",
    "Please note that the dimensions depend on what text embedding model you are using for this tutorial we are using **Vertex AI's embedding model** which uses 768 dimensions. If you choose to change your model, choose an embedding model that is compatible with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa7bba-3d15-4a3f-86c2-59d2c92a95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "# create Index\n",
    "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name = f\"pubmed_vector_index\",\n",
    "    contents_delta_uri = f\"gs://{bucket}/init_index\",\n",
    "    dimensions = 768,\n",
    "    approximate_neighbors_count = 150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    location=location\n",
    "    \n",
    ")\n",
    "\n",
    "#save index id\n",
    "index_id=index.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65b0cc-cff3-47d6-af8c-7c39b2418ecb",
   "metadata": {},
   "source": [
    "### Creating an Endpoint and Deploying our Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3aa4a2-1145-475a-bd04-33bf69551751",
   "metadata": {},
   "source": [
    "We will create a public endpoint for our vector store, you can also create a private one by setting up a VPC and specifying the VPC id for the params 'network'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55596202-13b9-4e35-8099-0602a2b13e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the endpoint\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name = \"pubmed_vector_endpoint\",\n",
    "    public_endpoint_enabled = True,\n",
    "    location = location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f771328-31c6-4da2-9d7d-8a548abd12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save endpoint id\n",
    "endpoint_id = index_endpoint.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79b19a-ca28-4610-824a-44f5bc6b72ab",
   "metadata": {},
   "source": [
    "Here we are deploying our index to our endpoint, which can take up to a hour. Its also okay if this cell stops or gets interrupted because the actions are carried out in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51412f2f-f32b-44a9-93bc-3e2f6185cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy our index to our endpoint\n",
    "deployed_index_id=\"deployed_pubmed_vector_index\"\n",
    "index_endpoint = index_endpoint.deploy_index(\n",
    "    index=index, deployed_index_id=deployed_index_id\n",
    ")\n",
    "index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cef7d-d0aa-42a8-a46e-7fd1f5c48c3b",
   "metadata": {},
   "source": [
    "### Adding Metadata to Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa34e7b-99c7-4a2e-b73b-146636a98285",
   "metadata": {},
   "source": [
    "After we have our documents stored in our bucket we can start to load our files back. This step is necessary though redundant because we will need to embed our docs for our vector store and we can attach metadata for each document. The first step of adding our metadata to the docs will be to remove the 'Key' column because this is no longer the location of our documents. Next, we'll convert the rest of the columns into a dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9016f15-db02-4073-b4c7-288d919bbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Remove the Key column to be replaced later\n",
    "first_50.pop('Key')\n",
    "#convert the metadata to dict\n",
    "first_50_dict = first_50.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80dce6-dc5b-4a73-8591-572be35c092a",
   "metadata": {},
   "source": [
    "Lets look at our metadata now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce004e-ab8d-4b9c-91d8-9320e1679fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_50_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a607a48-31b8-4081-a347-bb1528f8e725",
   "metadata": {},
   "source": [
    "Now we can load in our documents, add in the location of our docs in our bucket and the document name to our metadata, and finally attach that metadata to our documents. At the end we should have 50 documents before splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47170e83-3e9e-48e6-ab0f-cabdd39507e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add metadata\n",
    "from langchain_community.document_loaders import GCSDirectoryLoader\n",
    "print(f\"Processing documents from {bucket}\")\n",
    "loader = GCSDirectoryLoader(\n",
    "    project_name=project_id, bucket=bucket, prefix = 'docs'\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# loop through docs to add metadata to each one\n",
    "for i in range(50):\n",
    "    doc_md = documents[i].metadata\n",
    "    document_name = doc_md[\"source\"].split(\"/\")[-1]\n",
    "    source = f\"{bucket}/docs/{document_name}\"\n",
    "    # Add document name and source to the metadata\n",
    "    documents[i].metadata = {\"source\": source, \"document_name\": document_name}\n",
    "    documents[i].metadata.update(first_50_dict[i])# attached other metadata to doc\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb96ea-dd0c-47b7-9556-05e25c3efb1d",
   "metadata": {},
   "source": [
    "Lets take a look at our metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673a445-7c2e-4650-91fa-4b0b38196e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b65c0-fa38-456f-acb3-d406803ef204",
   "metadata": {},
   "source": [
    "### Splitting our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812ecaf-979f-4537-b420-071022a7b917",
   "metadata": {},
   "source": [
    "Splitting our data into chucks will help our vector store parse through our data faster and efficiently. We'll then add the chuck number to our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6503cf-02e5-4352-a6b1-13ef4e01c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add chunk number to metadata\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "print(f\"# of documents = {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb202a-6122-4083-81e4-ddcb33499e64",
   "metadata": {},
   "source": [
    "After splitting our data we now have 7620 documents. And looking at our metadata we can see that the chunk number is the last entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1036b8e-6c7f-43be-83b7-5b9e61628003",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d5b85-950e-4a44-b3fa-a2dcec7df036",
   "metadata": {},
   "source": [
    "### Embedding our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68b14e-0cf5-4973-90c1-0eee0c8bc8c9",
   "metadata": {},
   "source": [
    "Now we can embed our text into **numerical vectors** that will help our model find similar objects like documents that hold similar texts or find similar photos based on the numbers assigned to the object. Depending on the model you choose you have to find an embedder that is compatible to our model. Since we are using a PaLM2 model (text-bison) we can use the embedding model from Vertex AI that defaults to using **'textembedding-gecko'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4a98c-a332-469f-9a24-ce5abff23b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-005\")\n",
    "\n",
    "# initialize vector store\n",
    "vector_store = VectorSearchVectorStore.from_components(\n",
    "    project_id=project_id,\n",
    "    region=location,\n",
    "    gcs_bucket_name=bucket,\n",
    "    embedding=embeddings,\n",
    "    index_id=index_id,\n",
    "    endpoint_id=endpoint_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bfb5b-a3a6-4156-bca3-394774a94565",
   "metadata": {},
   "source": [
    "For our split documents to be read by our embedding model we need to make tuple called **Document** that contains **page content** and **metadata**. The code below loops through the split docs and assigns them to the label page_content and the same is done for all parts of our metadata under the label metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda4699-5c46-49bb-97e3-059199254bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store docs as embeddings in Matching Engine index\n",
    "# It may take a while since API is rate limited\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [doc.metadata for doc in doc_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f6ef6-b488-45d3-ac4c-2aca0d6eab56",
   "metadata": {},
   "source": [
    "lets look at our Document tuple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1af269-fdbb-4db5-9c1b-41e21d304b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb0bba-8ccd-4828-a7a2-6f34b03a03b9",
   "metadata": {},
   "source": [
    "Now we can add our split documents and their metadata to our vector store. This is the longest step of the tutorial and can take up 1hr to complete. As you wait you can read up on Creating a Inference Script section of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2f21b-06ab-470e-8807-638548d50f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = vector_store.add_texts(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b90f92-f223-42e0-9e5b-accd3fdfbeea",
   "metadata": {},
   "source": [
    "Test whether search from vector store is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd9aab-7f08-4a69-b7e4-9cd1d8f9110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=vector_store.similarity_search_with_score(\"brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3bc6b-8c43-476f-a662-abda830dc2da",
   "metadata": {},
   "source": [
    "### Creating an Interactive Inference Script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2291e-109e-4120-ad10-5dbfd341a07b",
   "metadata": {},
   "source": [
    "For us to submit queries and receive responses from our chatbot we need to create an **inference script** that will format inputs in a way that the chatbot can understand and format outputs in a way we can understand. We will also be supplying instructions to the chatbot through this script.\n",
    "\n",
    "Our script will utilize **LangChain** tools and packages to enable our model to:\n",
    "- **Connect to sources of context** (e.g. providing our model with tasks and examples)\n",
    "- **Rely on reason** (e.g. instruct our model how to answer based on provided context)\n",
    "\n",
    "**Warning**: The following tools must be installed via your terminal `pip install \"langchain\" \"xmltodict\" \"langchain-google-vertexai\" \"langchain-community\" \"unstructured\"` and the inference script must be run on the terminal via the command `python YOUR_SCRIPT.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad374085-c4b1-4083-85a5-90cba35846d6",
   "metadata": {},
   "source": [
    "The first part of our script will be to list all the tools that are required. \n",
    "-  **PubMedRetriever:** Utilizes the langchain retriever tool to specifically retrieve PubMed documents from the PubMed API.\n",
    "- **MatchingEngine:** Connects to Vertex AI Vector Search to be used as a langchain retriever tool to specifically retrieve embedded documents stored in your bucket. \n",
    "- **ConversationalRetrievalChain:** Allows the user to construct a conversation with the model and retrieves the outputs while sending inputs to the model.\n",
    "- **PromptTemplate:** Allows the user to prompt the model to provide instructions, best method for zero and few shot prompting\n",
    "- **VertexAIEmbeddings:** Text embedding model used before to convert text to numerical vectors.\n",
    "- **VertexAI**: Package used to import Google PaLM2 LLMs models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ad48d-c6c8-421a-a48b-88e979d15b57",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain_community.retrievers import PubMedRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain_google_vertexai import VertexAIModelGarden\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f4c31-71cd-4f39-8bfc-de098bdbaafc",
   "metadata": {},
   "source": [
    "Second will build a class that will hold the functions we need to send inputs and retrieve outputs from our model. For the beginning of our class we will establish some colors to our text conversation with our chatbot which we will utilize later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbb901-f811-4b8e-a956-4c8c7f914ae2",
   "metadata": {},
   "source": [
    "```python\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba36d057-5189-4075-a243-18996c6fc932",
   "metadata": {},
   "source": [
    "If you are using Vector Search instead of the PubMed API we need to create a function that will gather the necessary information to connect to our model, which will be the:\n",
    "- Project ID\n",
    "- Location of bucket and vector store (they should be in the same location)\n",
    "- Bucket name\n",
    "- Vector Store Index ID\n",
    "- Vector Store Endpoint ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a244a-7e71-40d3-ae78-8e166dd3c7ee",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_chain():\n",
    "    PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "    LOCATION_ID = os.environ[\"LOCATION_ID\"]\n",
    "    #ENDPOINT_ID = os.environ[\"ENDPOINT_ID\"] #uncomment if utilizing model from Model Garden\n",
    "    BUCKET = os.environ[\"BUCKET\"]\n",
    "    VC_INDEX_ID = os.environ[\"VC_INDEX_ID\"]\n",
    "    VC_ENDPOINT_ID = os.environ[\"VC_ENDPOINT_ID\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1012f-ed20-47b9-9162-924e03e836d5",
   "metadata": {},
   "source": [
    "Now we can define our Google PaLM2 model being `gemini-2.0-flash` and other parameters:\n",
    "\n",
    "- Max Output Tokens: Limit of tokens outputted by the model.\n",
    "- Temperature: Controls randomness, higher values increase diversity meaning a more unique response make the model to think harder. Must be a number from 0 to 1, 0 being less unique.\n",
    "- Top_p (nucleus): The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus. Must be a number from 0 to 1.\n",
    "- Top_k: Sample from the k most likely next tokens at each step. Lower k focuses on higher probability tokens. This means the model choses the most probable words. Lower values eliminate fewer coherent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadb1af-2c46-4ab1-92f9-6e0861f83324",
   "metadata": {},
   "source": [
    "```python\n",
    "llm = VertexAI(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    "    \n",
    "    \n",
    "#if using a model from the Model Garden uncomment\n",
    "#llm = VertexAIModelGarden(project=PROJECT_ID, endpoint_id=ENDPOINT_ID, location=LOCATION_ID)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b4f91-0c64-459b-a6e9-8a955c0797c7",
   "metadata": {},
   "source": [
    "We specify what our retriever both the PubMed and Vector Search retriever are listed, please only add one per script.\n",
    "\n",
    "If using Vector Search we need to initialize our vector store as we did before when we added our split documents and metadata to it. Then we set the vector store as a **retriever** with the search type being **'similarity'** meaning it will find texts that are similar to each other depending on the question you ask the model. We also set **'k'** to 3 meaning that our retriever will retrieve 3 documents that are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c61724-23d3-4b49-8c72-cbd208bdb5df",
   "metadata": {},
   "source": [
    "```python\n",
    "retriever= PubMedRetriever()\n",
    "\n",
    "#only if using Vector Search as a retriever\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-005\") #Make sure embedding model is compatible with model\n",
    "\n",
    "vector_store = VectorSearchVectorStore.from_components(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=LOCATION_ID,\n",
    "        gcs_bucket_name=BUCKET,\n",
    "        embedding=embeddings,\n",
    "        index_id=VC_INDEX_ID,\n",
    "        endpoint_id=VC_ENDPOINT_ID\n",
    "    )\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\":3}\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e464a-0931-444a-aa58-09ee0c4c9884",
   "metadata": {},
   "source": [
    "Here we are constructing our **prompt_template**, this is where we can try zero-shot or few-shot prompting. Only add one method per script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431051e-0e84-408e-9821-f50a9b88c9c1",
   "metadata": {},
   "source": [
    "#### Zero-shot prompting\n",
    "\n",
    "Zero-shot prompting does not require any additional training more so it gives a pre-trained language model a task or query to generate text (our output). The model relies on its general language understanding and the patterns it has learned during its training to produce relevant output. In our script we have connect our model to a **retriever** to make sure it gathers information from that retriever (this can be the PubMed API or Vector Search). \n",
    "\n",
    "See below that the task is more like instructions notifying our model they will be asked questions which it will answer based on the info of the scientific documents provided from the index provided (this can be the PubMed API or Vector Search index). All of this information is established as a **prompt template** for our model to receive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0316dc5-6274-4a5e-92e4-3d266ed6a4df",
   "metadata": {},
   "source": [
    "```python\n",
    "prompt_template = \"\"\"\n",
    "  Ignore everything before.\n",
    "  \n",
    "  Instruction:\n",
    "  Instructions:\n",
    "  I will provide you with research papers on a specific topic in English, and you will create a cumulative summary. \n",
    "  The summary should be concise and should accurately and objectively communicate the takeaway of the papers related to the topic. \n",
    "  You should not include any personal opinions or interpretations in your summary, but rather focus on objectively presenting the information from the papers. \n",
    "  Your summary should be written in your own words and ensure that your summary is clear, concise, and accurately reflects the content of the original papers. First, provide a concise summary then citations at the end.\n",
    "  \n",
    "  {question} Answer \"don't know\" if not present in the document. \n",
    "  {context}\n",
    "  Solution:\"\"\"\n",
    "  PROMPT = PromptTemplate(\n",
    "      template=prompt_template, input_variables=[\"context\", \"question\"],\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe7032-8507-4d07-baab-1b3bf0e92074",
   "metadata": {},
   "source": [
    "#### One-shot and Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614ea04-e1f8-4941-ae16-4359f718f98f",
   "metadata": {},
   "source": [
    "One and few shot prompting are similar to one-shot prompting, in addition to giving our model a task just like before we have also supplied an example of how the our model structure our output.\n",
    "\n",
    "See below that we have implemented one-shot prompting to our script.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb9669-5b77-4d9b-9f4e-a0d3a18b0fae",
   "metadata": {},
   "source": [
    "```python\n",
    "prompt_template = \"\"\"\n",
    "  Instructions:\n",
    "  I will provide you with research papers on a specific topic in English, and you will create a cumulative summary. \n",
    "  The summary should be concise and should accurately and objectively communicate the takeaway of the papers related to the topic. \n",
    "  You should not include any personal opinions or interpretations in your summary, but rather focus on objectively presenting the information from the papers. \n",
    "  Your summary should be written in your own words and ensure that your summary is clear, concise, and accurately reflects the content of the original papers. First, provide a concise summary then citations at the end. \n",
    "  Examples:\n",
    "  Question: What is a cell?\n",
    "  Answer: '''\n",
    "  Cell, in biology, the basic membrane-bound unit that contains the fundamental molecules of life and of which all living things are composed. \n",
    "  Sources: \n",
    "  Chow, Christopher , Laskey, Ronald A. , Cooper, John A. , Alberts, Bruce M. , Staehelin, L. Andrew , \n",
    "  Stein, Wilfred D. , Bernfield, Merton R. , Lodish, Harvey F. , Cuffe, Michael and Slack, Jonathan M.W.. \n",
    "  \"cell\". Encyclopedia Britannica, 26 Sep. 2023, https://www.britannica.com/science/cell-biology. Accessed 9 November 2023.\n",
    "  '''\n",
    "  \n",
    "  {question} Answer \"don't know\" if not present in the document. \n",
    "  {context}\n",
    "  \n",
    "\n",
    "  \n",
    "  Solution:\"\"\"\n",
    "  PROMPT = PromptTemplate(\n",
    "      template=prompt_template, input_variables=[\"context\", \"question\"],\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c66d53-97b2-46dc-a466-70a3d3bee4a7",
   "metadata": {},
   "source": [
    "The following set of commands control the chat history essentially telling the model to expect another question after it finishes answering the previous one. Follow up questions can contain references to past chat history so the **ConversationalRetrievalChain** combines the chat history and the followup question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question-answering chain to return a response.\n",
    "\n",
    "All of these pieces such as our conversational chain, prompt, and chat history are passed through a function called **run_chain** so that our model can return is response. We have also set the length of our chat history to one meaning that our model can only refer to the pervious conversation as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4d33b-60f2-4462-a8e6-bbce7f8a7b07",
   "metadata": {},
   "source": [
    "```python\n",
    "condense_qa_template = \"\"\"\n",
    "  Chat History:\n",
    "  {chat_history}\n",
    "  Here is a new question for you: {question}\n",
    "  Standalone question:\"\"\"\n",
    "  standalone_question_prompt = PromptTemplate.from_template(condense_qa_template)\n",
    " \n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        retriever=retriever, \n",
    "        condense_question_prompt=standalone_question_prompt, \n",
    "        return_source_documents=True, \n",
    "        combine_docs_chain_kwargs={\"prompt\":PROMPT},\n",
    "        )\n",
    "      return qa\n",
    "\n",
    "def run_chain(chain, prompt: str, history=[]):\n",
    "    print(prompt)\n",
    "    return chain({\"question\": prompt, \"chat_history\": history})\n",
    "\n",
    "MAX_HISTORY_LENGTH = 1 #increase to refer to more pervious chats\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1ef8d-66fe-4f84-933b-af2d730bd114",
   "metadata": {},
   "source": [
    "The final part of our script utilizes our class and incorporates colors to add a bit of flare to our conversation with our model. The model when first initialized should greet the user asking **\"Hello! How can I help you?\"** then instructs the user to ask a question or exit the session **\"Ask a question, start a New search: or CTRL-D to exit.\"**. With every question submitted to the model it is labeled as a **new search** we then run the run_chain function to get the models response or answer and add the response to the **chat history**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6ef65-ced4-445e-875c-7fee3483b81d",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "  chat_history = []\n",
    "  qa = build_chain()\n",
    "  print(bcolors.OKBLUE + \"Hello! How can I help you?\" + bcolors.ENDC)\n",
    "  print(bcolors.OKCYAN + \"Ask a question, start a New search: or CTRL-D to exit.\" + bcolors.ENDC)\n",
    "  print(\">\", end=\" \", flush=True)\n",
    "  for query in sys.stdin:\n",
    "    if (query.strip().lower().startswith(\"new search:\")):\n",
    "      query = query.strip().lower().replace(\"new search:\",\"\")\n",
    "      chat_history = []\n",
    "    elif (len(chat_history) == MAX_HISTORY_LENGTH):\n",
    "      chat_history.pop(0)\n",
    "    result = run_chain(qa, query, chat_history)\n",
    "    chat_history.append((query, result[\"answer\"]))\n",
    "    print(bcolors.OKGREEN + result['answer'] + bcolors.ENDC)  \n",
    "    if 'source_documents' in result: \n",
    "      print(bcolors.OKGREEN + 'Sources:')\n",
    "      for idx, ref in enumerate(result[\"source_documents\"]):\n",
    "            print(ref.page_content) #Use this for Vector store\n",
    "            #print(\"PubMed UID: \"+ref.metadata[\"uid\"])#Use this for PubMed retriever\n",
    "    print(bcolors.ENDC)\n",
    "    print(bcolors.OKCYAN + \"Ask a question, start a New search: or CTRL-D to exit.\" + bcolors.ENDC)\n",
    "    print(\">\", end=\" \", flush=True)\n",
    "  print(bcolors.OKBLUE + \"Bye\" + bcolors.ENDC)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcbd48-bb84-4310-b8eb-ad87850a8649",
   "metadata": {},
   "source": [
    "Running our script in the terminal will require us to export the following global variables before using the command `python NAME_OF_SCRIPT.py`. You can also check out our **example inference scripts** for the [Pubmed API](/example_scripts/example_langchain_chat_llama_2_zeroshot.py) and [Vertex AI Vector Search](/example_scripts/example_vectorsearch_chat_llama_2_zeroshot.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97df23-6893-438d-8a67-cb7dbf83e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreive our index and endpoint id\n",
    "print(index_id)\n",
    "print(endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab00a3-54ff-4873-8d25-eaf8bd18a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter the global variables in your terminal\n",
    "export PROJECT_ID='<PROJECT_ID>' \\\n",
    "export LOCATION_ID='<LOCATION_ID>' \\\n",
    "#export ENDPOINT_ID='<MODEL_ENDPOINT_ID>' \\ #Uncomment if using model from Model Garden\n",
    "export BUCKET='<BUCKET_NAME>' \\\n",
    "export VC_INDEX_ID='<VECTOR_SEARCH_INDEX ID>' \\\n",
    "export VC_ENDPOINT_ID='VECTOR_SEARCH_ENDPOINT_ID>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe127e6-c0b1-4e07-ad56-38c30a9bf858",
   "metadata": {},
   "source": [
    "You should see similar results on the terminal. In this example we ask the chatbot to explain brain cancer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8fb4b-e74f-4e8d-892b-0f913eff747d",
   "metadata": {},
   "source": [
    "![PubMed Chatbot Results](../../images/GCP_chatbot_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32a128",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Here you learned how to deploy and interact with a chat model and also how to deploy an inference script to create an interactive chatbot in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178c1c6-368a-48c5-8beb-278443b685a2",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec06a34-dc47-453f-b519-424804fa2748",
   "metadata": {},
   "source": [
    "**Warning:** Dont forget to delete the resources we just made to avoid accruing additional costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307bb17-757a-4579-a0d8-698eb1bb3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undeploy index\n",
    "!gcloud ai index-endpoints undeploy-index {endpoint_id} \\\n",
    "  --deployed-index-id={deployed_index_id} \\\n",
    "  --project={project_id} \\\n",
    "  --region={location}\n",
    "\n",
    "\n",
    "#Delete index and endpoint\n",
    "!gcloud ai indexes delete {index_id} \\\n",
    "  --project={project_id} \\\n",
    "  --region={location} --quiet\n",
    "\n",
    "!gcloud ai index-endpoints delete {endpoint_id} \\\n",
    "  --project={project_id} \\\n",
    "  --region={location} --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cea0a-a8fc-494e-8ce4-afb65847a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete bucket\n",
    "!gcloud storage rm --recursive gs://{bucket}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928d95d-d7ec-43f6-9135-79fcfc9520d9",
   "metadata": {},
   "source": [
    "If you have imported a model and deployed it don't forget to delete the model from the Model Registry and delete the endpoint."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

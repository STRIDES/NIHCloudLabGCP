{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1651316c",
   "metadata": {},
   "source": [
    "# Download sequence data from the NCBI Sequence Read Archive (SRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15022f97",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "DNA sequence data are typically deposited into the NCBI Sequence Read Archive, and can be accessed through the SRA website, or via a collection of command line tools called SRA Toolkit. Individual sequence entries are assigned an Accession ID, which can be used to find and download a particular file. For example, if you go to the [SRA database](https://www.ncbi.nlm.nih.gov/sra) in a browser window, and search for `SRX15695630`, you should see an entry for _C. elegans_. Alternatively, you can search the SRA metadata dataset in BigQuery to generate a list of accession numbers. Here we are going to generate a list of accessions using Big Query, use tools from the SRA Toolkit to download a few fastq files, then copy those fastq files to a cloud bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b857c5",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ Learn more about the Sequence Read Archive\n",
    "+ Learn how to download SRA data locally\n",
    "+ Learn how to interact with SRA metadata via BigQuery Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38a212",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Make sure you have enabled the [BigQuery API](https://cloud.google.com/endpoints/docs/openapi/enable-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69f333",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b500763",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01213dae",
   "metadata": {},
   "source": [
    "Install dependencies, using mamba (you could also use conda). At the time of writing, the version of SRA tools available with the Anaconda distribution was v.2.11.0. If you want to install the latest version, download and install from [here](https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit). If you do the direct install, you will also need to configure interactively following [this guide](https://github.com/ncbi/sra-tools/wiki/05.-Toolkit-Configuration), you can do that by opening a terminal and running the commands there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bb6f1-05ee-4215-94e3-8365f7550688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mamba install -c bioconda -c conda-forge sra-tools==2.11.0 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c010c2",
   "metadata": {},
   "source": [
    "Test that your install works and that fasterq-dump is available in your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e507538",
   "metadata": {},
   "outputs": [],
   "source": [
    "! fasterq-dump -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3de21-bb3d-4d38-bcb1-30f5b10c9d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz && \\\n",
    "tar -xvzf sratoolkit.current-ubuntu64.tar.gz && \\\n",
    "export PATH=$PATH:$(pwd)/sratoolkit.*-ubuntu64/bin && \\\n",
    "fasterq-dump --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d3512-5307-42f7-9405-495fe1ca5be2",
   "metadata": {},
   "source": [
    "### 2) Setup Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3265f75-ebf7-4d90-8f4d-a486df5cb693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e4215",
   "metadata": {},
   "source": [
    "Set up your directory structure for the raw fastq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa93698-a082-4c11-9d48-0abe775fbcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p data data/fasterqdump/raw_fastq data/prefetch_fasterqdump/raw_fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068c9da-7814-4b24-9ff8-12473048bdcf",
   "metadata": {},
   "source": [
    "### 3) Create Accession List using BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30c134-c903-4605-82d4-babdaaae30c0",
   "metadata": {},
   "source": [
    "Here we use BigQuery to generate a list of accessions. You can also generate a manual list by searching the [SRA Database](https://www.ncbi.nlm.nih.gov/sra) and saving to a file or list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3ec94-16cd-43b6-a4c9-d56aa593382e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the biquery api\n",
    "from google.cloud import bigquery\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fb23b-ec7e-423f-a531-9f39a1954087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Designate the client for the API\n",
    "client = bigquery.Client(location=\"US\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df380099-d6ee-43b5-8719-66b7a7b41916",
   "metadata": {},
   "source": [
    "Let's download bacterial samples, one of which happens to come from a swab of a sea horse (which we tell you for no particular reason!). You could change the SQL query as you like, feel free to take a look at the generated df, and then play with different parameters. For more inspiration, look at this [SRA tutorial](https://www.ncbi.nlm.nih.gov/sra/docs/sra-bigquery-examples/) or other links to SRA examples can be found in our [README](https://github.com/STRIDES/NIHCloudLabGCP?tab=readme-ov-file#download-data-from-the-sequence-read-archive-sra-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8d3f3-6a18-40a0-b9fe-7ab4c285b7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT *\n",
    "FROM `nih-sra-datastore.sra.metadata`\n",
    "WHERE organism = 'Mycobacteroides chelonae' \n",
    "limit 3\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049fdce-7ed7-43f4-a4d5-6811dd590dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe9c32-817b-4e9c-901f-d27b98b95db2",
   "metadata": {},
   "source": [
    "As you can see, most of what you need to know is shown in this data frame. If you wanted to just show the accession, you could replace the * for acc in the SELECT command. One other thing to think about, is how large are these files, and do you have space on your VM to download them? You can figure this out by looking at the 'jattr' column, and then converting the number of bites to GB, then add that for a few samples to get a ballpark figure. If you need more space, stop the VM, go Compute Engine and either [resize your disk](https://cloud.google.com/compute/docs/disks/resize-persistent-disk) or add a disk. You can see the amount of space on your disk from the command line using `!df -h .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca8b00-a467-4e70-a63e-faf6c53f6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['jattr'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d464a-ea2d-42d8-bb01-ee601e7a68cb",
   "metadata": {},
   "source": [
    "You can also get the same info using `vdb-dump --info <ACCESSION>` although that command may not always work as expected. You can also get the path for the sra compressed file in a bucket using `srapath <ACCESSION>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a04b7-4bd5-40e5-9860-65c0c0d48283",
   "metadata": {},
   "source": [
    "Save our accession list to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693f4f3-4ebf-41d7-95d6-8a23456897d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('list_of_accessionIDS.txt', 'w') as f:\n",
    "    accs = df['acc'].to_string(header=False, index=False)\n",
    "    f.write(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bda88c-1263-497f-bb9c-b949a8ff5272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat list_of_accessionIDS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f13c3-6b62-4408-84d8-ebad27c2eedb",
   "metadata": {},
   "source": [
    "### Download FASTQ Files with fasterq dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e261a3-54f4-4c30-8aee-4808afdc6251",
   "metadata": {},
   "source": [
    "Fasterq-dump is the replacement for the legacy fastq-dump tool. You can read [this guide](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump) to see the full details on this tool. You can also run `fasterq-dump -h` to see most of the options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f7b67",
   "metadata": {},
   "source": [
    "Fasterq dump doesn't run in batch mode, so one way to run a command on multiple samples is by using a for loop. There are many options you can explore, but here we are running -O for outdir, -e for the number of threads, -m for memory (4GB), and --location for the location we want to retrieve the file from. Depending on the type of cloud storage, it may be faster to select `NCBI` for the location. You may consider running a few tests with one or two of your accession numbers before downloading a whole batch. The default number of threads = 6, so adjust -e based on your machine size. For large files, you may also benefit from a machine type with more memory and/or threads. You may need to stop this VM, resize it, then restart and come back. There are also a bunch of ways to split your fastq files (defined [here](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump)) but the default of `split 3` will split into forward, reverse, and unpaired reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96307376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "! for x in `cat list_of_accessionIDS.txt`; do fasterq-dump -f -O data/raw_fastq -e 8 -m 4G $x ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e26c03-eeb9-4340-89e7-1eb82c9e32bb",
   "metadata": {},
   "source": [
    "### Download FASTQ files with prefetch + fasterq dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef5310-539e-4d21-bd5d-05bf41986c47",
   "metadata": {},
   "source": [
    "Using the example bacterial data, fasterq dump takes about 3.5 min to download the files. Under the hood, fasterq dump is pulling the compressed sra files from the database and converting them on the fly, which is slow (ish) because it has to do a lot over the network. A better, though less advertised method, is to disaggregate these functions using prefetch to pull the compressed files, then fasterq-dump to convert them locally, rather than over the network. For this to work, you need to either give the path to the prefetch directories in your text file, or make sure you cd into the raw_fastq dir so that it can find those directories with the .sra files. In this case, --location GCP is a lot faster than NCBI, but feel free to run your own tests with different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5fd57-2cb0-42f7-88c5-4f2f7ea4a1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "! prefetch --option-file list_of_accessionIDS.txt -O data/prefetch_fasterqdump/raw_fastq/ -f yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dafb175-67ea-4b70-a4f5-5e98737cece2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls data/prefetch_fasterqdump/raw_fastq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02bb95-c9b0-494a-a187-c1b955f2788e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "! for x in `cat list_of_accessionIDS.txt`; do fasterq-dump -f -O data/prefetch_fasterqdump/raw_fastq/ -e 8 -m 4G data/prefetch_fasterqdump/raw_fastq/$x; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5b0c8-ffb0-4e44-9178-f80e701d16cb",
   "metadata": {},
   "source": [
    "Comparing the two methods, we can see that fasterq-dump on its own took 3.5 min, whereas prefetch + fasterq-dump takes less than 40 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d262cb-d49f-4e33-b461-e4f5b9c778b7",
   "metadata": {},
   "source": [
    "### Copy Files to a Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9e5a6-db09-4923-a9df-feb2cd6d5e13",
   "metadata": {},
   "source": [
    "Create a new bucket, or give the path to an existing bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6703fae-2752-4717-93be-fd8d3e0b41d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil mb gs://cloud-lab-tutorials_sra/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d5b92-80dc-4d99-8f6f-88200eb98815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls data/prefetch_fasterqdump/raw_fastq/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2ea15-b0fb-4ce2-8093-972d348c988c",
   "metadata": {},
   "source": [
    "Using `-m` allows multithreading, `-r` would allow for recursive copy of a directory, but here we are just giving the path to fastq files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ff514-bbac-4974-afeb-9eb847ba857f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil -m cp data/prefetch_fasterqdump/raw_fastq/*.fastq gs://cloud-lab-tutorials_sra/raw_fastq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97b47b-830d-49ee-8add-c2c9fd4e41d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil ls gs://cloud-lab-tutorials_sra/raw_fastq/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0437556",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Here we learned about SRA, queried SRA metadata in BigQuery, and then downloaded data using SRA Toolkit. We found that the fastest download method was to use a combination of prefetch + fasterq-dump."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8979484-117f-465d-b134-36372a8c8bfc",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Make sure you shut down this VM, or delete it if you don't plan to use if further."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4dfe2b",
   "metadata": {},
   "source": [
    "# Running Federated Learning Locally and on GCP's Vertex AI \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ce8e7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Federated Learning is a privacy preserving technique to train machine learning models.\n",
    "When dealing with secure data (e.g. patient data) it is often restricted to merge datasets from different institutions (clients).\n",
    "Federated Learning enables the training of a machine learning model without sharing the underlying data.\n",
    "This is achieved by training local models at each client and only aggreagating the model weights. Instead of sending raw data to a central server, each client trains the model locally on its own data and only shares the model's updates (e.g., weights or gradients) with the central server. The server aggregates these updates (e.g., using algorithms like Federated Averaging, or FedAvg) to improve the global model.\n",
    "\n",
    "This approach is particularly useful in scenarios where data privacy, security, or bandwidth constraints make it impractical to centralize data.\n",
    "\n",
    "In this tutorial we will focus on centralized horizontal Federated Learning. Centralized FL has a coordinating server that controls the learning process and aggreagates the model weights [1].\n",
    "Horizontal FL means that the same features are available on each client (e.g. images) [2].\n",
    "The pendant to that would be vertical FL where different features are present, but for the same sample (e.g. patient).\n",
    "\n",
    "This is the general overview of a federated learning training process.\n",
    "The image was taken from the [NVIDIA blog](https://blogs.nvidia.com/blog/what-is-federated-learning/) and slightly modified.\n",
    "\n",
    "<img src=\"../../images/federated_learning_animation_still_white.png\" width=\"800\">\n",
    "\n",
    "The training process can be split up in [5 steps](https://flower.ai/docs/framework/tutorial-series-what-is-federated-learning.html):\n",
    "\n",
    "0. Initialize the global model\n",
    "1. Send global model to clients\n",
    "2. Local training\n",
    "3. Return model updates to coordinator\n",
    "4. Aggregate model updates by averaging (FedAvg)\n",
    "5. Repeat steps 1 to 4 until convergence\n",
    "\n",
    "We have extended these steps to 7 to allow a more detailed approach to our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34a8be",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebooks was run using the machinetype n1-highcpu-8 (8 vCPUs, 7.199 GB RAM) on Pytorch. Ensure Vertex AI and Cloud Storage APIs are enabled. Visit the following tutorial to set up notebooks that utilize: [GPUs Spinning up a Vertex AI Notebook](https://github.com/STRIDES/NIHCloudLabGCP/blob/42ee2b7dbffce54e53a212d8c02ac16fd872c5be/docs/vertexai.md) for faster speeds if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1d5af",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Understand Federated Learning \n",
    "* Learn to created a Centralized training and Federated Learning workflows locally\n",
    "* Evaluate and visualize model performance Centralized training vs. Federated Learning\n",
    "* Learn how to adapt the federated learning process to Google Clouds' Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b403d",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1607b6f",
   "metadata": {},
   "source": [
    "Install the following packages. In this tutorial we are using the **Pytorch Kernel** in a Vertex AI Workbench Jupyter Notebook which has Pytorch preinstalled. If you are not using the same setting you can install the rest of the needed packages by running `pip install torch pandas scikit-learn matplotlib ordereddict`.\n",
    "\n",
    "The kfp package will allow us to complie the functions that we are about to make into a pipeline which we will use in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e263073",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-aiplatform kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df176e7",
   "metadata": {},
   "source": [
    "Import our packages and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ea42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kfp import compiler\n",
    "from torch.nn import Sequential\n",
    "from collections import OrderedDict\n",
    "from google.cloud import aiplatform\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from kfp.dsl import component, pipeline, Output, Dataset, Model, Input, Artifact, Metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d3aee",
   "metadata": {},
   "source": [
    "### Centralized Training\n",
    "\n",
    "As a first step we demonstrate the training of a ML model through a traditional, centralized training. Although this is not a prerequisite for Federated Learning both trainings share many of the same steps and we will be comparing the accuracy of the two trainings (Centralized Training vs.Federated Learning). It will also help us determine if our model is trainable.\n",
    "\n",
    "To start centralized training we first define a class called `BreastCancerDataset`. In this tutorial we are using the Breast Cancer Wisconsin (Diagnosic) datset [3]. It contains 30 features, computed from digitized breast cancer images. The task is to perform binary classification into the two classes \"malignant\" (=1) and \"benign\" (=0).\n",
    "\n",
    "If you are done with the tutorial and understand the main principles of federated learning, you can create your own data classes here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fdec2",
   "metadata": {},
   "source": [
    "#### 1. Data Prep\n",
    "We have already split our data into training and validation datasets which you can see in the `data` directory. \n",
    "\n",
    "The class below main function is to take a standardize the feature columns within a dataframe (excluding the first column, which is an ID, and the last column, which is the diagnosis label). By standardizing our features we avoid any outliers that may cause our model to become biased in training. \n",
    "\n",
    "Then it converts the standardized features into a PyTorch tensor (`self.X`). Extracts the diagnosis labels (malignant = 1, benign = 0) from the last column of the DataFrame and converts them into a PyTorch tensor (`self.y`).\n",
    "\n",
    "We will use this class for federated learning as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad91089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        scaler = StandardScaler()\n",
    "        self.X = torch.tensor(scaler.fit_transform(df.iloc[:,1:-1].values))   # first (ID) and last (diagnisis) columns are excluded\n",
    "        self.y =  torch.tensor(df.iloc[:,-1].values)                          # load the diagnosis (malignant=1, benign=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015ec6e",
   "metadata": {},
   "source": [
    "After defining the class for our dataset we load it.\n",
    "The data is split in a train and validation subset.\n",
    "The loaded instance is additionaly wrapped into a PyTorch `DataLoader()` object.\n",
    "This makes the dataset accessible to the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b34039",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(\"data\", \"full_train_data.csv\"), dtype=np.float32)\n",
    "val_df = pd.read_csv(os.path.join(\"data\", \"full_val_data.csv\"), dtype=np.float32)\n",
    "\n",
    "train_data = BreastCancerDataset(train_df)\n",
    "val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31ab00",
   "metadata": {},
   "source": [
    "#### 2. Define Client class for model training and validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe23c4",
   "metadata": {},
   "source": [
    "Now we define a `Client` class that is used for the training.\n",
    "The client receives the model and the train and validation data loaders.\n",
    "The class will be used for the centralized and federated training.\n",
    "\n",
    "The class contains two functions:\n",
    "- `train()` runs the training of the model\n",
    "- `validate()` runs the validation of the model on the given `val_loader`\n",
    "\n",
    "The `train` function in the `Client` class trains the client's local model for one epoch using its assigned training data. It iterates through the training dataset in batches, computes predictions using the model, and calculates the loss with the specified loss function (`criterion`). The function performs backpropagation by calculating gradients and updating the model's weights using the optimizer. It also tracks the number of correct predictions to compute the training accuracy for the epoch. Finally, it records the epoch's loss and accuracy in the client's `metrics` dictionary for later evaluation and visualization.\n",
    "\n",
    "The `validate` function in the `Client` class evaluates the client's local model using its validation dataset. It sets the model to evaluation mode (`model.eval()`) to disable dropout and other training-specific behaviors. The function iterates over the validation dataset (similar to the training dataset), computes predictions, and calculates the loss for each batch without updating the model's weights. It also tracks the number of correct predictions to compute the validation accuracy. Finally, it records the average loss and accuracy for the validation epoch in the client's `metrics` dictionary for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be7cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, name, model, train_loader, val_loader, optimizer, criterion):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.metrics = dict({\"train_acc\": list(), \"train_loss\": list(), \"val_acc\": list(), \"val_loss\": list()})\n",
    "\n",
    "        print(f\"[INFO] Initialized client '{self.name}' with {len(train_loader.dataset)} train and {len(val_loader.dataset)} validation samples\")\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            Trains the model of the client for 1 epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        correct_predictions = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # iterate over training dataset\n",
    "        for inputs, labels in self.train_loader:\n",
    "            # make predictions\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "            # apply gradient\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calculate number of correct predictions\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / len(self.train_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"train_acc\"].append(accuracy)\n",
    "        self.metrics[\"train_loss\"].append(epoch_loss)\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "            Validates the model of the client based on the given validation data loader.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # iterate over validation data loader and make predictions\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = correct_predictions / len(self.val_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"val_acc\"].append(accuracy)\n",
    "        self.metrics[\"val_loss\"].append(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa6192",
   "metadata": {},
   "source": [
    "#### 3. Defining the model\n",
    "\n",
    "Now that we have our data set up and the client defined we can define our model!\n",
    "\n",
    "The `SimpleNN` class defines a simple and small feedforward neural network for binary classification tasks. It contains three linear layers, with only a few nodes each. The network takes an input of a specified size (`n_input`), processes it through the layers, and outputs a single value between 0 and 1, representing the probability of the positive class. It is designed to be lightweight and efficient, making it suitable for use in both centralized and federated learning scenarios. The `forward` method defines how the input data flows through the network during training and inference.\n",
    "\n",
    "After finishing the tutorial feel free to come back to here and implement your own models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f312fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_input):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.NN = Sequential(\n",
    "            nn.Linear(n_input, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.NN(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736696c1",
   "metadata": {},
   "source": [
    "The `n_input` is set to 30 because the input layer of the `SimpleNN` neural network is designed to accept 30 features (or columns) as input. This matches the number of standardized feature columns in the dataset used for training and validation. In the context of the `BreastCancerDataset` class, the dataset contains 30 numerical features after excluding the ID column and the diagnosis label column. These 30 features are then fed into the neural network to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(n_input=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c001",
   "metadata": {},
   "source": [
    "#### 4. Initalizing the client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81553053",
   "metadata": {},
   "source": [
    "With the model available we can set up our client that is used for centralized training.\n",
    "\n",
    "The `optimizer` is responsible for updating the model's parameters (weights) during training to minimize the loss function. It uses the gradients computed during backpropagation to adjust the weights in the direction that reduces the loss.\n",
    "\n",
    "The `criterion` is the loss function used to measure how well the model's predictions match the true labels. It calculates the error between the predicted outputs and the actual targets, which the optimizer then tries to minimize.\n",
    "\n",
    "All of these functions are inputed into `central_client` to start the centralized training. after running this cell you should see an output stating that the client has been initialized this mean that the client has been created!\n",
    "\n",
    "For this example the client has been assigned 397 samples for training and 172 samples for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.BCELoss()\n",
    "central_client = Client(\"central\", model, train_dataloader, val_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d630d0",
   "metadata": {},
   "source": [
    "#### 5. Begin training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76128a",
   "metadata": {},
   "source": [
    "Now we can start the training. Using the `central_client` that we just initalized we run training and validation for 10 epochs, where in each epoch we train the model once on all training samples and adapt the model. Then we validate the updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    # run one training epoch\n",
    "    central_client.train()\n",
    "    \n",
    "    # run validation of training epoch\n",
    "    central_client.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de8874",
   "metadata": {},
   "source": [
    "#### 6. Plotting metrics to confrim convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480d6bc",
   "metadata": {},
   "source": [
    "After training we plot the training and validation metrics to check for convergence of the model by monitoring the loss and accuracy over multiple epochs. Lets take a look at what some of the metrics mean.\n",
    "\n",
    "- **Training loss:** Measures how well the model is fitting the training data. A decreasing training loss over epochs indicates that the model is learning from the training data.\n",
    "- **Training accuracy:** Tracks the proportion of correct predictions on the training dataset. An increasing training accuracy suggests that the model is improving its ability to classify the training samples correctly.\n",
    "- **Validation loss:** Measures how well the model generalizes to unseen data (validation dataset). A decreasing validation loss indicates better generalization, while an increasing loss may suggest overfitting.\n",
    "- **Validation accuracy:** Tracks the proportion of correct predictions on the validation dataset. An increasing validation accuracy indicates that the model is improving its performance on unseen data.\n",
    "\n",
    "The model is considered to be converging when the training and validation losses stabilize (stop decreasing significantly) and the validation accuracy reaches a plateau.\n",
    "If the validation loss starts increasing while the training loss continues to decrease, it may indicate overfitting, meaning the model is memorizing the training data instead of generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ec021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(client, op_save):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for k, v in client.metrics.items():\n",
    "        x_vals = range(len(v))\n",
    "        plt.plot(x_vals, v, label=k)\n",
    "\n",
    "    plt.ylim(bottom=0.0, top=1.0)\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.title(client.name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if op_save is not None:\n",
    "        plt.savefig(op_save.path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8691013",
   "metadata": {},
   "source": [
    "Run the code cell below to see a visual of our model metrics! \n",
    "\n",
    "**Note:** This function also lets you save the image as a file which we will do later during the Vetex AI Federated Learning portion of this tutorial. If you would like to do this now you can change the `None` value to a file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e771c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(central_client, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea3792",
   "metadata": {},
   "source": [
    "#### 7. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5195194",
   "metadata": {},
   "source": [
    "Additionally, we evaluate the model on the validation dataset. The `run_predictions` function below will iterate through the validation dataset, computes predictions using the model, and rounds the outputs to classify them as either 0 or 1 (\"malignant\" (=1) and \"benign\" (=0).).\n",
    "\n",
    "It compares the predicted labels with the true labels and counts the number of correct predictions.\n",
    "The accuracy is computed as the ratio of correct predictions to the total number of samples in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa665078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model, test_data_path):\n",
    "    model.eval()\n",
    "    \n",
    "    test_df = pd.read_csv(test_data_path, dtype=np.float32)\n",
    "    test_data = BreastCancerDataset(test_df)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # iterate over validation data loader and make predictions\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # calculate overall acc.\n",
    "    accuracy = correct_predictions / len(test_dataloader.dataset)\n",
    "    \n",
    "    print(f\"{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a8da3",
   "metadata": {},
   "source": [
    "Lets run the cell below to see our models perdiction accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the centrally trained model:\")\n",
    "run_prediction(central_client.model, 'data/full_val_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf102fb",
   "metadata": {},
   "source": [
    "The train and validation accurracy increases upon the epochs, while the loss decreases.\n",
    "This is a sign that our model converges and we can move on to implement federated learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436fc43",
   "metadata": {},
   "source": [
    "### Implementing Federated Learning locally\n",
    "\n",
    "Now that we have shown that our model is trainable with the given breast cancer dataset, we can implement federated learning!\n",
    "\n",
    "In the `data` folder there there are two clients already prepared for this tutorial (`client_0`, `client_1`).\n",
    "The data was presplit homogeneously accross the three clients, stratified by diagnosis.\n",
    "\n",
    "Just like in centralized data we are going to prep our data then we initialize each client, using the centrally initiallized model (Steps 1-4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_model = SimpleNN(n_input=30)\n",
    "# initialize clients\n",
    "clients = list()\n",
    "for i in range(2):\n",
    "    train_df = pd.read_csv(os.path.join(\"data\", f\"client_{i}\", \"train_data.csv\"), dtype=np.float32)\n",
    "    val_df = pd.read_csv(os.path.join(\"data\", f\"client_{i}\", \"val_data.csv\"), dtype=np.float32)\n",
    "    \n",
    "    train_data = BreastCancerDataset(train_df)\n",
    "    val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=7, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.SGD(fed_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    clients.append(Client(f\"client_{i}\", fed_model, train_dataloader, val_dataloader, optimizer, criterion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154b756",
   "metadata": {},
   "source": [
    "#### Define model aggregation\n",
    "\n",
    "This step is new and unique to Federated Learning because we need to define a function that aggregates the model weights. In this tutorial we use the basiv FedAvg algorithm for that [4]. It calculates the weighted mean for each node in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(global_state_dict, client_states, n_data_points):\n",
    "    \"\"\"\n",
    "    Averages the weights of client models to update the global model by FedAvg.\n",
    "\n",
    "    Args:\n",
    "        global_state_dict: The state dict of the global PyTorch model.\n",
    "        client_states: A list of PyTorch models state dicts representing client models.\n",
    "        n_data_points: A list with the number of data points per client.\n",
    "\n",
    "    Returns:\n",
    "        The state dict of the updated global PyTorch model.\n",
    "    \"\"\"\n",
    "    averaged_state_dict = OrderedDict()\n",
    "\n",
    "    for key in global_state_dict.keys():\n",
    "        for state, n in zip(client_states, n_data_points):\n",
    "            averaged_state_dict[key] =+ state[key] * (n/ sum(n_data_points))\n",
    "   \n",
    "    return averaged_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea29e9",
   "metadata": {},
   "source": [
    "#### Definition of a coordination server\n",
    "\n",
    "To orchestrate the federated learning process we define a coordination server.\n",
    "It has just one function that runs the federated learning training.\n",
    "The function loops over the clients and trains one epoch on each client.\n",
    "Then the updated models are aggregated by the FedAvg function.\n",
    "The updated models are sent back to the clients before validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a27732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLServer:\n",
    "    def __init__(self, model, clients):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.n_data_points = [len(client.train_loader.dataset) for client in self.clients]\n",
    "\n",
    "    def run(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            print(f\"Epoch {i}\")\n",
    "\n",
    "            # Step 2 of figure at the beginning of the tutorial\n",
    "            for client in self.clients:\n",
    "                client.train()\n",
    "\n",
    "            # aggregate the models using FedAvg (Step 3 & 4 of figure at the beginning of the tutorial)\n",
    "            client_states = [client.model.state_dict() for client in self.clients]                 # Step 3\n",
    "            aggregated_state = fed_avg(self.model.state_dict(), client_states, self.n_data_points) # Step 4\n",
    "            self.model.load_state_dict(aggregated_state)\n",
    "            \n",
    "            # redistribute central model (Step 1 of figure at the beginning of the tutorial)\n",
    "            for client in fl_server.clients:\n",
    "                client.model.load_state_dict(aggregated_state)\n",
    "\n",
    "            # run validation of aggregated model\n",
    "            for client in self.clients:\n",
    "                client.validate()\n",
    "\n",
    "            # repeat for n epochs (Step 5 of figure at the beginning of the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09ef02",
   "metadata": {},
   "source": [
    "#### Start Federated Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fdc15",
   "metadata": {},
   "source": [
    "Now we can finally start our federated training by calling the `run()` function! This will create training and validation accuracy and loss metrics for each epoch which we will then visualize inthe next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ecda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_server = FLServer(fed_model, clients)\n",
    "# distribute the central model to all clients (Step 1 of figure at the beginning of the tutorial)\n",
    "for client in fl_server.clients:\n",
    "    client.model.load_state_dict(fl_server.model.state_dict())\n",
    "\n",
    "#run training with server\n",
    "fl_server.run(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd2df1",
   "metadata": {},
   "source": [
    "#### Plot training metrics per client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7a960",
   "metadata": {},
   "source": [
    "After training is completed we can again have a look at the convergence of the model.\n",
    "In this case we get one plot for each client, containing accurracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client in fl_server.clients:\n",
    "    plot_metrics(client, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd5d08",
   "metadata": {},
   "source": [
    "#### Compare Central vs. Federated Learning accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19feb0",
   "metadata": {},
   "source": [
    "Now we can compare the final performance of the centrally trained model against the model trained with federated learning.\n",
    "The accuracies will not match perfectly, but they are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d46a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Centrally trained model accuracy:\")\n",
    "# run_prediction(central_client.model, 'data/full_val_data.csv')\n",
    "# print()\n",
    "print(\"Model trained with federated learning accuracy:\")\n",
    "run_prediction(fl_server.model, 'data/full_val_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c137505",
   "metadata": {},
   "source": [
    "### FL Vertex AI Custom Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23024e",
   "metadata": {},
   "source": [
    "\n",
    "Google Cloud's **Vertex AI custom pipeline** is a user-defined machine learning workflow built using Kubeflow Pipelines (KFP) and executed on Google Cloud's Vertex AI Pipelines service. It allows you to orchestrate and automate ML tasks such as data preprocessing, model training, evaluation, and deployment. Custom pipelines are composed of modular components, each performing a specific task, and these components can exchange data through inputs and outputs. The pipeline is compiled into a JSON file and deployed to Vertex AI, where it runs in a managed environment with support for logging, monitoring, and metrics visualization. This approach enables scalable, reproducible, and efficient ML workflows integrated with Google Cloud's ecosystem.\n",
    "\n",
    "**Kubeflow Pipelines** is an open-source platform for building, deploying, and managing machine learning (ML) workflows on Kubernetes. It provides a way to define and orchestrate ML workflows as a series of reusable components, where each component performs a specific task (e.g., data preprocessing, model training, evaluation). These workflows are defined using Python code and compiled into a format that can be executed on Kubernetes. they allow you to orchestrate piplines easliy creating by creating workflows making them reusable and scalable. You can easily integrate other tools and workflows and visualize metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00596f",
   "metadata": {},
   "source": [
    "#### Create a bucket and add data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4750b",
   "metadata": {},
   "source": [
    "Before we create our pipeline we need to add our data to a bucket. Enter your bucket name and project id in the following cell. Make sure your bucket has a globally unique name otherwise it will result in an error.\n",
    "\n",
    "**Note:** If you are not running this notebook in Vertex AI Workbench then you will need to authenticate your credentials, run `!gcloud auth login` before creating a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86406dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Google cloud storage bucket.\n",
    "BUCKET='YOUR_BUCKET_NAME'\n",
    "\n",
    "!gsutil mb gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751166a",
   "metadata": {},
   "source": [
    "Copy your client datasets (test and validation) and full validation data set to the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r data/client_0 gs://$BUCKET/data/\n",
    "!gsutil cp -r data/client_1 gs://$BUCKET/data/\n",
    "!gsutil cp data/full_val_data.csv gs://$BUCKET/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def2085",
   "metadata": {},
   "source": [
    "#### Training component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ff8ca",
   "metadata": {},
   "source": [
    "In Kubeflow Pipelines (KFP), **components** are the building blocks of a pipeline. Each component is a self-contained piece of code that performs a specific task in the machine learning workflow, such as data preprocessing, model training, evaluation, or deployment. Components are reusable, modular, and can be combined to create end-to-end pipelines.\n",
    "\n",
    "Below we are creating two components, training and evaluation. Each component acts like it own environment and may require functions to be repeatedly defined.\n",
    "Each component is compramized with a `@component` and the function containg the steps of that component. The `@component` flag may contain the following:\n",
    "- The base image you would like the component to run in\n",
    "- Packages to be installed in environment\n",
    "\n",
    "The first component we will create is the `training` component where we are adding steps 1-5 with the additional aggregation step unique to federated learning. The function lables inputs, outputs and their datatypes. Its important to include the output datatypes as this will become the input to our next component. For this component we are outputting the training metrics and the model state.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87dd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/deeplearning-platform-release/pytorch-cu124.py310:latest\",  \n",
    "           packages_to_install=[\n",
    "               \"google-cloud-aiplatform\",\n",
    "               \"ordereddict\"\n",
    "               ])\n",
    "def training(\n",
    "    clients_data_dir: str, \n",
    "    epochs: int,\n",
    "    feature_inputs: int,\n",
    "    num_client: int,\n",
    "    client_metrics: Output[Metrics],\n",
    "    client_metrics_output: Output[Dataset],\n",
    "    trained_model_output: Output[Model]):\n",
    "\n",
    "    import os \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.nn import Sequential\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    class BreastCancerDataset(Dataset):\n",
    "        def __init__(self, df):\n",
    "            scaler = StandardScaler()\n",
    "            self.X = torch.tensor(scaler.fit_transform(df.iloc[:,1:-1].values))   # first (ID) and last (diagnisis) columns are excluded\n",
    "            self.y =  torch.tensor(df.iloc[:,-1].values)                          # load the diagnosis (malignant=1, benign=0)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "    \n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self, n_input):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.NN = Sequential(\n",
    "                nn.Linear(n_input, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.NN(x)\n",
    "    \n",
    "    class Client:\n",
    "        def __init__(self, name, model, train_loader, val_loader, optimizer, criterion):\n",
    "            self.name = name\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.criterion = criterion\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.metrics = dict({\"train_acc\": list(), \"train_loss\": list(), \"val_acc\": list(), \"val_loss\": list()})\n",
    "\n",
    "            print(f\"[INFO] Initialized client '{self.name}' with {len(train_loader.dataset)} train and {len(val_loader.dataset)} validation samples\")\n",
    "            \n",
    "            \n",
    "        def train(self):\n",
    "            \"\"\"\n",
    "                Trains the model of the client for 1 epoch.\n",
    "            \"\"\"\n",
    "            self.model.train()\n",
    "            correct_predictions = 0\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # iterate over training dataset\n",
    "            for inputs, labels in self.train_loader:\n",
    "                # make predictions\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "                # apply gradient\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # calculate number of correct predictions\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            # calculate overall loss and acc.\n",
    "            epoch_loss = running_loss / len(self.train_loader)\n",
    "            accuracy = correct_predictions / len(self.train_loader.dataset)\n",
    "\n",
    "            # save metrics\n",
    "            self.metrics[\"train_acc\"].append(accuracy)\n",
    "            self.metrics[\"train_loss\"].append(epoch_loss)\n",
    "        \n",
    "        def validate(self):\n",
    "            \"\"\"\n",
    "                Validates the model of the client based on the given validation data loader.\n",
    "            \"\"\"\n",
    "            self.model.eval()\n",
    "            total_loss = 0\n",
    "            correct_predictions = 0\n",
    "\n",
    "            # iterate over validation data loader and make predictions\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in self.val_loader:\n",
    "                    outputs = self.model(inputs)\n",
    "                    labels = torch.unsqueeze(labels, 1)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    predicted = torch.round(outputs)\n",
    "                    correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            # calculate overall loss and acc.\n",
    "            average_loss = total_loss / len(self.val_loader)\n",
    "            accuracy = correct_predictions / len(self.val_loader.dataset)\n",
    "\n",
    "            # save metrics\n",
    "            self.metrics[\"val_acc\"].append(accuracy)\n",
    "            self.metrics[\"val_loss\"].append(average_loss)\n",
    "    \n",
    "    def fed_avg(global_state_dict, client_states, n_data_points):\n",
    "        \"\"\"\n",
    "        Averages the weights of client models to update the global model by FedAvg.\n",
    "\n",
    "        Args:\n",
    "            global_state_dict: The state dict of the global PyTorch model.\n",
    "            client_states: A list of PyTorch models state dicts representing client models.\n",
    "            n_data_points: A list with the number of data points per client.\n",
    "\n",
    "        Returns:\n",
    "            The state dict of the updated global PyTorch model.\n",
    "        \"\"\"\n",
    "        averaged_state_dict = OrderedDict()\n",
    "\n",
    "        for key in global_state_dict.keys():\n",
    "            for state, n in zip(client_states, n_data_points):\n",
    "                averaged_state_dict[key] =+ state[key] * (n/ sum(n_data_points))\n",
    "    \n",
    "        return averaged_state_dict       \n",
    "\n",
    "    class FLServer:\n",
    "        def __init__(self, model, clients):\n",
    "            self.model = model\n",
    "            self.clients = clients\n",
    "            self.n_data_points = [len(client.train_loader.dataset) for client in self.clients]\n",
    "\n",
    "        def run(self, epochs):\n",
    "            for i in range(epochs):\n",
    "                print(f\"Epoch {i}\")\n",
    "\n",
    "                # Step 2 of figure at the beginning of the tutorial\n",
    "                for client in self.clients:\n",
    "                    client.train()\n",
    "\n",
    "                # aggregate the models using FedAvg (Step 3 & 4 of figure at the beginning of the tutorial)\n",
    "                client_states = [client.model.state_dict() for client in self.clients]                 # Step 3\n",
    "                aggregated_state = fed_avg(self.model.state_dict(), client_states, self.n_data_points) # Step 4\n",
    "                self.model.load_state_dict(aggregated_state)\n",
    "                \n",
    "                # redistribute central model (Step 1 of figure at the beginning of the tutorial)\n",
    "                for client in fl_server.clients:\n",
    "                    client.model.load_state_dict(aggregated_state)\n",
    "\n",
    "                # run validation of aggregated model\n",
    "                for client in self.clients:\n",
    "                    client.validate()\n",
    "\n",
    "                # repeat for n epochs (Step 5 of figure at the beginning of the tutorial\n",
    "\n",
    "    fed_model = SimpleNN(n_input=feature_inputs)\n",
    "    # initialize clients\n",
    "    clients = list()\n",
    "    for i in range(num_client):\n",
    "        train_df = pd.read_csv(os.path.join(clients_data_dir, f\"client_{i}\", \"train_data.csv\"), dtype=np.float32)\n",
    "        val_df = pd.read_csv(os.path.join(clients_data_dir, f\"client_{i}\", \"val_data.csv\"), dtype=np.float32)\n",
    "        \n",
    "        train_data = BreastCancerDataset(train_df)\n",
    "        val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=7, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.SGD(fed_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        clients.append(Client(f\"client_{i}\", fed_model, train_dataloader, val_dataloader, optimizer, criterion))\n",
    "    \n",
    "    fl_server = FLServer(fed_model, clients)\n",
    "    # distribute the central model to all clients (Step 1 of figure at the beginning of the tutorial)\n",
    "    for client in fl_server.clients:\n",
    "        client.model.load_state_dict(fl_server.model.state_dict())\n",
    "\n",
    "    #run training with server\n",
    "    fl_server.run(epochs=epochs)\n",
    "\n",
    "    #save model\n",
    "    torch.save(fl_server.model.state_dict(), trained_model_output.path)\n",
    "    \n",
    "    #### Create a list to store the clients' data for visulaization ###\n",
    "    clients_data = []\n",
    "\n",
    "    # Iterate through each client in fl_server.clients\n",
    "    for client in fl_server.clients:\n",
    "        # Create a dictionary for the current client\n",
    "        client_data = {\n",
    "            \"name\": client.name,\n",
    "            \"metrics\": {}\n",
    "        }\n",
    "\n",
    "        # Add each metric to the \"metrics\" dictionary\n",
    "        for metric_name, values in client.metrics.items():\n",
    "            client_metrics.log_metric(f\"{client.name} - {metric_name}\", values)\n",
    "            client_data[\"metrics\"][metric_name] = values\n",
    "\n",
    "        # Append the client's data to the list\n",
    "        clients_data.append(client_data)  \n",
    "\n",
    "    import json\n",
    "    with open(client_metrics_output.path, \"w\") as f:\n",
    "        json.dump(clients_data, f, indent=2)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ce344",
   "metadata": {},
   "source": [
    "Looking at the last steps of the training component you will notice that other than saving the model the function is also logging in the training and validation accuracy and losss metrics. This will result in the metrics displaying in the Google Cloud console (by going to Vertex AI > Pipelines > Metrics) but will expor it as a dataset to create  create a visual of our metric in the next component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fabb9",
   "metadata": {},
   "source": [
    "#### Visualization component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd966f",
   "metadata": {},
   "source": [
    "The visulization component (Step 6) will take the metric output from the pervious component and will create a plot for each client. The plot will save in a Google Cloud bucket. Both plots will be saved to one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6395bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/deeplearning-platform-release/pytorch-cu124.py310:latest\",  \n",
    "           packages_to_install=[\n",
    "               \"google-cloud-aiplatform\",\n",
    "               \"matplotlib\"\n",
    "               ])\n",
    "def visualization(metrics_input: Input[Dataset], plot_save: Output[Artifact]):\n",
    "    import json\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    with open(metrics_input.path, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "            #print(metrics)\n",
    "\n",
    "    # Create subplots: one for each client\n",
    "    num_clients = len(metrics)\n",
    "    fig, axes = plt.subplots(num_clients, 1, figsize=(8, 4 * num_clients), squeeze=False)\n",
    "    \n",
    "    # Iterate over clients and plot their metrics\n",
    "    for i, client in enumerate(metrics):\n",
    "        ax = axes[i, 0]  # Access the subplot for the current client\n",
    "        for k, v in client[\"metrics\"].items():\n",
    "            x_vals = range(len(v))\n",
    "            ax.plot(x_vals, v, label=k)\n",
    "\n",
    "        # Customize the subplot\n",
    "        ax.set_title(f\"Metrics for {client['name']}\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Adjust layout and save the combined plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_save.path)\n",
    "    print(f\"Combined plot saved to {plot_save.path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df6eb0",
   "metadata": {},
   "source": [
    "#### Evaluation component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c897e",
   "metadata": {},
   "source": [
    "Next we will create the `evaluation` component which will load the model from the training component then use the `run_prediction` function to evaluate the accuracy of the model (Step 7). The accuracy will be logged in our metrics using the  `log_metrics` function and we will be able to visualize them on the console by going to Vertex AI > Pipelines, then click on the the metric icon on your pipeline workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/deeplearning-platform-release/pytorch-cu124.py310:latest\",\n",
    "           packages_to_install=[\n",
    "               \"google-cloud-aiplatform\"\n",
    "               ])\n",
    "def evaluation(\n",
    "    model_input: Input[Model],\n",
    "    test_data_path: str,\n",
    "    feature_inputs: int,\n",
    "    eval_metrics: Output[Metrics]\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.nn import Sequential\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class BreastCancerDataset(Dataset):\n",
    "        def __init__(self, df):\n",
    "            scaler = StandardScaler()\n",
    "            self.X = torch.tensor(scaler.fit_transform(df.iloc[:,1:-1].values))   # first (ID) and last (diagnisis) columns are excluded\n",
    "            self.y =  torch.tensor(df.iloc[:,-1].values)                          # load the diagnosis (malignant=1, benign=0)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "    \n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self, n_input):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.NN = Sequential(\n",
    "                nn.Linear(n_input, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.NN(x)\n",
    "\n",
    "    def run_prediction(model, test_data_path):\n",
    "        model.eval()\n",
    "        \n",
    "        test_df = pd.read_csv(test_data_path, dtype=np.float32)\n",
    "        test_data = BreastCancerDataset(test_df)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # iterate over validation data loader and make predictions\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall acc.\n",
    "        accuracy = correct_predictions / len(test_dataloader.dataset)\n",
    "        #run prediction to check accuracy and save metrics in the console\n",
    "        eval_metrics.log_metric(\"accuracy\", accuracy)\n",
    "        eval_metrics.log_metric(\"total_samples\", len(test_dataloader.dataset))\n",
    "        print(f\"{accuracy:.2f}\")\n",
    "\n",
    "    fl_server = SimpleNN(n_input=feature_inputs)     \n",
    "    fl_server.load_state_dict(torch.load(model_input.path))\n",
    "\n",
    "    print(\"Model trained with federated learning accuracy:\")\n",
    "    run_prediction(fl_server, test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9f066",
   "metadata": {},
   "source": [
    "#### Construct the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f878e89",
   "metadata": {},
   "source": [
    "The `@dsl.pipeline` decorator in Kubeflow Pipelines defines a pipeline. It assigns a name to the pipeline (e.g., `\"federated-learning-pipeline\"`) for identification in the Kubeflow or Vertex AI Pipelines UI. The decorated function specifies the components (steps) and their dependencies, describing how data flows between them. This function does not execute the pipeline but prepares it for compilation into a JSON file that can be submitted for execution. The pipeline enables reproducible and scalable orchestration of machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"federated-learning-pipeline\",\n",
    "    description=\"A pipeline for federated learning with client initialization, training, and evaluation.\"\n",
    ")\n",
    "def federated_learning_pipeline(\n",
    "    clients_data_dir: str, \n",
    "    num_client: int, \n",
    "    feature_inputs: int, \n",
    "    epochs: int,\n",
    "    test_data_path: str,\n",
    "    ):\n",
    "    \n",
    "    train_model_task = training(\n",
    "        clients_data_dir=clients_data_dir,\n",
    "        num_client=num_client,\n",
    "        epochs=epochs,\n",
    "        feature_inputs=feature_inputs)\n",
    "    \n",
    "    visualization(\n",
    "        metrics_input=train_model_task.outputs[\"client_metrics_output\"])\n",
    "    \n",
    "    evaluation(\n",
    "        model_input=train_model_task.outputs[\"trained_model_output\"],\n",
    "        test_data_path=test_data_path,\n",
    "        feature_inputs=feature_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7be76b",
   "metadata": {},
   "source": [
    "#### Complie the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156440a0",
   "metadata": {},
   "source": [
    "Now that we have all of our components we will use `compiler` converts our pipeline into a JSON-based Intermediate Representation (IR) that can be executed by the Vertex AI Pipeline/Kubeflow Pipelines engine. It ensures the pipeline's structure is valid, defines the dependencies between components, and serializes metadata like inputs, outputs, and parameters. The compiler does not execute the pipeline but prepares it for orchestration on Kubernetes or other infrastructure. This process enables reproducible, scalable, and shareable machine learning workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=federated_learning_pipeline,\n",
    "    package_path=\"federated_learning_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dd20f",
   "metadata": {},
   "source": [
    "#### Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe49a57",
   "metadata": {},
   "source": [
    "Now we can finally submit/run our pipeline in Vertex AI! Specify your project is and location. Enter in the name of your pipeline to be displayed on the console. Enter in the path of the pipeline JSON file (this was made in the pervious step). Specify a `pipeline_root` this will be a bucket that will hold any input and outputs made by our pipeline, in our case this bucket hold a model file, our plots, and metrics. Lastly enter in any parameters needed to start our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7cb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "project_id = \"ENTER_YOUR_PROJECT_ID\"\n",
    "location = \"ENTER_YOUR_REGION (e.g. us-central1)\"\n",
    "\n",
    "aiplatform.init(project=project_id, location=location)\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"federated-learning-pipeline\",\n",
    "    template_path=\"federated_learning_pipeline.json\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/pipeline_root/federated_learning_pipeline\",\n",
    "    parameter_values={\n",
    "        \"clients_data_dir\": f\"gs://{BUCKET}/data\",\n",
    "        \"num_client\": 2,\n",
    "        \"feature_inputs\": 30,\n",
    "        \"epochs\": 10,\n",
    "        \"test_data_path\": f\"gs://{BUCKET}/data/full_val_data.csv\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751a74e",
   "metadata": {},
   "source": [
    "Run your pipeline! After your run the cell below you should see a link to your pipeline (under \"View Pipeline Job:\"). You can us this link to open the console and track your pipeline, view logs, and outputs. This pipeline will take ~6 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dab927",
   "metadata": {},
   "source": [
    "Once you head to the console after clicking the link you can view and track the progress of your pipeline.\n",
    "\n",
    "![image1](../../images/fl_console1.png)\n",
    "\n",
    "You can click on the icons to view metrics.\n",
    "\n",
    "![image1](../../images/fl_console3.png)\n",
    "\n",
    "Or head to the location of where some of you outputs may be, like our graphs.\n",
    "![image1](../../images/fl_console2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ab327",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the implementation of a federated learning pipeline on Google Cloud Platform (GCP) using Vertex AI Pipelines. We demonstrated how to preprocess data, train models across multiple clients, and aggregate the results in a federated learning setup. Additionally, we visualized client-specific metrics and registered the trained model in the Vertex AI Model Registry for future deployment and serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec88726",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Make sure you shutdown or delete any notebooks or buckets that have been created in this tutorial. You can also delete you pipeline by going to Vertex AI > Pipeline. Select your pipeline then click 'Delete'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22edf2",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "If you are further interested in Federated Learning here are some useful resources to continue your FL journey.\n",
    "\n",
    "### Other Tutorials:\n",
    "- [TensorFlow](https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm)\n",
    "- [FLOWER](https://flower.ai/docs/framework/tutorial-series-get-started-with-flower-pytorch.html)\n",
    "\n",
    "### Frameworks:\n",
    "- [NVFLare](https://developer.nvidia.com/flare)\n",
    "- [TensorFlow](https://www.tensorflow.org/federated)\n",
    "- [FLOWER](https://flower.ai/docs/framework/index.html)\n",
    "- [FeatureCloud](https://featurecloud.ai/)\n",
    "\n",
    "### Literature:\n",
    "[1] Rieke et al., (2020), \"[The future of digital health with federated learning](https://www.nature.com/articles/s41746-020-00323-1)\"\n",
    "\n",
    "[2] Zhang et al., (2021), \"[A survey on federated learning](https://www.sciencedirect.com/science/article/pii/S0950705121000381)\"\n",
    "\n",
    "[3] Wolberg et al., (1993), [Breast Cancer Wisconsin (Diagnostic)](https://doi.org/10.24432/C5DW2B)\n",
    "\n",
    "[4] McMahan et al., (2017), \"[Communication-Efficient Learning of Deep Networks from Decentralized Data](https://proceedings.mlr.press/v54/mcmahan17a.html)\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4dfe2b",
   "metadata": {},
   "source": [
    "# Introduction to Federated Learning Locally and on GCP's Vertex AI \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ce8e7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Federated Learning is a privacy preserving technique to train machine learning models.\n",
    "When dealing with secure data (e.g. patient data) it is often restricted to merge datasets from different institutions (clients).\n",
    "Federated Learning enables the training of a machine learning model without sharing the underlying data.\n",
    "This is achieved by training local models at each client and only aggreagating the model weights. Instead of sending raw data to a central server, each client trains the model locally on its own data and only shares the model's updates (e.g., weights or gradients) with the central server. The server aggregates these updates (e.g., using algorithms like Federated Averaging, or FedAvg) to improve the global model.\n",
    "\n",
    "This approach is particularly useful in scenarios where data privacy, security, or bandwidth constraints make it impractical to centralize data.\n",
    "\n",
    "In this tutorial we will focus on centralized horizontal Federated Learning. Centralized FL has a coordinating server that controls the learning process and aggreagates the model weights [1].\n",
    "Horizontal FL means that the same features are available on each client (e.g. images) [2].\n",
    "The pendant to that would be vertical FL where different features are present, but for the same sample (e.g. patient).\n",
    "\n",
    "This is the general overview of a federated learning training process.\n",
    "The image was taken from the [NVIDIA blog](https://blogs.nvidia.com/blog/what-is-federated-learning/) and slightly modified.\n",
    "\n",
    "<img src=\"../../images/federated_learning_animation_still_white.png\" width=\"800\">\n",
    "\n",
    "The training process can be split up in [5 steps](https://flower.ai/docs/framework/tutorial-series-what-is-federated-learning.html):\n",
    "\n",
    "0. Initialize the global model\n",
    "1. Send global model to clients\n",
    "2. Local training\n",
    "3. Return model updates to coordinator\n",
    "4. Aggregate model updates by averaging (FedAvg)\n",
    "5. Repeat steps 1 to 4 until convergence\n",
    "\n",
    "\n",
    "\n",
    "FL workflows typically include the following steps which we will go over in this tutorial:\n",
    "\n",
    "1. **Data Preparation:** Split the data across multiple clients.\n",
    "2. **Client Training:** Each client trains a local model on its data.\n",
    "3. **Model Aggregation:** The server aggregates the client models using algorithms like Federated Averaging (FedAvg).\n",
    "4. **Evaluation:** Evaluate the aggregated model.\n",
    "5. **Repeat:** Iterate for multiple rounds (epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34a8be",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebooks was run using the machinetype n1-highcpu-8 (8 vCPUs, 7.199 GB RAM) on Pytorch. Ensure Vertex AI and Cloud Storage APIs are enabled. Visit the following tutorial to set up notebooks that utilize: [GPUs Spinning up a Vertex AI Notebook](https://github.com/STRIDES/NIHCloudLabGCP/blob/42ee2b7dbffce54e53a212d8c02ac16fd872c5be/docs/vertexai.md) for faster speeds if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1d5af",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Understand Federated Learning \n",
    "* Learn to created a Centralized training and Federated Learning workflows locally\n",
    "* Learn how to adapt the federated learning process to Google Clouds' Vertex AI.\n",
    "* Evaluate and visualize model performance Centralized training vs. Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b403d",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1607b6f",
   "metadata": {},
   "source": [
    "Install the following packages. In this tutorial we are using the **Pytorch Kernel** in a Vertex AI Workbench Jupyter Notebook which has Pytorch preinstalled. If you are not using the same setting you can install the rest of the needed packages by running `pip install torch pandas scikit-learn matplotlib ordereddict`.\n",
    "\n",
    "The kfp package will allow us to complie the functions that we are about to make into a pipeline which we will use in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e263073",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-aiplatform kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df176e7",
   "metadata": {},
   "source": [
    "Import our packages and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ea42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from torch.nn import Sequential\n",
    "from collections import OrderedDict\n",
    "from google.cloud import aiplatform\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from kfp.v2.dsl import component, Output, Dataset, Model, Input, Artifact, Metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d3aee",
   "metadata": {},
   "source": [
    "### Centralized Training\n",
    "\n",
    "As a first step we demonstrate the training of a ML model through a traditional, centralized training. Although this is not a prerequisite for Federated Learning both trainings share many of the same steps and we will be comparing the accuracy of the two trainings (Centralized Training vs.Federated Learning). It will also help us determine if our model is trainable.\n",
    "\n",
    "To start centralized training we first define a class called `BreastCancerDataset`. In this tutorial we are using the Breast Cancer Wisconsin (Diagnosic) datset [3]. It contains 30 features, computed from digitized breast cancer images. The task is to perform binary classification into the two classes \"malignant\" (=1) and \"benign\" (=0).\n",
    "\n",
    "If you are done with the tutorial and understand the main principles of federated learning, you can create your own data classes here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fdec2",
   "metadata": {},
   "source": [
    "#### 1. Data Prep\n",
    "We have already split our data into training and validation datasets which you can see in the `data` directory. \n",
    "\n",
    "The class below main function is to take a standardize the feature columns within a dataframe (excluding the first column, which is an ID, and the last column, which is the diagnosis label). By standardizing our features we avoid any outliers that may cause our model to become biased in training. \n",
    "\n",
    "Then it converts the standardized features into a PyTorch tensor (`self.X`). Extracts the diagnosis labels (malignant = 1, benign = 0) from the last column of the DataFrame and converts them into a PyTorch tensor (`self.y`).\n",
    "\n",
    "We will use this class for federated learning as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad91089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        scaler = StandardScaler()\n",
    "        self.X = torch.tensor(scaler.fit_transform(df.iloc[:,1:-1].values))   # first (ID) and last (diagnisis) columns are excluded\n",
    "        self.y =  torch.tensor(df.iloc[:,-1].values)                          # load the diagnosis (malignant=1, benign=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015ec6e",
   "metadata": {},
   "source": [
    "After defining the class for our dataset we load it.\n",
    "The data is split in a train and validation subset.\n",
    "The loaded instance is additionaly wrapped into a PyTorch `DataLoader()` object.\n",
    "This makes the dataset accessible to the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b34039",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(\"data\", \"full_train_data.csv\"), dtype=np.float32)\n",
    "val_df = pd.read_csv(os.path.join(\"data\", \"full_val_data.csv\"), dtype=np.float32)\n",
    "\n",
    "train_data = BreastCancerDataset(train_df)\n",
    "val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31ab00",
   "metadata": {},
   "source": [
    "#### 2. Define Client class for model training and validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe23c4",
   "metadata": {},
   "source": [
    "Now we define a `Client` class that is used for the training.\n",
    "The client receives the model and the train and validation data loaders.\n",
    "The class will be used for the centralized and federated training.\n",
    "\n",
    "The class contains two functions:\n",
    "- `train()` runs the training of the model\n",
    "- `validate()` runs the validation of the model on the given `val_loader`\n",
    "\n",
    "The `train` function in the `Client` class trains the client's local model for one epoch using its assigned training data. It iterates through the training dataset in batches, computes predictions using the model, and calculates the loss with the specified loss function (`criterion`). The function performs backpropagation by calculating gradients and updating the model's weights using the optimizer. It also tracks the number of correct predictions to compute the training accuracy for the epoch. Finally, it records the epoch's loss and accuracy in the client's `metrics` dictionary for later evaluation and visualization.\n",
    "\n",
    "The `validate` function in the `Client` class evaluates the client's local model using its validation dataset. It sets the model to evaluation mode (`model.eval()`) to disable dropout and other training-specific behaviors. The function iterates over the validation dataset (similar to the training dataset), computes predictions, and calculates the loss for each batch without updating the model's weights. It also tracks the number of correct predictions to compute the validation accuracy. Finally, it records the average loss and accuracy for the validation epoch in the client's `metrics` dictionary for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be7cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, name, model, train_loader, val_loader, optimizer, criterion):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.metrics = dict({\"train_acc\": list(), \"train_loss\": list(), \"val_acc\": list(), \"val_loss\": list()})\n",
    "\n",
    "        print(f\"[INFO] Initialized client '{self.name}' with {len(train_loader.dataset)} train and {len(val_loader.dataset)} validation samples\")\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            Trains the model of the client for 1 epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        correct_predictions = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # iterate over training dataset\n",
    "        for inputs, labels in self.train_loader:\n",
    "            # make predictions\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "            # apply gradient\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calculate number of correct predictions\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / len(self.train_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"train_acc\"].append(accuracy)\n",
    "        self.metrics[\"train_loss\"].append(epoch_loss)\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "            Validates the model of the client based on the given validation data loader.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # iterate over validation data loader and make predictions\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall loss and acc.\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = correct_predictions / len(self.val_loader.dataset)\n",
    "\n",
    "        # save metrics\n",
    "        self.metrics[\"val_acc\"].append(accuracy)\n",
    "        self.metrics[\"val_loss\"].append(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa6192",
   "metadata": {},
   "source": [
    "#### 3. Defining the model\n",
    "\n",
    "Now that we have our data set up and the client defined we can define our model!\n",
    "\n",
    "The `SimpleNN` class defines a simple and small feedforward neural network for binary classification tasks. It contains three linear layers, with only a few nodes each. The network takes an input of a specified size (`n_input`), processes it through the layers, and outputs a single value between 0 and 1, representing the probability of the positive class. It is designed to be lightweight and efficient, making it suitable for use in both centralized and federated learning scenarios. The `forward` method defines how the input data flows through the network during training and inference.\n",
    "\n",
    "After finishing the tutorial feel free to come back to here and implement your own models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f312fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_input):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.NN = Sequential(\n",
    "            nn.Linear(n_input, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.NN(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736696c1",
   "metadata": {},
   "source": [
    "The `n_input` is set to 30 because the input layer of the `SimpleNN` neural network is designed to accept 30 features (or columns) as input. This matches the number of standardized feature columns in the dataset used for training and validation. In the context of the `BreastCancerDataset` class, the dataset contains 30 numerical features after excluding the ID column and the diagnosis label column. These 30 features are then fed into the neural network to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(n_input=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c001",
   "metadata": {},
   "source": [
    "#### 4. Initalizing the client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81553053",
   "metadata": {},
   "source": [
    "With the model available we can set up our client that is used for centralized training.\n",
    "\n",
    "The `optimizer` is responsible for updating the model's parameters (weights) during training to minimize the loss function. It uses the gradients computed during backpropagation to adjust the weights in the direction that reduces the loss.\n",
    "\n",
    "The `criterion` is the loss function used to measure how well the model's predictions match the true labels. It calculates the error between the predicted outputs and the actual targets, which the optimizer then tries to minimize.\n",
    "\n",
    "All of these functions are inputed into `central_client` to start the centralized training. after running this cell you should see an output stating that the client has been initialized this mean that the client has been created!\n",
    "\n",
    "For this example the client has been assigned 397 samples for training and 172 samples for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.BCELoss()\n",
    "central_client = Client(\"central\", model, train_dataloader, val_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d630d0",
   "metadata": {},
   "source": [
    "#### 5. Begin training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76128a",
   "metadata": {},
   "source": [
    "Now we can start the training. Using the `central_client` that we just initalized we run training and validation for 10 epochs, where in each epoch we train the model once on all training samples and adapt the model. Then we validate the updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    # run one training epoch\n",
    "    central_client.train()\n",
    "    \n",
    "    # run validation of training epoch\n",
    "    central_client.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de8874",
   "metadata": {},
   "source": [
    "#### 6. Plotting Metrics to confrim convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480d6bc",
   "metadata": {},
   "source": [
    "After training we plot the training and validation metrics to check for convergence of the model by monitoring the loss and accuracy over multiple epochs. Lets take a look at what some of the metrics mean.\n",
    "\n",
    "- **Training loss:** Measures how well the model is fitting the training data. A decreasing training loss over epochs indicates that the model is learning from the training data.\n",
    "- **Training accuracy:** Tracks the proportion of correct predictions on the training dataset. An increasing training accuracy suggests that the model is improving its ability to classify the training samples correctly.\n",
    "- **Validation loss:** Measures how well the model generalizes to unseen data (validation dataset). A decreasing validation loss indicates better generalization, while an increasing loss may suggest overfitting.\n",
    "- **Validation accuracy:** Tracks the proportion of correct predictions on the validation dataset. An increasing validation accuracy indicates that the model is improving its performance on unseen data.\n",
    "\n",
    "The model is considered to be converging when the training and validation losses stabilize (stop decreasing significantly) and the validation accuracy reaches a plateau.\n",
    "If the validation loss starts increasing while the training loss continues to decrease, it may indicate overfitting, meaning the model is memorizing the training data instead of generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ec021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(client, op_save):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for k, v in client.metrics.items():\n",
    "        x_vals = range(len(v))\n",
    "        plt.plot(x_vals, v, label=k)\n",
    "\n",
    "    plt.ylim(bottom=0.0, top=1.0)\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.title(client.name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if op_save is not None:\n",
    "        plt.savefig(op_save.path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8691013",
   "metadata": {},
   "source": [
    "Run the code cell below to see a visual of our model metrics! \n",
    "\n",
    "**Note:** This function also lets you save the image as a file which we will do later during the Vetex AI Federated Learning portion of this tutorial. If you would like to do this now you can change the `None` value to a file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e771c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(central_client, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea3792",
   "metadata": {},
   "source": [
    "#### 7. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5195194",
   "metadata": {},
   "source": [
    "Additionally, we evaluate the model on the validation dataset. The `run_predictions` function below will iterate through the validation dataset, computes predictions using the model, and rounds the outputs to classify them as either 0 or 1 (\"malignant\" (=1) and \"benign\" (=0).).\n",
    "\n",
    "It compares the predicted labels with the true labels and counts the number of correct predictions.\n",
    "The accuracy is computed as the ratio of correct predictions to the total number of samples in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa665078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model, test_data_path):\n",
    "    model.eval()\n",
    "    \n",
    "    test_df = pd.read_csv(test_data_path, dtype=np.float32)\n",
    "    test_data = BreastCancerDataset(test_df)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # iterate over validation data loader and make predictions\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            labels = torch.unsqueeze(labels, 1)\n",
    "            predicted = torch.round(outputs)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # calculate overall acc.\n",
    "    accuracy = correct_predictions / len(test_dataloader.dataset)\n",
    "    \n",
    "    print(f\"{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a8da3",
   "metadata": {},
   "source": [
    "Lets run the cell below to see our models perdiction accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the centrally trained model:\")\n",
    "run_prediction(central_client.model, 'data/full_val_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf102fb",
   "metadata": {},
   "source": [
    "The train and validation accurracy increases upon the epochs, while the loss decreases.\n",
    "This is a sign that our model converges and we can move on to implement federated learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436fc43",
   "metadata": {},
   "source": [
    "### Implementing Federated Learning locally\n",
    "\n",
    "Now that we have shown that our model is trainable with the given breast cancer dataset, we can implement federated learning!\n",
    "\n",
    "In the `data` folder there there are two clients already prepared for this tutorial (`client_0`, `client_1`).\n",
    "The data was presplit homogeneously accross the three clients, stratified by diagnosis.\n",
    "\n",
    "Just like in centralized data we are going to prep our data then we initialize each client, using the centrally initiallized model (Steps 1-4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_model = SimpleNN(n_input=30)\n",
    "# initialize clients\n",
    "clients = list()\n",
    "for i in range(2):\n",
    "    train_df = pd.read_csv(os.path.join(\"data\", f\"client_{i}\", \"train_data.csv\"), dtype=np.float32)\n",
    "    val_df = pd.read_csv(os.path.join(\"data\", f\"client_{i}\", \"val_data.csv\"), dtype=np.float32)\n",
    "    \n",
    "    train_data = BreastCancerDataset(train_df)\n",
    "    val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=7, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.SGD(fed_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    clients.append(Client(f\"client_{i}\", fed_model, train_dataloader, val_dataloader, optimizer, criterion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154b756",
   "metadata": {},
   "source": [
    "#### Define model aggregation\n",
    "\n",
    "This step is new and unique to Federated Learning because we need to define a function that aggregates the model weights. In this tutorial we use the basiv FedAvg algorithm for that [4]. It calculates the weighted mean for each node in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(global_state_dict, client_states, n_data_points):\n",
    "    \"\"\"\n",
    "    Averages the weights of client models to update the global model by FedAvg.\n",
    "\n",
    "    Args:\n",
    "        global_state_dict: The state dict of the global PyTorch model.\n",
    "        client_states: A list of PyTorch models state dicts representing client models.\n",
    "        n_data_points: A list with the number of data points per client.\n",
    "\n",
    "    Returns:\n",
    "        The state dict of the updated global PyTorch model.\n",
    "    \"\"\"\n",
    "    averaged_state_dict = OrderedDict()\n",
    "\n",
    "    for key in global_state_dict.keys():\n",
    "        for state, n in zip(client_states, n_data_points):\n",
    "            averaged_state_dict[key] =+ state[key] * (n/ sum(n_data_points))\n",
    "   \n",
    "    return averaged_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea29e9",
   "metadata": {},
   "source": [
    "#### Definition of a coordination server\n",
    "\n",
    "To orchestrate the federated learning process we define a coordination server.\n",
    "It has just one function that runs the federated learning training.\n",
    "The function loops over the clients and trains one epoch on each client.\n",
    "Then the updated models are aggregated by the FedAvg function.\n",
    "The updated models are sent back to the clients before validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a27732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLServer:\n",
    "    def __init__(self, model, clients):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.n_data_points = [len(client.train_loader.dataset) for client in self.clients]\n",
    "\n",
    "    def run(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            print(f\"Epoch {i}\")\n",
    "\n",
    "            # Step 2 of figure at the beginning of the tutorial\n",
    "            for client in self.clients:\n",
    "                client.train()\n",
    "\n",
    "            # aggregate the models using FedAvg (Step 3 & 4 of figure at the beginning of the tutorial)\n",
    "            client_states = [client.model.state_dict() for client in self.clients]                 # Step 3\n",
    "            aggregated_state = fed_avg(self.model.state_dict(), client_states, self.n_data_points) # Step 4\n",
    "            self.model.load_state_dict(aggregated_state)\n",
    "            \n",
    "            # redistribute central model (Step 1 of figure at the beginning of the tutorial)\n",
    "            for client in fl_server.clients:\n",
    "                client.model.load_state_dict(aggregated_state)\n",
    "\n",
    "            # run validation of aggregated model\n",
    "            for client in self.clients:\n",
    "                client.validate()\n",
    "\n",
    "            # repeat for n epochs (Step 5 of figure at the beginning of the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fdc15",
   "metadata": {},
   "source": [
    "Now we can finally start our federated training by calling the `run()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ecda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_server = FLServer(fed_model, clients)\n",
    "# distribute the central model to all clients (Step 1 of figure at the beginning of the tutorial)\n",
    "for client in fl_server.clients:\n",
    "    client.model.load_state_dict(fl_server.model.state_dict())\n",
    "\n",
    "#run training with server\n",
    "fl_server.run(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7a960",
   "metadata": {},
   "source": [
    "After training is completed we can again have a look at the convergence of the model.\n",
    "In this case we get one plot for each client, containing accurracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client in fl_server.clients:\n",
    "    plot_metrics(client, None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19feb0",
   "metadata": {},
   "source": [
    "Now we can compare the final performance of the centrally trained model against the model trained with federated learning.\n",
    "The accuracies will not match perfectly, but they are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d46a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Centrally trained model accuracy:\")\n",
    "run_prediction(central_client.model)\n",
    "print()\n",
    "print(\"Model trained with federated learning accuracy:\")\n",
    "run_prediction(fl_server.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c137505",
   "metadata": {},
   "source": [
    "### Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23024e",
   "metadata": {},
   "source": [
    "\n",
    "The training and validation client datasets are currently stored in your working environment. Create a cloud storage bucket and push these csv files to the bucket. Make sure you Bucket Name is universally unique or you will run into a error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-cu124.py310:latest\",    \n",
    "    packages_to_install=[\n",
    "        \"fl_packages @ git+https://github.com/STRIDES/NIHCloudLabGCP@fl-tutorial#subdirectory=notebooks/FederatedLearning/scripts\"\n",
    "    ]\n",
    ")\n",
    "def initialize_model(\n",
    "    client_data_dir: str, \n",
    "    num_clients: int, \n",
    "    feature_inputs: int, \n",
    "    #model_output: Output[Model],\n",
    "    clients_output: Output[Artifact]\n",
    "    ):\n",
    "    \n",
    "    import torch\n",
    "    import os\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch.nn as nn\n",
    "\n",
    "    from torch.nn import Sequential\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "\n",
    "    ## Step 1: Initialize clients and model\n",
    "    # load the model\n",
    "    fed_model = SimpleNN(n_input=feature_inputs)\n",
    "\n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        train_df = pd.read_csv(os.path.join(client_data_dir, f\"client_{i}\", \"train_data.csv\"), dtype=np.float32)\n",
    "        val_df = pd.read_csv(os.path.join(client_data_dir, f\"client_{i}\", \"val_data.csv\"), dtype=np.float32)\n",
    "        \n",
    "        train_data = BreastCancerDataset(train_df)\n",
    "        val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=7, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.SGD(fed_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.BCELoss()\n",
    "    \n",
    "        clients.append(Client(f\"client_{i}\", fed_model, train_dataloader, val_dataloader, optimizer, criterion))\n",
    "    \n",
    "    #save model\n",
    "    #torch.save(fed_model.model.state_dict(), model_output.path)\n",
    "\n",
    "    # Serialize and save the clients\n",
    "    with open(clients_output.path, \"wb\") as f:\n",
    "        pickle.dump(clients, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"cit-oconnellka-9999\", location=\"us-central1\")\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"example-pipeline\",\n",
    "    template_path=\"example-pipeline.json\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/pipeline_root-test/example-pipeline\",\n",
    "    parameter_values={\n",
    "        \"feature_inputs\": 30,\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential\n",
    "from kfp.v2.dsl import component, pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Define the SimpleNN class globally\n",
    "\n",
    "\n",
    "# Component to initialize the model\n",
    "@component(base_image=\"gcr.io/deeplearning-platform-release/pytorch-cu124.py310:latest\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gs://federated-learning-resources-zy-2/packages/my_package-0.1.tar.gz\"\n",
    "    ]\n",
    ")\n",
    "def initialize_model(feature_inputs: int):\n",
    "    import google.cloud.storage\n",
    "    from my_package.test_models_func import SimpleNN\n",
    "    fed_model = SimpleNN(n_input=feature_inputs)\n",
    "    print(\"Model initialized successfully\")\n",
    "\n",
    "# Define the pipeline\n",
    "@pipeline(name=\"example-pipeline\")\n",
    "def example_pipeline(feature_inputs: int):\n",
    "    initialize_model(feature_inputs=feature_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ba245",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/setup.py sdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=example_pipeline,\n",
    "    package_path=\"example-pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc28585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"cit-oconnellka-9999\", location=\"us-central1\")\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"example-pipeline\",\n",
    "    template_path=\"example-pipeline.json\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/pipeline_root-test/example-pipeline\",\n",
    "    parameter_values={\n",
    "        \"feature_inputs\": 30,\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7006cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/deeplearning-platform-release/pytorch-cu124.py310:latest\",  \n",
    "           packages_to_install=[\n",
    "               \"google-cloud-aiplatform\",\n",
    "               \"matplotlib\",\n",
    "               \"ordereddict\"\n",
    "               ])\n",
    "def train_model(\n",
    "    #model_input: Input[Model],\n",
    "    clients_input: Input[Artifact],\n",
    "    epochs: int,\n",
    "    feature_inputs: int,\n",
    "    visualization_output: Output[Artifact],\n",
    "    trained_model_output: Output[Model]\n",
    "    ):\n",
    "\n",
    "    import pickle\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    def fed_avg(global_state_dict, client_states, n_data_points):\n",
    "        \"\"\"\n",
    "        Averages the weights of client models to update the global model by FedAvg.\n",
    "\n",
    "        Args:\n",
    "            global_state_dict: The state dict of the global PyTorch model.\n",
    "            client_states: A list of PyTorch models state dicts representing client models.\n",
    "            n_data_points: A list with the number of data points per client.\n",
    "\n",
    "        Returns:\n",
    "            The state dict of the updated global PyTorch model.\n",
    "        \"\"\"\n",
    "        averaged_state_dict = OrderedDict()\n",
    "\n",
    "        for key in global_state_dict.keys():\n",
    "            for state, n in zip(client_states, n_data_points):\n",
    "                averaged_state_dict[key] =+ state[key] * (n/ sum(n_data_points))\n",
    "    \n",
    "        return averaged_state_dict\n",
    "    \n",
    "    class FLServer:\n",
    "        def __init__(self, model, clients):\n",
    "            self.model = model\n",
    "            self.clients = clients\n",
    "            self.n_data_points = [len(client.train_loader.dataset) for client in self.clients]\n",
    "\n",
    "        def run(self, epochs):\n",
    "            for i in range(epochs):\n",
    "                print(f\"Epoch {i}\")\n",
    "\n",
    "                # Step 2 of figure at the beginning of the tutorial\n",
    "                for client in self.clients:\n",
    "                    client.train()\n",
    "\n",
    "                # aggregate the models using FedAvg (Step 3 & 4 of figure at the beginning of the tutorial)\n",
    "                client_states = [client.model.state_dict() for client in self.clients]                 # Step 3\n",
    "                aggregated_state = fed_avg(self.model.state_dict(), client_states, self.n_data_points) # Step 4\n",
    "                self.model.load_state_dict(aggregated_state)\n",
    "                \n",
    "                # redistribute central model (Step 1 of figure at the beginning of the tutorial)\n",
    "                for client in fl_server.clients:\n",
    "                    client.model.load_state_dict(aggregated_state)\n",
    "\n",
    "                # run validation of aggregated model\n",
    "                for client in self.clients:\n",
    "                    client.validate()\n",
    "\n",
    "                # repeat for n epochs (Step 5 of figure at the beginning of the tutorial\n",
    "\n",
    "\n",
    "    def plot_metrics(client, op_save):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        for k, v in client.metrics.items():\n",
    "            x_vals = range(len(v))\n",
    "            plt.plot(x_vals, v, label=k)\n",
    "\n",
    "        plt.ylim(bottom=0.0, top=1.0)\n",
    "        plt.xlim(left=0)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Metric\")\n",
    "        plt.title(client.name)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if op_save is not None:\n",
    "            plt.savefig(op_save.path)\n",
    "        plt.close()\n",
    "  \n",
    "       # Load the model\n",
    "    fed_model = SimpleNN(n_input=feature_inputs)\n",
    "    #fed_model.load_state_dict(torch.load(model_input.path))\n",
    "\n",
    "    # Load the clients\n",
    "    with open(clients_input.path, \"rb\") as f:\n",
    "        clients = pickle.load(f)\n",
    "\n",
    "    # Step 2: Train the federated learning server\n",
    "    fl_server = FLServer(fed_model, clients)\n",
    "    # distribute the central model to all clients (Step 1 of figure at the beginning of the tutorial)\n",
    "    for client in fl_server.clients:\n",
    "        client.model.load_state_dict(fl_server.model.state_dict())\n",
    "\n",
    "    #run training with server\n",
    "    fl_server.run(epochs=epochs)\n",
    "\n",
    "    #plot metrics for each client\n",
    "    for client in fl_server.clients:\n",
    "        client_plot_path = os.path.join(visualization_output.path, f\"{client.name}_metrics.png\")\n",
    "        plot_metrics(client, client_plot_path)\n",
    "\n",
    "    #save model\n",
    "    torch.save(fl_server.model.state_dict(), trained_model_output.path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/deeplearning-platform-release/pytorch-cu124.py310:latest\",\n",
    "           packages_to_install=[\n",
    "               \"google-cloud-aiplatform\"\n",
    "               ])\n",
    "def evaluate(\n",
    "    model_input: Input[Model],\n",
    "    test_data_path: str,\n",
    "    feature_inputs: int,\n",
    "    metrics_output: Output[Metrics]\n",
    "    ):\n",
    "   import pandas as pd\n",
    "\n",
    "   from torch.utils.data import DataLoader\n",
    "\n",
    "   def run_prediction(model, test_data_path):\n",
    "        model.eval()\n",
    "        \n",
    "        test_df = pd.read_csv(test_data_path, dtype=np.float32)\n",
    "        test_data = BreastCancerDataset(test_df)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # iterate over validation data loader and make predictions\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # calculate overall acc.\n",
    "        accuracy = correct_predictions / len(test_dataloader.dataset)\n",
    "        \n",
    "        print(f\"{accuracy:.2f}\")\n",
    "   fed_model = SimpleNN(n_input=feature_inputs)     \n",
    "   fed_model.load_state_dict(torch.load(model_input.path))\n",
    "\n",
    "   #run prediction to check accuracy and save metrics\n",
    "   with open(metrics_output.path, \"w\") as f:\n",
    "        accuracy_m=run_prediction(fed_model.model, test_data_path)\n",
    "        f.write(f\"Model trained with federated learning accuracy: {accuracy_m}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "@dsl.pipeline(\n",
    "    name=\"federated-learning-pipeline\",\n",
    "    description=\"A pipeline for federated learning with client initialization, training, and evaluation.\"\n",
    ")\n",
    "def federated_learning_pipeline(\n",
    "    client_data_dir: str, \n",
    "    num_clients: int, \n",
    "    feature_inputs: int, \n",
    "    epochs: int,\n",
    "    test_data_path: str,\n",
    "    ):\n",
    "\n",
    "    class BreastCancerDataset(Dataset):\n",
    "        def __init__(self, df):\n",
    "            scaler = StandardScaler()\n",
    "            self.X = torch.tensor(scaler.fit_transform(df.iloc[:,1:-1].values))   # first (ID) and last (diagnisis) columns are excluded\n",
    "            self.y =  torch.tensor(df.iloc[:,-1].values)                          # load the diagnosis (malignant=1, benign=0)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "        \n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self, n_input):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.NN = Sequential(\n",
    "                nn.Linear(n_input, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16,1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            logits = self.NN(x)\n",
    "            return logits\n",
    "    class Client:\n",
    "        def __init__(self, name, model, train_loader, val_loader, optimizer, criterion):\n",
    "            self.name = name\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.criterion = criterion\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.metrics = dict({\"train_acc\": list(), \"train_loss\": list(), \"val_acc\": list(), \"val_loss\": list()})\n",
    "\n",
    "            print(f\"[INFO] Initialized client '{self.name}' with {len(train_loader.dataset)} train and {len(val_loader.dataset)} validation samples\")\n",
    "            \n",
    "            \n",
    "        def train(self):\n",
    "            \"\"\"\n",
    "                Trains the model of the client for 1 epoch.\n",
    "            \"\"\"\n",
    "            self.model.train()\n",
    "            correct_predictions = 0\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # iterate over training dataset\n",
    "            for inputs, labels in self.train_loader:\n",
    "                # make predictions\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "                # apply gradient\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # calculate number of correct predictions\n",
    "                predicted = torch.round(outputs)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            # calculate overall loss and acc.\n",
    "            epoch_loss = running_loss / len(self.train_loader)\n",
    "            accuracy = correct_predictions / len(self.train_loader.dataset)\n",
    "\n",
    "            # save metrics\n",
    "            self.metrics[\"train_acc\"].append(accuracy)\n",
    "            self.metrics[\"train_loss\"].append(epoch_loss)\n",
    "        \n",
    "        def validate(self):\n",
    "            \"\"\"\n",
    "                Validates the model of the client based on the given validation data loader.\n",
    "            \"\"\"\n",
    "            self.model.eval()\n",
    "            total_loss = 0\n",
    "            correct_predictions = 0\n",
    "\n",
    "            # iterate over validation data loader and make predictions\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in self.val_loader:\n",
    "                    outputs = self.model(inputs)\n",
    "                    labels = torch.unsqueeze(labels, 1)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    predicted = torch.round(outputs)\n",
    "                    correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            # calculate overall loss and acc.\n",
    "            average_loss = total_loss / len(self.val_loader)\n",
    "            accuracy = correct_predictions / len(self.val_loader.dataset)\n",
    "\n",
    "            # save metrics\n",
    "            self.metrics[\"val_acc\"].append(accuracy)\n",
    "            self.metrics[\"val_loss\"].append(average_loss)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    initialize_model_task = initialize_model(\n",
    "        client_data_dir=client_data_dir,\n",
    "        num_clients=num_clients,\n",
    "        feature_inputs=feature_inputs)\n",
    "    \n",
    "    train_model_task = train_model(\n",
    "        #model_input=initialize_model_task.outputs[\"model_output\"],\n",
    "        clients_input=initialize_model_task.outputs[\"clients_output\"],\n",
    "        epochs=epochs,\n",
    "        feature_inputs=feature_inputs)\n",
    "    \n",
    "    evaluate_task = evaluate(\n",
    "        model_input=train_model_task.outputs[\"trained_model_output\"],\n",
    "        test_data_path=test_data_path,\n",
    "        feature_inputs=feature_inputs),\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "@dsl.pipeline(\n",
    "    name=\"federated-learning-pipeline\",\n",
    "    description=\"A pipeline for federated learning with client initialization, training, and evaluation.\"\n",
    ")\n",
    "def federated_learning_pipeline(\n",
    "    client_data_dir: str, \n",
    "    num_clients: int, \n",
    "    feature_inputs: int, \n",
    "    epochs: int,\n",
    "    test_data_path: str,\n",
    "    trained_model_output: Output[Model],\n",
    "    test_metrics_output: Output[Artifact],\n",
    "    visualization_output: Output[Artifact]\n",
    "    ):\n",
    "\n",
    "    ## Step 1: Initialize clients and model\n",
    "    # load the model\n",
    "    fed_model = SimpleNN(n_input=feature_inputs)\n",
    "\n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        train_df = pd.read_csv(os.path.join(client_data_dir, f\"client_{i}\", \"train_data.csv\"), dtype=np.float32)\n",
    "        val_df = pd.read_csv(os.path.join(client_data_dir, f\"client_{i}\", \"val_data.csv\"), dtype=np.float32)\n",
    "        \n",
    "        train_data = BreastCancerDataset(train_df)\n",
    "        val_data = BreastCancerDataset(val_df)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=7, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.SGD(fed_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.BCELoss()\n",
    "    \n",
    "        clients.append(Client(f\"client_{i}\", fed_model, train_dataloader, val_dataloader, optimizer, criterion))\n",
    "    \n",
    "    # Step 2: Train the federated learning server\n",
    "    fl_server = FLServer(fed_model, clients)\n",
    "    # distribute the central model to all clients (Step 1 of figure at the beginning of the tutorial)\n",
    "    for client in fl_server.clients:\n",
    "        client.model.load_state_dict(fl_server.model.state_dict())\n",
    "\n",
    "    #run training with server\n",
    "    fl_server.run(epochs=epochs)\n",
    "\n",
    "    #save model\n",
    "    torch.save(fl_server.model.state_dict(), trained_model_output.path)\n",
    "    \n",
    "    #plot metrics for each client\n",
    "    for client in fl_server.clients:\n",
    "        client_plot_path = os.path.join(visualization_output.path, f\"{client.name}_metrics.png\")\n",
    "        plot_metrics(client, client_plot_path)\n",
    "        \n",
    "    #run prediction to check accuracy and save metrics\n",
    "    with open(test_metrics_output.path, \"w\") as f:\n",
    "        f.write(f\"Model trained with federated learning accuracy: {run_prediction(fl_server.model, test_data_path)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=federated_learning_pipeline,\n",
    "    package_path=\"federated_learning_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2faabe1",
   "metadata": {},
   "source": [
    "if you run into any errors run: `!gcloud auth login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d363e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Google cloud storage bucket.\n",
    "BUCKET='federated-learning-resources-zy-2'\n",
    "\n",
    "!gsutil mb gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e8217",
   "metadata": {},
   "source": [
    "Copy cleint data and the val testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r data/client_0 gs://$BUCKET/data/\n",
    "!gsutil cp -r data/client_1 gs://$BUCKET/data/\n",
    "!gsutil cp data/full_val_data.csv gs://$BUCKET/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7cb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"cit-oconnellka-9999\", location=\"us-central1\")\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"federated-learning-pipeline\",\n",
    "    template_path=\"federated_learning_pipeline.json\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/pipeline_root/federated_learning_pipeline\",\n",
    "    parameter_values={\n",
    "        \"client_data_dir\": f\"gs://{BUCKET}/data\",\n",
    "        \"num_clients\": 2,\n",
    "        \"feature_inputs\": 30,\n",
    "        \"epochs\": 10,\n",
    "        \"test_data_path\": f\"gs://{BUCKET}/data/full_val_data.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2826bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"federated-learning-pipeline\",\n",
    "    description=\"A pipeline for Federated Learning with tabular data.\"\n",
    ")\n",
    "def federated_learning_pipeline(\n",
    "    input_data_path: str,\n",
    "    num_clients: int,\n",
    "    output_dir: str,\n",
    "    validation_data_path: str,\n",
    "    epochs: int\n",
    "):\n",
    "    # Step 1: Prepare data\n",
    "    prepare_task = prepare_data(\n",
    "        input_data_path=input_data_path,\n",
    "        num_clients=num_clients,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Step 2: Client training\n",
    "    client_tasks = []\n",
    "    for i in range(num_clients):\n",
    "        client_task = client_training(\n",
    "            client_data_path=f\"{output_dir}/client_{i}/train_data.csv\",\n",
    "            epochs=epochs\n",
    "        )\n",
    "        client_tasks.append(client_task)\n",
    "    \n",
    "    # Step 3: Aggregate models\n",
    "    aggregate_task = aggregate_models(\n",
    "        client_models=[task.outputs[\"model_output\"] for task in client_tasks]\n",
    "    )\n",
    "    \n",
    "    # Step 4: Evaluate model\n",
    "    evaluate_task = evaluate_model(\n",
    "        model=aggregate_task.outputs[\"aggregated_model\"],\n",
    "        validation_data=validation_data_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=federated_learning_pipeline,\n",
    "    package_path=\"federated_learning_pipeline.json\"\n",
    ")\n",
    "\n",
    "# Submit the pipeline\n",
    "aiplatform.init(project=\"your-project-id\", location=\"us-central1\")\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"federated-learning-pipeline\",\n",
    "    template_path=\"federated_learning_pipeline.json\",\n",
    "    parameter_values={\n",
    "        \"input_data_path\": \"gs://your-bucket/input-data.csv\",\n",
    "        \"num_clients\": 3,\n",
    "        \"output_dir\": \"gs://your-bucket/output\",\n",
    "        \"validation_data_path\": \"gs://your-bucket/validation-data.csv\",\n",
    "        \"epochs\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c2982",
   "metadata": {},
   "source": [
    "Store the path of these files in their respective variable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adba6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save client datasets to path\n",
    "client_0_input_path = f'gs://$BUCKET/data/client_0'\n",
    "\n",
    "# save validation_dataset to gc bucket\n",
    "client_1_input_path = f'gs://$BUCKET/data/client_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4d514",
   "metadata": {},
   "source": [
    "Create a python script called task.py in a folder called \"scripts\". This script adapts the code snippets run above to work with resources from cloud storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fa8cd",
   "metadata": {},
   "source": [
    "Create a file called \"setup.py\" in your working directory. When this script is run, it will create a python package that contains all of the dependcies of your run. \n",
    "\n",
    "\n",
    "```python\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='breast_cancer_federated_learning',\n",
    "    version='0.1',\n",
    "    install_requires=[\n",
    "        'torch',\n",
    "        'numpy',\n",
    "        'pandas',\n",
    "        'matplotlib',\n",
    "        'scikit-learn',\n",
    "        'google-cloud-storage',\n",
    "        'google-cloud-aiplatform',\n",
    "    ],\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Breast Cancer Federated Learning Training Script',\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e05ef3",
   "metadata": {},
   "source": [
    "Run setup.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee905aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ./scripts/setup.py sdist bdist_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896e983",
   "metadata": {},
   "source": [
    "Copy the python package to your cloud storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp dist/breast_cancer_federated_learning-0.1-py3-none-any.whl gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3580b0b2",
   "metadata": {},
   "source": [
    "Submit the training job. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1dbf0",
   "metadata": {},
   "source": [
    "Upload the model to the Vertex AI Model Registry. This will allow you to deploy endpoints using the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3527ad",
   "metadata": {},
   "source": [
    "Create an endpoint and deploy it to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22edf2",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "If you are further interested in Federated Learning here are some useful resources to continue your FL journey.\n",
    "\n",
    "### Other Tutorials:\n",
    "- [TensorFlow](https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm)\n",
    "- [FLOWER](https://flower.ai/docs/framework/tutorial-series-get-started-with-flower-pytorch.html)\n",
    "\n",
    "### Frameworks:\n",
    "- [NVFLare](https://developer.nvidia.com/flare)\n",
    "- [TensorFlow](https://www.tensorflow.org/federated)\n",
    "- [FLOWER](https://flower.ai/docs/framework/index.html)\n",
    "- [FeatureCloud](https://featurecloud.ai/)\n",
    "\n",
    "### Literature:\n",
    "[1] Rieke et al., (2020), \"[The future of digital health with federated learning](https://www.nature.com/articles/s41746-020-00323-1)\"\n",
    "\n",
    "[2] Zhang et al., (2021), \"[A survey on federated learning](https://www.sciencedirect.com/science/article/pii/S0950705121000381)\"\n",
    "\n",
    "[3] Wolberg et al., (1993), [Breast Cancer Wisconsin (Diagnostic)](https://doi.org/10.24432/C5DW2B)\n",
    "\n",
    "[4] McMahan et al., (2017), \"[Communication-Efficient Learning of Deep Networks from Decentralized Data](https://proceedings.mlr.press/v54/mcmahan17a.html)\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

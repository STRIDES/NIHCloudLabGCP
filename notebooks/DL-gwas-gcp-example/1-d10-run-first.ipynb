{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e9938a",
   "metadata": {},
   "source": [
    "# Deep Learning GWAS with Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962d7d9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This task will deploy a deep learning (1D Convolutional Neural Network - MLP) GWAS experiment using the Kubeflow pipelines machine learning framework. We introduce you to the basic operation of the features that Kubeflow provides that are most relevant in a basic research analysis workflow and empower you to make meaningful use of them, these are tools that when used correctly, are user friendly and very effective. This framework is used by CERN in some of their research and CERN is also part of the KF open source community. This experiment will leave you with a template and the skills to build your own workflow relevant to your research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e4582",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ Use Kubeflow to orchestrate ML runs\n",
    "+ Deploy Kubeflow on GCP\n",
    "+ Conduct GWAS using deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9fe14d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "See the [README](./README.md) in this directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae558c",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7ab6c",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee0923-844f-42c8-9273-7e32d7178628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mark in Kale as skip\n",
    "! pip3 install --upgrade pip\n",
    "! pip3 install pendulum==2.1.2\n",
    "! pip3 install pandas==1.4.4\n",
    "! pip3 install matplotlib==3.3.4\n",
    "! pip3 install --upgrade tensorflow==2.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352040d-d4ca-41af-81ca-8df30e7de6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark in Kale as skip\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619de0db-3878-4254-93a4-4404553d0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mark in katib as imports\n",
    "import pendulum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.regularizers import l1,l2, L1L2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42103b5f-1c53-4047-99ba-690e5bb7d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time in the pipeline parameters and in Katib (as string and the only value) to the output of this\n",
    "pendulum.now(tz='America/New_York').__str__()[:16].replace('T','').replace(':','').replace('_','-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c927b",
   "metadata": {},
   "source": [
    "### Begin Kayle Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd604bf6-0d00-4178-9deb-28465c915c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark in Kayle as pipeline parameters\n",
    "data_file_to_run = \"IMP_height.txt\"\n",
    "\n",
    "experiment_description = \"Soy Height GWAS\"\n",
    "\n",
    "learning_rate = 0.001 # 0.0001957\n",
    "conv_1_dropout_rate = 0.50 # Dropout rate for first convolutional layer\n",
    "conv_1_kernel_l1 = 0.2 # L1 and l2 regularization for the first conv1d layer's weights\n",
    "conv_1_kernel_l2 = 0.6\n",
    "conv_1_bias_l2 = 0.6 # L1 and l2 regularization for the first conv1d layer's bias\n",
    "conv_1_activity_l2 = 0.00001 # L1 and l2 activity  regularization for the first conv1d layer\n",
    "\n",
    "conv_x_kernel_l1 = 0.3\n",
    "conv_x_kernel_l2 = 0.3\n",
    "conv_x_bias_l2 = 0.6\n",
    "conv_x_activity_l2 = 0.0001\n",
    "\n",
    "dense_x_kernel_l1 = 0.3\n",
    "dense_x_kernel_l2 = 0.6\n",
    "dense_x_bias_l2 = 0.0001\n",
    "dense_x_activity_l2 = 0.0001\n",
    "\n",
    "dense_out_kernel_l1 = 0.1\n",
    "dense_out_kernel_l2 = 0.6\n",
    "dense_out_bias_l2 = 0.6\n",
    "dense_out_activity_l2 = 0.00001\n",
    "\n",
    "conv_initializer = 'TruncatedNormal'  #    # 'TruncatedNormal' 'glorot_uniform'  \"GlorotNormal\",  \"HeNormal\" 'random_normal'  \n",
    "dese_initializer = \"GlorotNormal\" # 'TruncatedNormal' # \"GlorotUniform\"\n",
    "\n",
    "dropout_rate = 0.151\n",
    "num_dense_layers = 3\n",
    "num_dense_units = 20\n",
    "\n",
    "conv_activation = \"elu\" # \"linear\"\n",
    "activation = \"elu\"\n",
    "loss = 'huber_loss' # 'mean_squared_error' 'mean_absolute_error'\n",
    "\n",
    "final_activation_scale_factor = 3.5\n",
    "\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "\n",
    "time = '2023-01-051310'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06747ae-77b6-4524-b803-fc0d87512fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark in Kayle as pipeline step: \"preprocessing\": Depends on none\n",
    "\n",
    "ht = pd.read_csv(data_file_to_run, sep = '\\t')\n",
    "ht_pd = ht_relevant_cols = ht.drop(columns = ['strain', 'height', 'folds'])\n",
    "phenotypes_norm = ht_pd.pop(\"norm_phe\")\n",
    "for col in ht_pd.columns:\n",
    "    ht_pd[col] = ht_pd[col].astype('category')\n",
    "ohe_height_genotypes = pd.get_dummies(ht_pd)\n",
    "\n",
    "def train_test_splitting(row, split_ratio):\n",
    "    string_of_row = \"\".join([str(l) for l in list(row.values)])\n",
    "    return (abs(hash(string_of_row)) % 10) / 10 < split_ratio\n",
    "belongs_in_train_set_index =\\\n",
    "    np.array([train_test_splitting(ohe_height_genotypes.loc[i],0.7)\n",
    "     for i in np.arange(ht_pd.shape[0])])\n",
    "\n",
    "train_ohe_height_genotypes = ohe_height_genotypes[belongs_in_train_set_index]\n",
    "val_ohe_height_genotypes = ohe_height_genotypes[~belongs_in_train_set_index]\n",
    "\n",
    "train_phenotypes_norm = phenotypes_norm[belongs_in_train_set_index]\n",
    "val_phenotypes_norm = phenotypes_norm[~belongs_in_train_set_index]\n",
    "\n",
    "# Make sure the number of rows in test and train add up to the original rows\n",
    "assert train_ohe_height_genotypes.shape[0] + val_ohe_height_genotypes.shape[0] == ht_pd.shape[0]\n",
    "\n",
    "# Data as a numpy array...\n",
    "train_ohe_height_genotypes_np = train_ohe_height_genotypes.values\n",
    "val_ohe_height_genotypes_np = val_ohe_height_genotypes.values\n",
    "\n",
    "train_phenotypes_norm_np = train_phenotypes_norm.values\n",
    "val_phenotypes_norm_np = val_phenotypes_norm.values\n",
    "\n",
    "# Reshape to fit the conv1D network. \n",
    "train_np_ohe_reshaped_for_conv_1_d =\\\n",
    "    train_ohe_height_genotypes_np.reshape((train_ohe_height_genotypes_np.shape[0],\n",
    "                                          train_ohe_height_genotypes_np.shape[1], 1))\n",
    "val_np_ohe_reshaped_for_conv_1_d =\\\n",
    "    val_ohe_height_genotypes_np.reshape((val_ohe_height_genotypes_np.shape[0],\n",
    "                                        val_ohe_height_genotypes_np.shape[1],1))\n",
    "\n",
    "np.save('train_data_ready', train_np_ohe_reshaped_for_conv_1_d)\n",
    "np.save('val_data_ready', val_np_ohe_reshaped_for_conv_1_d)\n",
    "np.save('train_labels_ready', train_phenotypes_norm_np)\n",
    "np.save('val_labels_ready',val_phenotypes_norm_np)\n",
    "\n",
    "# Since the data was reshaped for the convolutional 1D neural network, we are also saving the \n",
    "# non-reshaped data to be used for calculating saliency.\n",
    "\n",
    "np.save(\"val_snps_for_saliency\", val_ohe_height_genotypes_np)\n",
    "\n",
    "print(\"Preprocessing successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01e8b2-78ba-4315-9e12-9153b1c57ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark in Kayle as pipeline step \"train\": depends on \"data-preprocessing\"\n",
    "nb_classes = 3\n",
    "\n",
    "data_files = ['./train_data_ready.npy',\n",
    "              \"./val_data_ready.npy\",\n",
    "              \"./train_labels_ready.npy\",\n",
    "              \"./val_labels_ready.npy\"]\n",
    "# artifact_bucket_root_name = artifacts_bucket.split('/')[-1]\n",
    "# print(artifact_bucket_root_name)\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.get_bucket(artifact_bucket_root_name)\n",
    "\n",
    "ht_np_train = np.load('train_data_ready.npy', allow_pickle=True)\n",
    "ht_np_val = np.load('val_data_ready.npy', allow_pickle=True)\n",
    "train_labels_np = np.load('train_labels_ready.npy', allow_pickle=True)\n",
    "val_labels_np = np.load('val_labels_ready.npy', allow_pickle=True)\n",
    "\n",
    "train_snps = ht_np_train\n",
    "train_phenotypes = train_labels_np\n",
    "val_snps = ht_np_val\n",
    "val_phenotypes = val_labels_np\n",
    "\n",
    "# print(\"min\")\n",
    "# print(val_phenotypes.min())\n",
    "# print(\"max\")\n",
    "# print(val_phenotypes.max())\n",
    "\n",
    "inputs =\\\n",
    "    tf.keras.layers.Input(\n",
    "        shape=(train_snps.shape[1], \n",
    "               train_snps.shape[2])) # train_snps.shape[1] ,nb_classes))\n",
    "\n",
    "x = Conv1D(10,\n",
    "           nb_classes,\n",
    "           padding='same',\n",
    "           activation = conv_activation,\n",
    "           kernel_initializer = conv_initializer,\n",
    "           kernel_regularizer=tf.keras.regularizers.L1L2(l1=conv_1_kernel_l1, l2=conv_1_kernel_l2),\n",
    "           bias_regularizer=tf.keras.regularizers.L2(conv_1_bias_l2),\n",
    "           activity_regularizer=tf.keras.regularizers.L2(conv_1_activity_l2)\n",
    "          )(inputs)\n",
    "\n",
    "           # kernel_initializer = conv_initializer ,\n",
    "           # kernel_regularizer=\"l2\", bias_regularizer = \"l2\")\n",
    "x = Dropout(conv_1_dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(10,\n",
    "           20,\n",
    "           padding='same',\n",
    "           activation = conv_activation,\n",
    "           kernel_initializer = conv_initializer,\n",
    "           kernel_regularizer=tf.keras.regularizers.L1L2(l1=conv_x_kernel_l1, l2=conv_x_kernel_l2),\n",
    "           bias_regularizer=tf.keras.regularizers.L2(conv_x_bias_l2),\n",
    "           activity_regularizer=tf.keras.regularizers.L2(conv_x_activity_l2)\n",
    "           # kernel_initializer = 'TruncatedNormal',\n",
    "           # kernel_regularizer=\"l2\",\n",
    "           # bias_regularizer=\"l2\"\n",
    "          )(x) # Leaving l1 l2 on head layer only to see if this prevents everything from zeroing out.\n",
    "\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "\n",
    "shortcut = Conv1D(10,\n",
    "                  4,\n",
    "                  padding='same',\n",
    "                  activation = conv_activation,\n",
    "                  kernel_initializer = conv_initializer,\n",
    "                  kernel_regularizer=tf.keras.regularizers.L1L2(l1=conv_x_kernel_l1, l2=conv_x_kernel_l2),\n",
    "                  bias_regularizer=tf.keras.regularizers.L2(conv_x_bias_l2),\n",
    "                  activity_regularizer=tf.keras.regularizers.L2(conv_x_activity_l2))(inputs)\n",
    "shortcut = Dropout(dropout_rate)(shortcut)\n",
    "x = tf.keras.layers.Add()([shortcut,x])\n",
    "\n",
    "x = Conv1D(10,\n",
    "           4,\n",
    "           padding='same',\n",
    "           activation = conv_activation,\n",
    "           kernel_initializer = conv_initializer,\n",
    "           kernel_regularizer=tf.keras.regularizers.L1L2(l1=conv_x_kernel_l1, l2=conv_x_kernel_l2),\n",
    "           bias_regularizer=tf.keras.regularizers.L2(conv_x_bias_l2),\n",
    "           activity_regularizer=tf.keras.regularizers.L2(conv_x_activity_l2)\n",
    "           # kernel_initializer = 'TruncatedNormal', \n",
    "           # kernel_regularizer = \"l2\",\n",
    "           # bias_regularizer = \"l2\"\n",
    "          )(x)\n",
    "\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "if num_dense_layers > 0:\n",
    "    y = x\n",
    "    for i in np.arange(num_dense_layers):\n",
    "        y = Dense(num_dense_units, \n",
    "                  activation,\n",
    "                  kernel_initializer=dese_initializer,\n",
    "                  kernel_regularizer=tf.keras.regularizers.L1L2(l1=dense_x_kernel_l1, l2=dense_x_kernel_l2),\n",
    "                  bias_regularizer=tf.keras.regularizers.L2(dense_x_bias_l2),\n",
    "                  activity_regularizer=tf.keras.regularizers.L2(dense_x_activity_l2)\n",
    "                  )(y)\n",
    "        # y = Dropout(dropout_rate)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        \n",
    "    x = tf.keras.layers.Concatenate(axis=1)([x,y])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "outputs_unscaled = Dense(1,\n",
    "                activation=\"softsign\",\n",
    "                kernel_initializer=dese_initializer,\n",
    "                kernel_regularizer=tf.keras.regularizers.L1L2(l1=dense_out_kernel_l1, l2=dense_out_kernel_l2),\n",
    "                bias_regularizer=tf.keras.regularizers.L2(dense_out_bias_l2),\n",
    "                activity_regularizer=tf.keras.regularizers.L2(dense_out_activity_l2),\n",
    "                # bias_regularizer = \"l2\",\n",
    "                # kernel_initializer = 'TruncatedNormal',\n",
    "                name = 'out')(x) # Should have no activation\n",
    "# Softsign coerces the output to the range {-1,1}. The labels are norm scaled, \n",
    "# where the range {-2,2} or {-3,3} encompasses most values. We multiply by a scalar \n",
    "# and the range will terminate at +/- said scalar. No telling which one is optimal,\n",
    "# so we'll le the the tuner figure out what: \n",
    "outputs = Lambda(lambda x: x * final_activation_scale_factor)(outputs_unscaled) \n",
    "\n",
    "our_data_model = Model(inputs = inputs, outputs = outputs)\n",
    "# qa_data_model = Model(inputs = inputs, outputs = outputs)\n",
    "our_data_model.compile(loss=loss,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
    "              jit_compile=True)\n",
    "\n",
    "history =\\\n",
    "    our_data_model.fit(x = train_snps,\n",
    "                       y = train_phenotypes,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       validation_data=(val_snps, val_phenotypes),\n",
    "                       shuffle= True,\n",
    "                       use_multiprocessing=True)\n",
    "# Requirement 6: save and log your artifact. \n",
    "# I'm adding a random number to the file name as an\n",
    "# extra layer of safety nets against race conditions\n",
    "# / file name conflicts\n",
    "\n",
    "# tn = str(int(np.random.random() * 10 ** 12))\n",
    "model_folder = f\"a-{time}-model\"\n",
    "our_data_model.save(model_folder)\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "history_df[[\"mean_absolute_error\", \"val_mean_absolute_error\"]].plot()\n",
    "plt.savefig(f'{model_folder}-history.png')\n",
    "\n",
    "print(model_folder)\n",
    "\n",
    "val_mean_absolute_error = float(history_df['val_mean_absolute_error'].values.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74954dbc-9133-40d5-9f23-400a39807724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark in Kayle as pipeline step: \"saliency - known\": depends on \"data-preprocessing\"\n",
    "\n",
    "# Calculate observed p values: (Depends on data preprocessing)\n",
    "\n",
    "val_snps_s = np.load(\"val_snps_for_saliency.npy\", allow_pickle=True)\n",
    "val_phenotypes_k = np.load('val_labels_ready.npy', allow_pickle=True)\n",
    "\n",
    "p_values = []\n",
    "for i in np.arange(int(val_snps_s.shape[1] / 3)):\n",
    "    column_index_lower_bound = 3 * i\n",
    "    column_index_upper_bound = 3 * i + 3\n",
    "    data = val_snps_s[:,column_index_lower_bound:column_index_upper_bound]\n",
    "    data_reshaped = np.argmax(data, axis=1)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(data_reshaped, val_phenotypes_k)\n",
    "    p_values.append(p_value)\n",
    "p_values_observed_np = np.array(p_values)\n",
    "np.save('p_values_observed_np', \n",
    "        p_values_observed_np, \n",
    "        allow_pickle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ffbb4-eb9e-438e-ac1e-292a2e45a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mark in Kayle as pipeline step: Pipeline step: manhattan_known: Depends on: saliency_known \n",
    "model_folder = f\"a-{time}-model\"\n",
    "p_values_observed_np = np.load('p_values_observed_np.npy',\n",
    "                               allow_pickle=True)\n",
    "\n",
    "neg_log = -1 * np.log10(p_values_observed_np)\n",
    "slug = np.arange(p_values_observed_np.shape[0])\n",
    "\n",
    "pt = plt.scatter(x = slug, y = neg_log, c=neg_log)\n",
    "plt.title(f\"Manhattan Plot for {experiment_description} - observed\")\n",
    "plt.xlabel(\"Ordinal SNP\")\n",
    "plt.ylabel(\"- log10 observed P values\")\n",
    "cbar = plt.colorbar(pt)\n",
    "cbar.set_label(\"- log10 observed P values\")\n",
    "plt.savefig(f\"{model_folder}-manhattan-observed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fae63-9e34-4d5b-b5d7-76e08e94142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_snps_s = np.load(\"val_snps_for_saliency.npy\", allow_pickle=True)\n",
    "model_folder = f\"a-{time}-model\"\n",
    "\n",
    "def get_saved_model(final_activation_scale_factor: float, model_folder: str):\n",
    "    final_model = tf.keras.models.load_model(model_folder)\n",
    "    for layer in final_model.layers:\n",
    "        layer.trainable=False\n",
    "    return final_model\n",
    "    # print(layer.weights)\n",
    "\n",
    "final_model = get_saved_model(final_activation_scale_factor=final_activation_scale_factor, model_folder = model_folder)\n",
    "val_phenotypes_p = final_model.predict(val_snps_s).flatten()\n",
    "\n",
    "p_values_p = []\n",
    "for i in np.arange(int(val_snps_s.shape[1] / 3)):\n",
    "    column_index_lower_bound = 3 * i\n",
    "    column_index_upper_bound = 3 * i + 3\n",
    "    data = val_snps_s[:,column_index_lower_bound:column_index_upper_bound]\n",
    "    data_reshaped = np.argmax(data, axis=1)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(data_reshaped, val_phenotypes_p)\n",
    "    p_values_p.append(p_value)\n",
    "p_values_predicted_np = np.array(p_values_p)\n",
    "np.save('p_values_predicted_np', \n",
    "        p_values_predicted_np, \n",
    "        allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043678ab-c0ae-45d7-8fdc-259d3138adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mark in Kayle as pipeline step: Pipeline step: manhattan_known: Depends on: saliency_known \n",
    "model_folder = f\"a-{time}-model\"\n",
    "p_values_predicted_np = np.load('p_values_predicted_np.npy',\n",
    "                               allow_pickle=True)\n",
    "\n",
    "neg_log = -1 * np.log10(p_values_predicted_np)\n",
    "slug = np.arange(p_values_predicted_np.shape[0])\n",
    "\n",
    "pt = plt.scatter(x = slug, y = neg_log, c=neg_log)\n",
    "plt.title(f\"Manhattan Plot for {experiment_description} - predicted\")\n",
    "plt.xlabel(\"Ordinal SNP\")\n",
    "plt.ylabel(\"- log10 predicted P values\")\n",
    "cbar = plt.colorbar(pt)\n",
    "cbar.set_label(\"- log10 predicted P values\")\n",
    "plt.savefig(f\"{model_folder}-manhattan-predicted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae392ab-a377-49a1-afaf-b5330892d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mark in Kayle as pipeline step: qq plot\n",
    "\n",
    "model_folder = f\"a-{time}-model\"\n",
    "\n",
    "p_values_observed_np = np.load('p_values_observed_np.npy',\n",
    "                               allow_pickle=True)\n",
    "\n",
    "neg_log_o = -1 * np.log10(p_values_observed_np)\n",
    "\n",
    "\n",
    "p_values_predicted_np = np.load('p_values_predicted_np.npy',\n",
    "                               allow_pickle=True)\n",
    "\n",
    "neg_log_p = -1 * np.log10(p_values_predicted_np)\n",
    "\n",
    "pt = plt.scatter(x = neg_log_o, y = neg_log_p, c= neg_log_p - neg_log_o)\n",
    "plt.title(f\"QQ Plot for {experiment_description}\")\n",
    "plt.xlabel(\"- log10 observed P values\")\n",
    "plt.ylabel(\"- log10 predicted P values\")\n",
    "cbar = plt.colorbar(pt)\n",
    "cbar.set_label(\"- log10 predicted P values - log10 observed\")\n",
    "plt.savefig(f\"{model_folder}-manhattan-predicted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be03ced0-99f2-43fd-b29e-407d0f85212c",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "You learned the first step to running a pipeline with Kubeflow to find the best model. **Now move to the second notebook, and find the model folder for the best model that Katib found.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b240cfb-b264-49dc-ae2c-73b10ef8a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(val_mean_absolute_error)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

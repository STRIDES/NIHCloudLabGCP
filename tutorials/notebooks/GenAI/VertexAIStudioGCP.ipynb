{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# VAI Studio on GCP - Article Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In research you often need to read several papers to understand new method or finidings and this can be quite taxing if you only need to get the gist of a article or need to quickly skim the article. In this tutorial we will use generative AI to summarize long documents but with the goal of still perserving the most important information using Generative AI Studio's Article Summary model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Generative AI pricing](https://cloud.google.com/vertex-ai/pricing#generative_ai_models), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvKl-BtQTRiQ"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_xMwRLuyDrj"
   },
   "source": [
    "Here you will use LLM via the API to summarize the extracted texts. Please note that LLMs currently have input text limit and stuffing a large input text might not be accepted. You can read more about quotas and limits [here](https://cloud.google.com/vertex-ai/docs/quotas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Generative AI Studio console [here](https://console.cloud.google.com/vertex-ai/generative/language?_ga=2.182664366.923116401.1692009977-1042353744.1691708677).\n",
    "\n",
    "Scroll down to **Summarization** and click on the model **Article Summary**. You will see a prompt session were you will need to enter in the contents of your article as the console does not allow you to upload files. For this tutorial this article is about how gut microbiota affects Alzeheimer's disease because of the gut-brain-microbiota axis network [here](https://www.aging-us.com/article/102930/pdf).\n",
    "\n",
    " <img src=\"../../../images/GCPGenStudio2.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "To the left you can control the parameters that we have been using before this is a great way to test what each parameter does and how they effect each other. Once you are done click **submit**, you should have a similar output as below. For explainations on the parameters **temperature, token limit max, top p, and top k** see the following article [here](https://cloud.google.com/vertex-ai/docs/generative-ai/text/test-text-prompts#generative-ai-test-text-prompt-drest).\n",
    "\n",
    " <img src=\"../../../images/GCPGenStudio3.png\" width=\"600\" height=\"600\">\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try increasing the temperature parameter and see if we recieve a different output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../images/GCPGenStudio4.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our output becomes shorter and straight to the point this is because the temperature parameter controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less creative responses, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aVrDWkJs3Y"
   },
   "source": [
    "### Troubleshooting\n",
    "\n",
    "If you model responds with an error its generally because the extracted text is too long for the generative model to process. In order for the model to work best try to not include any abstract or reference sections of your article, if errors still come up try limiting the article even more by removing other sections such as the intro or extracting the first 30,000 words of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtp21WX3T7d_"
   },
   "source": [
    "### Recap\n",
    "\n",
    "Although full text is too large for the model, you have managed to create a concise, paragraph of the most important information from a portion of the PDF using the model. This method is the most simiplest and is ideal for shorter documents but can still be used when you limit the character number you want the model to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "summarization_large_documents.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covid Pipeline\n",
    "The SRA code was borrowed from the ipyrad cookbooks. See more example notebooks here: https://github.com/dereneaton/ipyrad/blob/master/newdocs/API-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility purposes, it is nice to be able to download the raw data for your analysis from an online repository like NCBI with a simple script at the top of your notebook. We've written a simple wrapper for the sratools command line program to try to make this easier to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1, download SRA data and align to a reference genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CPU=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the following since this is a fresh VM\\\n",
    "#see the pangolin notebook for how to install mamba and add to your path\n",
    "#you can also use conda, since it is preinstalled, it is just a lot slower and less reliable\n",
    "!mamba install -y ipyrad -c bioconda \n",
    "!mamba install -y -c bioconda sra-tools\n",
    "!mamba install -y -c bioconda vcftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyrad.analysis as ipa\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch info for a published data set by its accession ID\n",
    "You can find the study ID or individual sample IDs from published papers or by searching the NCBI or related databases. ipyrad can take as input one or more accessions IDs for individual Runs or Studies (SRR or SRP, and similarly ERR or ERP, etc.). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "From an sratools object you can fetch just the info, or you can download the files as well. Here we call `.run()` to download the data into a designated workdir. There are arguments for how to name the files according to name fields in the fetch_runinfo table. The accessions argument here is a list of the first five SRR sample IDs in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually define list of SRA files\n",
    "list_of_srrs = ['SRR14086881','SRR14086882','SRR14086883','SRR14086884','SRR14086885','SRR14086886','SRR14086887','SRR14086888','SRR14086889','SRR14086890']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sra object\n",
    "sra2 = ipa.sratools(accessions=list_of_srrs, workdir=\"downloaded\")\n",
    "\n",
    "# call download (run) function\n",
    "sra2.run(auto=True, name_fields=(1,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data files\n",
    "You can see that the files were named according to the SRR and species name in the table. The intermediate .sra files were removed and only the fastq files were saved. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also navigate in the menu on the left\n",
    "! ls -l downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the data to a reference genome\n",
    "First, download the sars-cov-2 reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "#download the reference\n",
    "#use the bio.entrez toolkit within biopython to download the accession numbers\n",
    "#save those sequences to a single fasta file\n",
    "Entrez.email = \"email@example.com\"  # Always tell NCBI who you are\n",
    "filename = \"sarscov2_reference.fasta\"\n",
    "acc_nums=['NC_045512']\n",
    "for acc in acc_nums:\n",
    "        net_handle = Entrez.efetch(\n",
    "            db=\"nucleotide\", id=acc, rettype=\"fasta\", retmode=\"text\"\n",
    "        )\n",
    "        out_handle = open(filename, \"a\")\n",
    "        out_handle.write(net_handle.read())\n",
    "        out_handle.close()\n",
    "        net_handle.close()\n",
    "        print(\"Saved\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run BWA. It will just take a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the reference\n",
    "!bwa index sarscov2_reference.fasta\n",
    "# run bwa mem\n",
    "!bwa mem -t $CPU sarscov2_reference.fasta ./downloaded/SRR14086881_MSHSPSP-RNP263.fastq > sars.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the quality of the alignment, what percentage of the input reads aligned to the reference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools flagstat sars.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import VCF to Variant Transforms Tool and Query with BigQuery\n",
    "\n",
    "Here we are going to shift gears and use Google BigQuery, which is a serverless (you aren't responsible for managing VMs) data warehouse for analyzing data using SQL. We could translate that to giant database with very fast SQL query capabilities. We will download a vcf file, create a bucket, copy our data to the bucket, and then import some example bigquery vcf files to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download a vcf from this [galaxy url](https://usegalaxy.org/u/carlosfarkas/h/snpeffsars-cov-2) which comes from this [paper](https://www.frontiersin.org/articles/10.3389/fmicb.2021.665041/full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl https://usegalaxy.org/datasets/bbd44e69cb8906b553c8fa023002fca0/display?to_ext=vcf --output sra-covid.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "vcftools --vcf sra-covid.vcf --maf 0.005 --recode --out sra-covid-maf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cloud bucket\n",
    "The data is currently stored on this notebook instance, which is fine for any analyses here, but if we want to use that data somewhere else on GCP, or to access it when the VM is shut down, we need to copy the data to a cloud storage bucket. If we do not have one made, we can create one using the gsutil command line tool, or you can do it using the [console](https://cloud.google.com/storage/docs/quickstart-console)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your own project here and bucket name of choice. It must be globally unique\n",
    "!export PROJECT='us-gcp-ame-adv-c01-npd-1'\n",
    "!export BUCKET='vcf-to-bq2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot with the gsutil mb command, read more [here](https://cloud.google.com/storage/docs/gsutil/commands/mb).\n",
    "Note that if you end up with permission issues further down, you may need to delete this and create it using the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -c regional -l us-east4 gs://vcf-to-bq2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's copy the vcf file to the gs bucket using gsutil cp. If you are moving lots of files use gsutil -m cp /ex_dir/* or recursive with gsutil -m cp -r /ex_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp sra-covid-maf.recode.vcf gs://vcf-to-bq2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at our bucket to make sure the files are organized as expected. You can also just look at the bucket in the UI\n",
    "!gsutil ls gs://vcf-to-bq2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the vcf file to bigquery format using Life Sciences API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new tab with the console and from the Navigation menu, click on BigQuery. In the BigQuery UI, click on the three dots to the right of your project name, click Create Dataset. Name the Dataset vcf_to_bq, then click Create. Now return to the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your VCF file is in the correct format for variant transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/googlegenomics/gcp-variant-transforms/blob/master/docs/vcf_files_preprocessor.md\n",
    "\n",
    "# Parameters to replace:\n",
    "GOOGLE_CLOUD_PROJECT='us-gcp-ame-adv-c01-npd-1'\n",
    "GOOGLE_CLOUD_REGION='us-central1'\n",
    "GOOGLE_CLOUD_LOCATION='us-central1'\n",
    "TEMP_LOCATION='gs://vcf-to-bq2/temp'\n",
    "INPUT_PATTERN='gs://vcf-to-bq2/*.vcf'\n",
    "REPORT_PATH='gs://vcf-to-bq2/report.tsv'\n",
    "RESOLVED_HEADERS_PATH='gs://vcf-to-bq2/resolved_headers.vcf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "COMMAND=\"vcf_to_bq_preprocess \\\n",
    "  --input_pattern ${INPUT_PATTERN} \\\n",
    "  --report_path ${REPORT_PATH} \\\n",
    "  --job_name vcf-to-bigquery-preprocess \\\n",
    "  --resolved_headers_path ${RESOLVED_HEADERS_PATH} \\\n",
    "  --report_all_conflicts true \\\n",
    "  --runner DataflowRunner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -v ~/.config:/root/.config \\\n",
    "  gcr.io/cloud-lifesciences/gcp-variant-transforms \\\n",
    "  --project \"${GOOGLE_CLOUD_PROJECT}\" \\\n",
    "  --region \"${GOOGLE_CLOUD_REGION}\" \\\n",
    "  --temp_location \"gs://vcf-to-bq2/temp\" \\\n",
    "  \"${COMMAND}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run vcf to bq and run a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "GOOGLE_CLOUD_PROJECT='us-gcp-ame-adv-c01-npd-1'\n",
    "GOOGLE_CLOUD_REGION='us-central1'\n",
    "INPUT_PATTERN='gs://$BUCKET/sra-covid-maf.recode.vcf'\n",
    "OUTPUT_TABLE='GOOGLE_CLOUD_PROJECT:vcf_to_bq.covid_vcf'\n",
    "BUCKET='vcf-to-bq'\n",
    "TEMP_LOCATION='gs://$BUCKET/temp'\n",
    "GOOGLE_CLOUD_LOCATION='us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_PATTERN='gs://genomics-public-data/NA12878.chr20.sample.DeepVariant-0.7.2.vcf'\n",
    "OUTPUT_TABLE='$GOOGLE_CLOUD_PROJECT:vcf_to_bq.covid_vcf'\n",
    "TEMP_LOCATION='gs://${BUCKET}/temp'\n",
    "PROJECT = 'us-gcp-ame-adv-c01-npd-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMAND=\"/opt/gcp_variant_transforms/bin/vcf_to_bq \\\n",
    "    --worker_machine_type n1-standard-4 \\\n",
    "    --project $PROJECT \\\n",
    "    --input_pattern ${INPUT_PATTERN} \\\n",
    "    --output_table ${OUTPUT_TABLE} \\\n",
    "    --temp_location ${TEMP_LOCATION} \\\n",
    "    --job_name vcf-to-bigquery \\\n",
    "    --runner DataflowRunner \\\n",
    "    --max_num_workers 5 \\\n",
    "    --region us-central1\"\n",
    "!gcloud beta lifesciences pipelines run \\\n",
    "    --project $PROJECT \\\n",
    "    --logging \"${TEMP_LOCATION}/runner_logs_$(date +%Y%m%d_%H%M%S).log\" \\\n",
    "    --service-account-scopes https://www.googleapis.com/auth/cloud-platform \\\n",
    "    --docker-image gcr.io/cloud-lifesciences/gcp-variant-transforms \\\n",
    "    --command-line \"${COMMAND}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore VCF queries in BigQuery\n",
    "The rest of the notebook could be run in the BigQuery UI in the console. Here we are querying bigquery datasets via the APi, but if you paste the block of SQL code in the editor window of bigquery, it would have the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the biquery api\n",
    "from google.cloud import bigquery\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the client for the API\n",
    "client = bigquery.Client(location=\"US\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some cancer genomic data from the [ISB Cancer Gateway in the Cloud](https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/progapi/bigqueryGUI/GettingStartedWithGoogleBigQuery.html). If you follow that link, it gives a pretty good tutorial on how to use BigQuery using the UI. You can also use some of the sample notebooks from the left hand nav menu that walk you through how to use bigquery from a notebook in addition to what we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the most frequently observed mutations are for KRAS mutant tumor samples.\n",
    "query = \"\"\"\n",
    "WITH temp1 AS (\n",
    "   SELECT\n",
    "     sample_barcode_tumor,\n",
    "     Hugo_Symbol,\n",
    "     Variant_Classification,\n",
    "     Variant_Type,\n",
    "     SIFT,\n",
    "     PolyPhen\n",
    "   FROM `isb-cgc-bq.TCGA_versioned.somatic_mutation_hg38_gdc_r10`\n",
    "   GROUP BY\n",
    "     sample_barcode_tumor,\n",
    "     Hugo_Symbol,\n",
    "     Variant_Classification,\n",
    "     Variant_Type,\n",
    "     SIFT,\n",
    "     PolyPhen)\n",
    "SELECT\n",
    "  COUNT(*) AS num,\n",
    "  Hugo_Symbol,\n",
    "  Variant_Classification,\n",
    "  Variant_Type,\n",
    "  SIFT,\n",
    "  PolyPhen\n",
    "FROM temp1\n",
    "GROUP BY\n",
    "  Hugo_Symbol,\n",
    "  Variant_Classification,\n",
    "  Variant_Type,\n",
    "  SIFT,\n",
    "  PolyPhen\n",
    "ORDER BY num DESC\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's follow these steps to look at the [DeepVariant platinum genomes calls](https://cloud.google.com/life-sciences/docs/tutorials/analyze-variants-advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first look at total calls per sample\n",
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  call.name AS call_name,\n",
    "  COUNT(call.name) AS call_count_for_call_set\n",
    "FROM\n",
    "  `bigquery-public-data.human_genome_variants.platinum_genomes_deepvariant_variants_20180823` v, v.call\n",
    "GROUP BY\n",
    "  call_name\n",
    "ORDER BY\n",
    "  call_name\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People don't actually have ~30 million variants, so let's filter to real variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  call.name AS call_name,\n",
    "  COUNT(call.name) AS call_count_for_call_set\n",
    "FROM\n",
    "  `bigquery-public-data.human_genome_variants.platinum_genomes_deepvariant_variants_20180823` v, v.call\n",
    "WHERE\n",
    "  EXISTS (SELECT 1\n",
    "            FROM UNNEST(v.alternate_bases) AS alt\n",
    "          WHERE\n",
    "            alt.alt NOT IN (\"<NON_REF>\", \"<*>\"))\n",
    "GROUP BY\n",
    "  call_name\n",
    "ORDER BY\n",
    "  call_name\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the no-calls and the number of variants starts to look more reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  call.name AS call_name,\n",
    "  COUNT(call.name) AS call_count_for_call_set\n",
    "FROM\n",
    "  `bigquery-public-data.human_genome_variants.platinum_genomes_deepvariant_variants_20180823` v, v.call\n",
    "WHERE\n",
    "  EXISTS (SELECT 1 FROM UNNEST(call.genotype) AS gt WHERE gt > 0)\n",
    "  AND NOT EXISTS (SELECT 1 FROM UNNEST(call.genotype) AS gt WHERE gt < 0)\n",
    "GROUP BY\n",
    "  call_name\n",
    "ORDER BY\n",
    "  call_name\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Make sure before you move onto the next thing, you try running a few queries in the BigQuery editor within the UI. You can also create a new dataset under your project and explore the public datasets available there too."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
